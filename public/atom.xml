<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Just Another Linux Blog]]></title>
  <link href="http://www.chriscowley.me.uk/atom.xml" rel="self"/>
  <link href="http://www.chriscowley.me.uk/"/>
  <updated>2013-04-29T21:01:10+02:00</updated>
  <id>http://www.chriscowley.me.uk/</id>
  <author>
    <name><![CDATA[Chris Cowley]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Using Hiera with Puppet]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2013/04/11/using-hiera-with-puppet/"/>
    <updated>2013-04-11T20:24:00+02:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2013/04/11/using-hiera-with-puppet</id>
    <content type="html"><![CDATA[<p>Using Hiera with Puppet is something I have struggled with a bit. I could see the benefits, namely decoupling my site configuration from my logic. However, for some reason I struggled a bit to really get my head around it. This was compounded by it being quite new (only really integrated in Puppet 3), so the docs are  little lacking.</p>

<!-- more -->


<p>There is some though, the <a href="http://docs.puppetlabs.com/hiera/latest/">documentation on PuppetLab&#8217;s site</a> is excellent, but a bit light. It explains the principles well, but is a little limited in real-world examples. Probably the best resource I found was Kelsey Hightower&#8217;s excellent presentation at <a href="http://youtu.be/z9TK-gUNFHk">PuppetConf 2012</a>:</p>

<p>I learnt a lot from that, but it would be nice if there was an equivalent written down. I suppose that is what I am aiming at here.</p>

<h1>Configuration</h1>

<ul>
<li>NFS Module: https://github.com/chriscowley/my-puppet/tree/master/modules/nfs</li>
<li>Hiera Config: https://github.com/chriscowley/my-puppet/blob/master/hiera.yaml</li>
<li>Hiera Data: https://github.com/chriscowley/my-puppet/tree/master/hieradata</li>
</ul>


<p>I am using Open Source Puppet 3. If you are using 2.7 or Puppet Enterprise, files will be in a slightly different place. That is all explained in the documentation linked above.</p>

<p>The first thing you need to do is configure Hiera using the file <code>/etc/puppet/hiera.yaml</code>. Mine looks like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>---
</span><span class='line'>:backends:
</span><span class='line'>- yaml
</span><span class='line'>:yaml:
</span><span class='line'>:datadir: /etc/puppet/hieradata/
</span><span class='line'>:hierarchy:
</span><span class='line'>- %{::clientcert}
</span><span class='line'>- common</span></code></pre></td></tr></table></div></figure>


<p>This tells Hiera to use only the YAML backend - I do not like JSON because it always looks messy to me. It will look for the data in the folder <code>/etc/puppet/hieradata</code>. Finally it will look in that folder for a file called <clientcert>.yaml, then common.yaml. The process it uses to apply the values is explained very nicely in this image:
<img src="http://docs.puppetlabs.com/hiera/latest/images/hierarchy1.png"></p>

<p>Next, create the file <code>/etc/puppet/hieradata/&lt;certname&gt;.yaml</code> that contains your NFS exports:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>---
</span><span class='line'>exports:
</span><span class='line'>- /srv/iso
</span><span class='line'>- /srv/images</span></code></pre></td></tr></table></div></figure>


<p>Now, checkout my NFS module from Github links above. If you are not on RHEL6 or similar (I use Centos personally) you will have to modify it as needed.</p>

<p>There are 2 files that are really interesting here. The manifest file (manifests/server.pp) and the template to build the <code>/etc/exports</code> file (templates/exports.erb). We&#8217;ll take apart the manifest, the template just iterates over the data passed to it from that.</p>

<p>The first line creates an array variable called $exports from the Hiera data. Specifically, it looks for a key called <em>exports</em>. Hiera then goes through the hierarchy explained earlier looking for that key. In this case it will find it in the <certname>.yaml.</p>

<p>This data is now used for 2 things. First it creates the necessary folders, then it build <code>/etc/exports</code>. Here there is a minor problem, because you cannot do a <em>for each</em> loop in a Puppet manifest. We can fiddle it a bit by using a <a href="http://docs.puppetlabs.com/puppet/3/reference/lang_defined_types.html">defined type</a>.</p>

<p>The line <code>list_exports { $exports:; }</code> passes the <code>$exports</code> array to the type we define above it. This then goes ahead and creates the folders ready to be exported. The <code>-&gt;</code> builds an <a href="http://docs.puppetlabs.com/puppet/3/reference/lang_relationships.html#chaining-arrows">order relationship</a> with the File resource for <em>/etc/exports</em>. Specifically, that the directories need to be created before they are exported.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  define list_exports {
</span><span class='line'>    $export = $name
</span><span class='line'>    file { $export:
</span><span class='line'>      ensure =&gt; directory,
</span><span class='line'>      mode =&gt; '0755',
</span><span class='line'>      owner =&gt; 'root',
</span><span class='line'>      group =&gt; 'root'
</span><span class='line'>    }
</span><span class='line'>  }
</span><span class='line'>  list_exports { $exports:; } -&gt; File['/etc/exports']</span></code></pre></td></tr></table></div></figure>


<p>Now it can go ahead and build the <code>/etc/exports</code> file using that same $exports array in the <code>templates/exports.erb</code> template:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  &lt;% [exports].flatten.each do |export| -%&gt;
</span><span class='line'>  &lt;%= export %&gt; 192.168.1.0/255.255.255.0(rw,no_root_squash,no_subtree_check)
</span><span class='line'>  &lt;% end -%&gt;</span></code></pre></td></tr></table></div></figure>


<p>There is nothing especially Hiera&#8217;y about this, other than where the data in that array came from.</p>

<p>The rest of the manifest deals with installing the packages and configuring services. Once again, nothing especially linked with Hiera, but hopefully it will be useful for anyone wanting to Puppetize their NFS servers - which of course you should be.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Stop the hate on software RAID]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2013/04/07/stop-the-hate-on-software-raid/"/>
    <updated>2013-04-07T20:21:00+02:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2013/04/07/stop-the-hate-on-software-raid</id>
    <content type="html"><![CDATA[<p><img class="right" src="http://www.chriscowley.me.uk/images/NetappClustering.jpg">I&#8217;ve had a another bee in my bonnet recently. Specifically, it has been to do with hardware vs software RAID, but I think it goes deeper than that. It started a couple of months back with a discussion on <a href="http://redd.it/18dp63">Reddit</a>. Some of the comments were:</p>

<blockquote><p>Get out, get out now.</p>

<p>while he still can..</p>

<p>WHAT!?
60 TB on software raid.
Jeezus.</p>

<p>Software raid? Get rid of it.</p></blockquote>

<p>It then got re-awakened the other day when Matt Simmons (aka <a href="http://www.standalone-sysadmin.com/blog/">The Standalone Sysadmin</a>) asked the following question on Twitter:</p>

<blockquote class="twitter-tweet"><p>So what are the modern arguments for / against hardware / software RAID? I don&#8217;t get out much. <a href="https://twitter.com/search/%23sysadmin">#sysadmin</a></p>&mdash; Matt Simmons (@standaloneSA) <a href="https://twitter.com/standaloneSA/status/319932013492703233">April 4, 2013</a></blockquote>


<script async src="http://www.chriscowley.me.uk//platform.twitter.com/widgets.js" charset="utf-8"></script>


<!-- more -->


<p>At the time of writing, 2 people replied: myself and <a href="http://utcc.utoronto.ca/~cks/space/blog/">Chris Siebenmann</a>. Both of us basically said software RAID is better, hardware is at best pointless.</p>

<p>First of all, I need to define what I mean by hardware RAID. First, I do not care about what you are using for your c:\ drive in Windows, or your / partition in Linux. I am talking about the place where you store your business critical data. If you file server goes down, that is a bad day, but the business will live on. Lose your business data, then you will be out of a job (most likely alongside everyone else). Hardware RAID can thus fall into to categories:</p>

<ul>
<li>a bunch of disks attached to a PCI-e card in a big server</li>
<li>an external storage array. This could be either SAN or NAS, once again I do not care in this instance.</li>
</ul>


<p>I am firmly of the opinion that hardware RAID cards should no longer exist. They are at best pointless and at worst a liability. Modern systems are so fast that there is no real performance hit. Also management is a lot easier; if you have a hardware array then you will need to load the manufacturer&#8217;s utilities in order to manage it. By manage, I mean to be told when a disk has failed. On Linux, there is no guarantee that will work. There is a couple of vendors that require packages from RHEL4 to be installed on RHEL6 systems to install their tools. Also, they are invariable closed source, will most likely taint my kernel with binary blobs and generally cause a mess on my previously clean system. By contrast, using software RAID means that I can do all the management with trivial little scripts that can easily be integrated with any monitoring system that I choose to use.</p>

<p>I can understand why people are skeptical of software RAID. There have been performance reasons and practical reasons for it not to be trusted. I&#8217;m not going to address the performance argument, suffice to say that RAID is now 25 years old - CPUs have moved on a lot in that time. I remember when the first Promise IDE controllers came out, that used a kind of pseudo-hardware RAID - it was not pretty. The preconceptions are compounded by the plethora of nasty controllers built in to consumer motherboards and (possibly worst of all) Window&#8217;s built in RAID that was just bad.</p>

<p>The thing is, those days are now a long way behind us. For Linux there is absolutely no need for hardware RAID, even Windows will be just fine with an motherboard based RAID for its c: drive.</p>

<p><span class='pullquote-right' data-pullquote='I would say that hardware RAID is a liability'>
In fact I would say that hardware RAID is a liability. You go to all that effort to safe-guard your data, but the card becomes a single-point-of-failure. It dies, then you spend your time searching Ebay for the same model of card. You buy it, then you pray that the RAID data is stored on the disks and not the controller (not always the case). By contrast, if you use software RAID and the motherboard dies, then you pull the disks and plug them into whatever box running Linux and you recover your data.
</span></p>

<p>There is definitely a time and place for an external array. If you are using virtualisation properly, you need shared storage. The best way to do that, 9 times out of 10, is with an external array. However, even that may well not be as it seems. There are some that still develop dedicated hardware and come out with exciting gear (HP 3Par and Hitachi Data Systems spring to mind). However, the majority of storage is now on Software.</p>

<p>Let take a look at these things and see just how much &#8220;hardware&#8221; is actually involved.</p>

<p>The EMC VMAX is a big, big black box of storage. Even the &#8220;baby&#8221; 10k one scales up to 1.5PB and 4 engines. The 40k will go up to 3PB and 8 engines. Look a little deeper (one line further on the spec sheet) and you find that what those engines are: quad Xeons (dual on the 10/20k). The great big bad VMAX is a bunch of standard x86 servers running funky software to do all the management and RAID calculations.</p>

<p><span class='pullquote-right' data-pullquote='since the Clariion CX4 EMC has been using Windows Storage Server'>
Like its big brother, the VNX is also a pair of Xeon servers. Even more, it runs Windows. In fact since the Clariion CX4 EMC has been using Windows Storage Server (based on XP) Move along to EMC&#8217;s other lines we find Isilion is nothing more than a big pile of Supermicro servers running (IIRC) FreeBSD.
</span></p>

<p>Netapp&#8217;s famed FAS range similarly runs on commodity hardware,OnTAP is <a href="https://en.wikipedia.org/wiki/NetApp_filer">BSD</a> based.</p>

<p>The list goes on, Dell Compellent? When I looked at it in early 2012, it was still running on Supermicro dual Xeons. The plan was to move it to Dell R-series servers as soon as possible. They were doing validation at the time, I suspect the move is complete now. Reading between the lines, I came away with the impression that it runs on FreeBSD, but I do not know for sure. CoRAID use Supermicro servers, they unusually run Plan9 as their OS. HP StoreVirtual (formerly Lefthand) runs or Proliant Gen8 servers or VMware. In all these cases, there is no extra hardware involved.</p>

<p><span class='pullquote-right' data-pullquote='The people that write the MD stack in the Linux kernel are not cowboys'>
The people that write the MD stack in the Linux kernel are not cowboys. It has proved over and over again that is both stable and fast. I have trusted some of the most important data under my care to their software:  for many years the ERP system at <a href="http://www.snellgroup.com">Snell</a> has been running on MD devices quite happily. We found it much faster than the P410 cards in the DL360G5 servers that host it. Additionally, you do not need to load in any funky modules or utilities - everything you need to manage the devices is there in the distribution.
</span></p>

<p>ZFS also recommends to bypass any RAID devices and let it do everything in software, as does Btrfs. With <em>Storage Spaces</em> in Server 2012 Microsoft is definitely angling towards software controlled storage as well.</p>

<p>As with everything in IT, hardware is falling by the wayside in storage. Modern processors can do the processing so fast that there is no performance need for hardware in between your OS and the disks any more. The OS layers (Storage Spaces on Windows and especially MD/LVM on Linux) are so mature now that their reliability can be taken as a given. With the management advantages, there really is no technical reason to stick with hardware RAID. In fact the closer you can get the raw disks to your OS the better.</p>

<p><span class='pullquote-right' data-pullquote='we need to know what is inside that magic black box, especially when it is in the spec sheet'>
As I said at the start, the subject here is software vs hardware RAID, but my problem goes deeper than that particular argument. As technology professionals, we are technical people. We need to understand what is going on under the bonnet - that is our job! It may be fine for a vendor to pull the wool over a CFO&#8217;s eyes, but we need to know what is inside that magic black box, especially when it is in the spec sheet.
</span></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Writeable TFTP Server On CentOS]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2013/03/25/writeable-tftp-server-on-centos/"/>
    <updated>2013-03-25T15:45:00+01:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2013/03/25/writeable-tftp-server-on-centos</id>
    <content type="html"><![CDATA[<p>Well this caught me out for an embarassingly long time. There are <a href="http://blog.penumbra.be/tag/tftp/">loads</a> <a href="http://www.question-defense.com/2008/11/13/linux-setup-tftp-server-on-centos">of</a> <a href="http://wiki.centos.org/EdHeron/PXESetup">examples</a> of setting up a TFTP server on the web. The vast majority of them assume that you are using them read-only for PXE booting.</p>

<!-- more -->


<p>I needed to make it writeable so that it could be used for storing switch/router backups. It is trivially simple once you have read the man page (pro tip: RTFM).</p>

<p>I am doing this on RHEL6, it should be fine on Centos, Scientific Linux or Fedora as is. Any other distro it will require some modification. First install it (install the client as well to test at the end:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yum install tftp tftp-server xinetd
</span><span class='line'>chkconfig xinetd on</span></code></pre></td></tr></table></div></figure>


<p>Now edit the file `/etc/xinetd.d/tftp to read:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>service tftp
</span><span class='line'>{
</span><span class='line'>    socket_type = dgram
</span><span class='line'>    protocol    = udp
</span><span class='line'>    wait        = yes
</span><span class='line'>    user        = root
</span><span class='line'>    server      = /usr/sbin/in.tftpd
</span><span class='line'>    server_args = -c -s /var/lib/tftpboot
</span><span class='line'>    disable     = no
</span><span class='line'>    per_source  = 11
</span><span class='line'>    cps         = 100 2
</span><span class='line'>    flags       = IPv4
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>There are 2 changes to this file from the defaults. The <code>disable</code> line enables the service. Normally that is where you leave it. However, you cannot upload to the server in this case without pre-creating the files.</p>

<p>The second change adds a <code>-c</code> flag to the <code>server_args</code> line. This tells the service to create the files as necessary.</p>

<p>It still will not work though. You need to tweak the filesystem permissions and SELinux:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>chmod 777 /var/lib/tftpboot
</span><span class='line'>setsebool -P tftp_anon_write 1</span></code></pre></td></tr></table></div></figure>


<p>Of course you&#8217;ll also need to open up the firewall. So add the following line to <code>/etc/sysconfig/iptables</code>:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>-A INPUT -m state --state NEW -m udp -p udp -m udp --dport 69 -j ACCEPT</span></code></pre></td></tr></table></div></figure>


<p>If your IPtables set up is what comes out of the box, there will be a similar line to allow SSH access (tcp:22), I would add this line just after that one. If you have something more complicated, then you will probably know how to add this one as well anyway.</p>

<p>You should now be able to upload something to the server</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>echo "stuff" &gt; test
</span><span class='line'>tftp localhost -c put test</span></code></pre></td></tr></table></div></figure>


<p>Your test file should now be in <code>var/lib/tftpboot</code>.</p>

<p>One final note with regards to VMware. This does not work if you are using the VMXNET3 adapter, so make sure you are using the E1000. GETs will work and the file will be created, but no data will be put on the server. To annoy you even more, the test PUTting to localhost will work, but PUTs from a remote host will not.</p>

<p>It has been noted in the VMware forums <a href="http://communities.vmware.com/thread/215456">here</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What a boss owes their staff]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2013/03/18/what-a-boss-owes-their-staff/"/>
    <updated>2013-03-18T20:47:00+01:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2013/03/18/what-a-boss-owes-their-staff</id>
    <content type="html"><![CDATA[<p><img class="right" src="http://www.chriscowley.me.uk/images/reputation-management-starts-with-trust.jpg"> I recently had a conversation on Twitter with my friend <a href="http://www.robborley.com/">Rob Borley</a> who runs a <a href="http://www.dootrix.com/">mobile startup</a>. He had asked what interesting perks he should be giving his <a href="https://twitter.com/bobscape/statuses/313610008535367680">staff</a>.</p>

<!-- more -->


<p><span class='pullquote-right' data-pullquote='When you have a great work environment what is it that is at the root?'>
My initial response was the standard IT answer. Training, certifications and a lab to play in, which they already have. I like to find the root cause of things, usually that means looking for the underlying reason something is broken. In this case I wanted to put a more positive spin on it. When you have a great work environment what is it that is at the root? The answer is simple: trust.
</span></p>

<p>By way of a silly example, if I were to put a cake in the middle of my son&#8217;s classroom, I can guarantee that the majority of the cake will go into the mouths of a few, while most will probably not get any. Why? They are children, that is why. However, if I give it to his teacher then she will make sure that it gets evenly distributed to everyone. She, like your staff, is an adult and she behaves as such.</p>

<p><span class='pullquote-right' data-pullquote='if someone is going to sit there surfing Engadget all day, you are powerless to stop them'>
The has been a lot in the news recently about remote-working. Chiefly because of the new Yahoo CEO <a href="http://allthingsd.com/20130222/physically-together-heres-the-internal-yahoo-no-work-from-home-memo-which-extends-beyond-remote-workers/">putting a stop</a> to it. I have to fall in line with what Tony Schwartz <a href="http://www.businessinsider.com/want-productive-employees-treat-them-like-adults-2013-3">wrote in response</a> to that on Business Insider. Basically, if you cannot trust your staff to work when they are not in the office, you have hired the wrong people. You cannot be watching them all the time, nor can middle-management once you are past the start-up stage. Basically, if someone is going to sit there surfing Engadget all day, you are powerless to stop them. However, they will not be delivering, so they have to go. Likewise I have had colleagues who everytime I looked at their screen were surfing Ebay, or the Register. We hardly ever discussed computers, we mostly discussed trains and bikes. We delivered however, so who cares what was in our browser window and conversation? I myself got pulled to one side one day by my old boss to ask why I was playing around with an ESX server. We had no VMware servers, nor did we have any plans to. My response was that it would help make me better at my job. A year later we started rolling out a VMware infrastructrue, a project which I lead because I had taken the time to learn stuff. My boss had <em>trusted</em> me that I was not wasting my time and it paid off for him because we did not have to get in expensive consultants.
</span></p>

<p><span class='pullquote-right' data-pullquote='Trust leads to everything else that we like about work.'>
Trust leads to everything else that we like about work. Allowing your staff to work from home whenever they want is a question of trust. Perhaps one of them is spending time learning how to program in <a href="http://golang.org/">Go</a> even though you are a Dot Net house. Let them do so, trust them that they are going to make themselves a better programmer.
</span></p>

<p>This stuff may pay off directly (as in my VMware example), may be it won&#8217;t. If you let people work from home, maybe at times you will wonder what they are doing. You will however have a happier employee. If that employee has no desire to go anywhere else, but wants to deliver the best they can for your company then you can only win.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Thoughts on the shiney new VMAX]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2013/03/01/thoughts-on-the-shiney-new-vmax/"/>
    <updated>2013-03-01T15:21:00+01:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2013/03/01/thoughts-on-the-shiney-new-vmax</id>
    <content type="html"><![CDATA[<p><img class="right" src="http://www.emc.com/R1/images/EMC_Image_C_1310593327367_header-image-vmax-10k.png"> I&#8217;ve spent a significant amount of time recently swatting up on EMC&#8217;s new <a href="http://chucksblog.emc.com/chucks_blog/2013/02/introducing-vmax-cloud-edition.html">VMAX Cloud Edition</a>. It has to be said that this looks like one of the most interesting storage announcements I have seen in a long time. In fact I have a project coming up that I think it may well be a perfect fit for.</p>

<!-- more -->


<p>First a massive thanks to EMC&#8217;s Matthew Yeager (@mpyeager) who answered a couple of questions I had. He really went the extra mile to clarify a couple of things and the <a href="http://www.youtube.com/watch?v=WoElTAevLDs">video</a> he made is well worth a watch. Also Martin Glassborow (@storagebod) has <a href="http://www.storagebod.com/wordpress/?p=1293">interesting things to say</a> as well.</p>

<p>This is a product that could put a lot of people out of a job. If you are the sort of person who likes to keep hold of your little castle&#8217;s of knowledge then you will not like this from what I can see. Finally we are able to be truly customer focused, balancing cost, performance and capacity to give them exactly what they want. EMC claim this is a world first and to my knowledge they are right.</p>

<p><span class='pullquote-right' data-pullquote='With that amount of data the amount of art involved diminishes'>
Storage architects put a lot of time and effort in to tweaking quotes and systems to balance price, capacity and performance for a given work load. However, most of this is just reading up on the best-practises for a given array and situation and applying them. There is nothing that clever to it - reading and practise is what it comes down to. However, it has alway been as much an art as a science because an individual architect does not have a very large dataset to refer to. On the other hand EMC have got 60 million hours of metrics across more than 7000 VMAX systems out in the field. With that amount of data the amount of art involved diminishes and it becomes purely a science.
</span></p>

<p>What you get is a <a href="http://www.emc.com/storage/symmetrix-vmax/vmax-10k.htm">VMAX 10k</a>, but instead of defining storage pools, tiering policies, RAID levels etc you balance 3 facters: Space, performance and cost. Need a certain performance level for a certain amount of space no matter the cost? Just dial it in and mail EMC a cheque. Have a certain budget, need a certain amount of space, but performance not a problem? Same again.</p>

<p>No longer will we  be carefully balancing the number of SATA and FC spindles and the types of RAID level. No longer will be worrying about what percentage of our workload we need to keep on the SSD layer to assure the necessary number of IOPS. We will not even be calculating how much space we have after the RAID overheads.</p>

<p><span class='pullquote-right' data-pullquote='Here we have the abilty to easily integrate a VMAX with the likes of OpenStack Cinder, Puppet, Libvirt, or whatever'>
That is all very interesting, but so far it is just a new approach to the UI. It is an excellent approach, but nothing especially clever. One of things I gravitated towards was the white paper about integrating with <a href="http://www.emc.com/collateral/white-papers/h11468-vmax-cloud-edition-wp.pdf">vCloud</a>. Despite it being geared toward VMware (I wonder why? - not!) the principles equally apply to any situation where automation is required. I am a huge DevOps fan (Puppet in particular). Storage arrays have never been particularly automation friendly. In addition to the cloud portal, the VMAX CE also has a RESTful API. Now that is awesome! Here we have the abilty to easily integrate a VMAX with the likes of OpenStack Cinder, Puppet, Libvirt, or whatever you want.
</span></p>

<p>Finally <a href="http://virtualgeek.typepad.com">Chad Sakac</a> informs me that VMAX CE is just the first. EMC intend to roll this management style out to other product lines. Personally I think this would suit both Isilion and Atmos lines very nicely.</p>

<p>I am really excited about this product. It brings a paradigm shift in storage management and automation. Also I am led to believe that the price is exceptional as well, to point that it seems EMC may even be pushing VNX down a market level (to where it should be perhaps?). I have been <a href="http://www.chriscowley.me.uk/blog/2012/12/10/emc-extremio-thoughts/">a bit nasty</a> to EMC in the past, but recently they are doing some stuff that has really got me interested. This and <a href="https://github.com/puppetlabs/Razor">Razor</a> are 2 projects that are definitely worth keeping an eye on.</p>

<iframe width="420" height="315" src="http://www.youtube.com/embed/WoElTAevLDs" frameborder="0" allowfullscreen></iframe>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Linux to Storage]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2013/02/25/the-linux-to-storage/"/>
    <updated>2013-02-25T13:09:00+01:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2013/02/25/the-linux-to-storage</id>
    <content type="html"><![CDATA[<p>Martin &#8220;Storagebod&#8221; Glassborow recently wrote an interesting article where he asked &#8220;Who&#8217;ll do a Linux to Storage?&#8221;. As someone who is equal parts Storage and Linux, the same question runs around my head quite often. Not just that, but how to do it. It is safe to say that all the constituent parts are already in the Open Source Ecosystem. It just needs someone to pull them all together wrap them up in an integrated interface (be that a GUI, CLI, an API or all).</p>

<!--more -->


<p>Linux, obviously, has excellent NFS support. Until recently it was a little lacking in terms of block support. <a href="http://sourceforge.net/projects/iscsitarget/">iSCSI Enterprise Target</a> is ok, but is not packaged for RHEL, which for most shops makes it a big no-no. Likewise <a href="http://stgt.sourceforge.net/">TGT</a> is not bad, I have certainly used it to to good effect, but administering it is a bit like pulling teeth. Additionally, neither are VMware certified and I am pretty sure that TGT at  least is missing a required feature for certification as well (may be persistent reservations). There is a third SCSI target in Linux though: <a href="http://www.linux-iscsi.org/">LIO Kernel Target</a> by Rising Tide Systems. This is a lot newer, but is already VMware Ready certified. Red Hat used it in RHEL6 for FCoE target support, but not for iSCSI. in RHEL7 they will be <a href="http://groveronline.com/2012/11/tgtd-lio-in-rhel-7/">using it for all block storage</a>. It has a much nicer interface than the other targets on Linux, using a very intuitive CLI, nice JSON config files and a rather handy API. Rising Tide are a bit of an unknown however, or at least I thought so. It turns our that both QNAP and Netgear use LIO Kernel Target in their larger devices - hence the VMware certification. In any case, Red Hat are behind it, although I think they are working on a fork of at least the CLI, so I think success is assured there. That solves the problem of block storage, be it iSCSI, Fibre-Channel, FCoE, Infiniband or even USB.</p>

<p>Another important building block in an enterprise storage system is some way of distributing the data for both redunancy and performance. Marin mentions <a href="http://ceph.com/">Ceph</a> which is an excellent system. Personally I would put my money on <a href="http://www.gluster.org/">GlusterFS</a> though. I have had slightly better performance from it. Red Hat bought Gluster about a year ago, and have put some serious development effort into it. As well as POSIX access via Fuse, it has Object storage for use with OpenStack, a native Qemu connector is coming in the next versions. Hadoop can also access it directly. There is also a very good Puppet module for it, which gets around one of Martin&#8217;s critisms of Ceph.</p>

<p>Which brings me nicely to managing this theoretical system. Embedding Puppet in this sort of solution would also make sense. There will be need to a way of keep config sync&#8217;ed on all the nodes (I mentioned that this disruptive product will be scale-out didn&#8217;t I? NO? OK, it will be - prediction for the day). Puppet does this already very well, so why re-invent the wheel.</p>

<p>All this can sit on top of Btrfs allowing each node to have up to 16 exabytes of local storage. For now I am not convinced by it, at least on RHEL 6 as I have seen numerous kernel panics, nor did I have a huge amount of joy on Fedora 17, but there is no doubt that it will get there. Alternatively, there is always the combination of XFS and LVM. XFS is getting on a bit now, but it has been revived by Red Hat in recent years and it is a proven performer with plenty of life left in it yet.</p>

<p>After all that, who do I think is ripe to do some serious disrupting in the storage market? Who will &#8220;Do a Red Hat&#8221; as Martin puts it? Simple: it will be Red Hat! Look at the best of breed tools at every level of the storage stack on Linux and you will find it is either from Red Hat (Gluster) or they are heavily involved (LIO target). They have the resources and the market/mind share to do it. Also they have a long history of working with and feeding back to the community, so the fortuitous circle will continue.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Something from the shadows]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2013/02/21/something-from-the-shadows/"/>
    <updated>2013-02-21T16:06:00+01:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2013/02/21/something-from-the-shadows</id>
    <content type="html"><![CDATA[<p>An intriguing startup came out of stealth mode a few days ago. <a href="http://pernixdata.com/">Pernix Data</a> was founded by Pookan Kumar and Satyam Vaghani, both of who were pretty near top of the pile in VMware&#8217;s storage team.</p>

<!--more -->


<p>What they are offering is, to me at least, a blinding flash of the obvious. It is a softweare layer that runs on a VMware hypervisor that uses local flash as a cache for whatevery is coming off your main storage array. <img class="right" src="http://pernixdata.com/images/home_graphic3.png" width="300" height="217">. That could be an SSD (or multiple) or a PCI-e card.</p>

<p>Reading what they have to say, it is completely transparent to the hypervisor, so everything just works. Obviously me being an Open Source fanatic I imediately started thinking how I could do this with Linux; it took me about 5 minutes.</p>

<p>You take your SAN array and give your LUN to your Hypervisors (running KVM obviously, and with a local SSD). Normally you would stick a clustered file system (such as GFS2) on that shared LUN. Instead you use a tiered block device on top of that LUN. There are two that come immediately to mind: <a href="https://github.com/facebook/flashcache/">Flashcache</a> and <a href="http://sourceforge.net/projects/tier/files/">Btier</a>.</p>

<p>Finally, you can put your clustered file system on that tiered device. I do not have the time or facilities to test this, but I cannot see why it would not work. Maybe someone at Red Hat (seeing as they do the bulk of KVM and GFS2 development) can run with this and see what happens.
What their plans are I do not know. It is very early days, maybe they will be a success maybe not. As they are both ex-VMware, I would not be at all surprised if they get bought back into the VMware fold. Certainly this is a functionality that I would have like to have seen in the past.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How much should you spend on IT]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2013/02/06/how-much-should-you-spend-on-it/"/>
    <updated>2013-02-06T16:04:00+01:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2013/02/06/how-much-should-you-spend-on-it</id>
    <content type="html"><![CDATA[<p>A recent discussion/argument I had on Reddit got me thinking about the cost of solutions we put in.</p>

<p>In an ideal world everything would did have full redundancy, and the customer would never have any downtime. Everything would always be up-to-date and keeping it so would require restarting. The reality is very different unfortunately.</p>

<!--more-->


<p>This potentially rambling post was inspired by someone accusing me of having &#8220;a horrible idea&#8221; because I suggested someone put pfsense on an Atom PC as a VPN router for a small office. He then proceeded to expain to me how you should always buy an expensive black box from a vendor (he didn&#8217;t say black box if I am honest, I am interpreting), how you have to always have support on absolutely everything. I called &#8216;bullshit&#8217; and the whole thing went round in circles a bit until we both realised that were actually singing from the same song sheet, but from different ends of the room.</p>

<p><span class='pullquote-right' data-pullquote='it is always necessary to look at the actual requirements of the end-user'>
When looking at a solution it is always necessary to look at the actual requirements of the end-user. I had a conversation with a Director at $lastjob once. We had recently had a planned outage on the website for a few minutes one Sunday night so I could de-commission the old SAN. He said that he wanted us to get to 99.999% IT uptime. My reply after some quick calculations was that we had actually achieved that for the last 3 years at least, but that I would not like to guarantee it in the future with our current and planned infrastructure. This lead to him asking me to go ahead and do the calculations on how to guarantee it. When I went back to him with my figure (done using lots of Open Source, and no vendor support) he changed his mind. This was in what would be classed as an SME - heading towards £100 million a year turnover and one of world&#8217;s best in their field. Not a small company by any means, but they could not justify that cost.
</span></p>

<p>Having said that they could justify a lot. All our servers were clustered, storage was Fibre-channel, they had a 100TB 8Gb array for a team of 2 people who crunched monster video files all day. All that was justified expenditure, but they were not an internet company, so a bit of downtime could be justified. Even when we had a major disaster and a large swathe of Linux VMs disappeared from this world, nobody actually had to stop working and no money was lost.</p>

<p>A small business is not going to dump the money for multi-thousand pound Cisco router and a zero-contention synchronous internet connection. They may think that they need the best of everything, they may even be willing to pay for it if they have got enough of daddy&#8217;s funding behind them. However that would be foolish, that money would be better spent on giving everyone a Christmas bonus.</p>

<p>Support contracts are another bone of contention. Now everything I have is under one, but that is not always necessary. I once needed to get a couple of TBs of storage into a large office asap. I happenned to have a few FC HBAs, a couple of old Proliants and a pile of MSA1000s in a cupboard. I built up a box with a pair of HBAs and a single MSA1000 and sent the whole lot up to the office with strict instructions that all the extras were for spares only. If something broke, no need for support - just swap it out. I figured it would be good for at least another 3 years. Especially as backups were pretty reliable there. Would a new SAN with expensive support have been more reliable, I doubt it. We would have to wait 4 hours for a new disk, rather than the 5 minutes a took to walk to the cupboard.</p>

<p><span class='pullquote-right' data-pullquote='wisdom can fall at either end of the price-spectrum'>
It is not always necessary to get the shiniest stuff, with the longest/quickest support contract. We know our gear, we know how reliable it is, we know how long it lasts. The people paying the bills do not, they rely on us to advise them honestly and wisely. That wisdom can fall at either end of the price-spectrum, but needs to be based on the ACTUAL risks and their effect.
</span></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[In praise of old school UNIX]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2013/02/05/in-praise-of-old-school-unix/"/>
    <updated>2013-02-05T16:13:00+01:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2013/02/05/in-praise-of-old-school-unix</id>
    <content type="html"><![CDATA[<p>What am I doing today? Documentation that is what. I am writing a document on how to do <a href="http://www.chriscowley.me.uk/blog/2012/11/19/sftp-chroot-on-centos/">this</a>. To any Linux user it is a very simple process and I could just give them a link to my own website.</p>

<!--more-->


<p>I am not writing this for a techinical audience though. The people who are going to perform this work will be the &#8216;Level 1 operatives&#8217;. This translates roughly to &#8220;anyone we can find on the street corners of some Far East city&#8221;. If I tell them to press the red button labelled &#8220;press me&#8221; and it turns out to be orange, they will stop. I cannot assume the ability to edit a file in Vi. How can you work around this, well you need to make everything a copy and paste operation. This is easily done in Bash thanks to IO redirection and of course Sed.</p>

<p>Now, a brief recap may be in order, as there are some perfectly knowledgable Linux users who do not know what Sed is. Really, one of them sits behind me. Sed stands for Stream EDitor, and it parses text and applies transformations to it. It was one of the first UNIX utilities. It kind of sits between <a href="https://en.wikipedia.org/wiki/Grep">Grep</a> and <a href="https://en.wikipedia.org/wiki/AWK_programming_language">Awk</a> and is <a href="http://uuner.doslash.org/forfun/">surprisingly powerful</a>.</p>

<p>Anyway, I need to edit a line in a file then add a block of code at the end.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cp -v /etc/ssh/sshd_config{,.dist}
</span><span class='line'>sed -i ''/^Subsystem/s#/usr/libexec/openssh/sftp-server#internal-sftp#g' \ 
</span><span class='line'>    /etc/ssh/sshd_config</span></code></pre></td></tr></table></div></figure>


<p>First line obviously is a contracted cp line which puts the suffix <em>.dist</em> on the copy.</p>

<p>The basic idea is that it runs through the file (/etc/ssh/sshd_config) and looks for any line that starts with &#8220;Subsystem&#8221; (<code>/^Subsystem/</code>). If it finds a line that matches it then will perform a &#8220;substituion&#8221; (<code>/s#</code>). The next 2 blocks tell it what the substitution will be in the order &#8220;#From#To#&#8221;. The reason for  the change from <code>/</code> to <code>#</code> is because of the / in the path name (thanks to <a href="http://www.reddit.com/user/z0nk">Z0nk</a>  for reminding me that you can use arbitary seperators). The &#8220;#g&#8221; tells Sed to perform the substituion on every instance it finds on the line, rather than just the first one. It is completely superfluous in this example, but I tend to put it in from force of habit. Finally the &#8220;-i&#8221; tells Sed to perform the edit in place, rather than outputing to Stdout.</p>

<p>The next bit is a bit cleverer. With a single command I want to add a block of text to the file.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cat &lt;&lt;EOF | while read inrec; do echo $inrec &gt;&gt; /etc/ssh/sshd_config; done
</span><span class='line'>Match Group transfer
</span><span class='line'>ChrootDirectory /var/local/
</span><span class='line'>ForceCommmand internal-sftp
</span><span class='line'>X11Forwarding no
</span><span class='line'>AllowTcpForwarding no
</span><span class='line'>
</span><span class='line'>EOF</span></code></pre></td></tr></table></div></figure>


<p>Here <code>cat &lt;&lt;EOF</code> tells it send everything you type to Stdout until it sees the string EOF. This then gets piped to a <code>while</code> loop that appends each line of that Stdout to the file we want to extend (<em>/etc/ssh/sshd_config</em> in this case).</p>

<p>Using these old tools and a bit of knowledge of how redirection works has enabled me to make a document that anyone who can copy/paste can follow. It is very easy for technical people to forget that not everyone has the knowledge we have. To us opening Vi is perfectly obvious, but to others maybe it isn&#8217;t and they are not being paid enough to know. They are just being paid to follow a script. I may not like it, but it is the case - it also helped turn a boring documentation session into something a little more interesting. Which is nice!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Trac with Nginx on Centos]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2013/01/21/trac-with-nginx-on-centos/"/>
    <updated>2013-01-21T11:53:00+01:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2013/01/21/trac-with-nginx-on-centos</id>
    <content type="html"><![CDATA[<p><a href="tac.edgewall.org">Trac</a> is an excellent online project management tool. Nginx us a great web server. Surely the combination of the two should be a match made in heaven. I would say so, although there are a couple problems. The easiest way to deploy Trac is on Apache using mod_wsgi or FastCGI. This option does not really exist for Nginx. Okay, it can do FastCGI, but I have not get that set up on my server.</p>

<!-- more -->


<p>What Nginx does do extremely well is Proxy stuff. Use this with Trac&#8217;s built in web server (tracd) and you have  a very nice little set up. Ruby people, before you get all upset, Trac&#8217;s server is not just for dev (a la WEBrick). It is fine for production use.</p>

<p>The first thing to do (obviously) is to install Trac. I get it from Pip, although a slightly older version is in <a href="http://fedoraproject.org/wiki/EPEL">EPEL</a> if you prefer to stick with native packages.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sudo yum install python-setuptools
</span><span class='line'>sudo pip install Trac
</span><span class='line'>sudo useradd -r -d /var/local/trac trac
</span><span class='line'>sudo -u trac trac-admin /var/local/trac/trac.example.com initenv
</span><span class='line'>sudo -u trac htpasswd -c /var/local/trac/.htpasswd example-realm chris</span></code></pre></td></tr></table></div></figure>


<p>Answer the little question it asks you - this includes the VCS you want to use, but that is for another day. I use the TracGit plugin to link to my Git repos.</p>

<p>Create an init script (<code>/etc/init.d/trac</code>) that contains something like:</p>

<div><script src='https://gist.github.com/4586539.js?file='></script>
<noscript><pre><code></code></pre></noscript></div>


<p>Start up the Trac daemon and enable it on boot up</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sudo chmod +x /etc/init.d/trac
</span><span class='line'>sudo chkconfig trac on
</span><span class='line'>sudo /etc/init.d/trac start</span></code></pre></td></tr></table></div></figure>


<p>Open it up and have a look at least. You will probably want to modify it to use your authentication realm.</p>

<p>Now you need to configure Nginx. I use the package from <a href="http://fedoraproject.org/wiki/EPEL">EPEL</a>. Enable that if you have not already and run</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sudo yum install nginx</span></code></pre></td></tr></table></div></figure>


<p>Create the file <code>/etc/nginx/conf.d/trac.conf</code> containing something like:</p>

<div><script src='https://gist.github.com/4586630.js?file='></script>
<noscript><pre><code></code></pre></noscript></div>


<p>Finally enable Nginx on boot up and start it:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sudo chkconfig nginx on
</span><span class='line'>sudo server nginx start</span></code></pre></td></tr></table></div></figure>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What will HP do next?]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2012/12/17/what-will-hp-do-next/"/>
    <updated>2012-12-17T15:16:00+01:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2012/12/17/what-will-hp-do-next</id>
    <content type="html"><![CDATA[<p>HP recently announced a new range of 3Par based arrays that are aimed at mid-range enterprise. There now appear to be 2 ranges for the future:</p>

<!--more -->


<ul>
<li>HP StoreServ 10000 is the big boy, scales up to 1.6PB, 192 FC ports, 32 10Gb iSCSI - the works.</li>
<li>HP StoreServ 7000 is the mid-range one, with <em>only</em> 24 FC and 8 1-Gb iSCSI. This split into the 7200 (2U) and 7400 (4U)</li>
</ul>


<p>With the entry level <a href="http://www8.hp.com/us/en/hp-news/press-release.html?id=1332554#.UM8Mm3eTW01">7200 starting at $20k</a> that does not leave a lot of room at the low end for both the P4000 and the P2000 ranges. At the higher end the 7400 starts at $32k, which certainly leaves no space for the venerable EVA.</p>

<p>In <a href="http://h30507.www3.hp.com/t5/Around-the-Storage-Block-Blog/Blogger-Q-amp-A-with-David-Scott/ba-p/128097">an interview with Around the Storage Block</a> HP Storage GM David Scott is quite critical of EMC who have a range of different and fairly unrelated product lines (Atmos, VMAX, VNX/VNXe, Isilion). For now HP is fairly similar: P9000 (Hitachi), P4000 (Lefthand, P2000 (Dot Hill). When you look at where they have priced the 3Par gear, it does appear that they are betting the farm on it.</p>

<p>Something I have been quite vocal about over the last 5 years or so is that fact that HP&#8217;s storage portfolio is all over the place. Compared to Netapp, who have a very homogenous portfolio (everything runs OnTAP, you know one Netapp product, you can jump on to the rest), HP have got a one interface for P2000, another for P4000, another for EVA. Nothing sits together. HP needs to get all this in line. EMC have already started with Unisphere, but they still have multiple product architectures (VMAX, VNX, Isilion for example).</p>

<p>I personally think that these other ranges will drop by the wayside, although I am reading a bit between the lines here. Dot Hill do seem to be setting themselves up to be more than just an OEM supplier to HP. Maybe it is wishful thinking as I am a <a href="http://www.chriscowley.me.uk/blog/2010/01/12/some-great-new-san-gear/">huge fan of Dot Hill</a>, but they have some very interesting products. I hope/expect to see a lot more of Dot Hill themselves over the next few years, rather than just being behind Oracle/Netapp/HP badges.</p>

<p>The P9000 range is a similar story at the other end of the market. The Storserve 10k seems to be very similar, pretty much the same capacity and number of ports. Feature set is also close enough. I also have the impression that Hitachi are starting to push a bit harder in their own right as well.</p>

<p>Essentially I think 3Par will become HP&#8217;s own architecture. It has the flexibity to  cover everything from a single bay with 12 SATA disks at the low end (perhaps on DL hardware) all the way up to PB+ scales, taking in all-flash on the way.</p>

<p>This leaves the P4000, which has been re-branded the <a href="http://www8.hp.com/us/en/products/disk-storage/product-detail.html?oid=4118659">StoreVirtual 4000</a>. This seems to me to be a no-brainer. It is already running on commodity DL180 hardware and includes an appliance option. My guess is the physical implementation of this will be phased out. It will become the Virtual Appliance front-end to all this new 3Par based physical goodness.</p>

<p>Finally, I have skipped over the EVA. What does the future hold in store for HP&#8217;s venerable high-end platform. I think nothing. It will go into maintenance mode and be quietly end-of-life&#8217;d. Existing customers will be pushed to <a href="http://h30507.www3.hp.com/t5/Around-the-Storage-Block-Blog/EVA-to-HP-3PAR-StoreServ-online-import/ba-p/128391">migrate</a> over to Storeserv 7000.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[EMC ExtremIO Thoughts]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2012/12/10/emc-extremio-thoughts/"/>
    <updated>2012-12-10T16:39:00+01:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2012/12/10/emc-extremio-thoughts</id>
    <content type="html"><![CDATA[<p>There has been quite a bit of musing recently on the web about EMC and what will come out of their ExtremIO acquisition. They have recently (finally) started demonstrating an all-flash array. The name says it all: ExtremIO. It is for super high IOPS applications - Virtual desktops, enormous DBs, that sort of thing.</p>

<!-- more -->


<p>It is a bit of a depature from traditional EMC, in that it <a href="http://storagenewsletter.com/news/systems/all-ssd-system-from-emc-xtremio-">appears</a> that it is going to be a true scale-out architecture. This is has more in common with Isilion (not developed at EMC) than VMAX (developed at EMC).</p>

<p>The problem is that EMC are <em>extremely</em> late to the market this time around. VMAX was ahead of the curve by adding flash. In the all flash arena there are several options already there, <a href="http://violin-memory.com">Violin</a>, <a href="http://whiptail.com/">Whiptail</a> spring straight to mind.</p>

<p>Over at <a href="http://blog.thestoragearchitect.com/2012/12/10/xtremio-aka-project-x-wheres-the-innovation/">The Storage Architect</a> Chris Evans gives the standard counter-arguments to EMC&#8217;s marketing spin. Namely:</p>

<ol>
<li>Other vendor solutions aren&#8217;t as resilient</li>
<li>It&#8217;s a 1.0 product, expect more from 2.0 and beyond</li>
<li>It gives our customers choice</li>
</ol>


<p>I hate to say it, but EMC have one HUGE advantage over all the startups. Quite simply they are EMC! As experts we know that Violin (for example) have a more mature product than EMC do.</p>

<p><span class='pullquote-right' data-pullquote='When the guy with the credit card sees the name &#8220;EMC&#8221; it will be hard to persuade him that such a mature brand has the more immature product'>
When the guy with the credit card sees the name &#8220;EMC&#8221; it will be hard to persuade him that such a mature brand has the more immature product. This won&#8217;t be the case everywhere, but in a lot of large enterprise they would go to their storage experts (like me) and ask for advice on which flash array to go for; they then stipulate that it has to come from EMC&#8217;s portfolio. At which point I through my hands up in despair, tell the to buy ExtremIO, the guy who has the better, more mature, solution loses the business that was rightfully theirs.
</span></p>

<p>It is not a 1.0 product, I can not accept that EMC acquired ExtremIO based on stuff that was only on paper. At best this is a 1.5 product, but realistically it is a 2.0 product. From a company with the resources of EMC, this should be coming out of the blocks running - it should be the best in class. OK, ExtremIO were further from version 1.0 than EMC were maybe lead to believe at the time, but they have got a lot of resources. After a year, they should not be in damage control mode.</p>

<p>Does it really give the customers choice? I would go one step further than what Chris has said - that an all-flash VMAX or VNX would have made sense. I agree with him, but I also think that they have actually removed choice.</p>

<p>I would say that EMC have cocked-up here. They under-estimated the market for all-flash arrays. Even my <a href="http://www.violin-memory.com/news/press-releases/nats-selects-violin-memory-flash-storage-for-virtual-desktop-infrastructure/">old employer have got some</a> and that is in Air Traffic control - there is no-one else who relies more on &#8220;tried and tested&#8221; technology than them. They then rushed to through some money at the problem, but like of Violin were already happy where they were.</p>

<p><span class='pullquote-right' data-pullquote='I wish that EMC would be punished for this'>
Robin Harris at <a href="http://storagemojo.com/2012/12/05/emcs-xtreme-embarrassment/">StorageMojo</a> thinks this will be a costly mistake for EMC. I disagree, I think by announcing that this is coming EMC will stall the market and thus come out of this fine. Unfortunately there is too much latency in the enterprise storage space for it to be otherwise. I wish it were a bit more dynamic and I wish that EMC would be punished for this, thus rewarding one of the underdogs. That does not happen enough in the enterprise space, especially for an Englishman like me (we do love the underdogs).
</span></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SFTP Chroot on CentOS]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2012/11/19/sftp-chroot-on-centos/"/>
    <updated>2012-11-19T19:59:00+01:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2012/11/19/sftp-chroot-on-centos</id>
    <content type="html"><![CDATA[<p>This came up today where I needed to give secure file transfer to customers. To complicate things I had to use an out-of-the-box RHEL6 system. The obvious answer was to use SSH and limit those users to SFTP only. Locking them into a chroot was not a requirement, but it seemed like a good idea to me. I found plenty of docs that got 80% of the way, or took a shortcut, but this should be complete.</p>

<!-- more -->


<p>The basic steps are:</p>

<ol>
<li>Create a group and the users to that group</li>
<li>Modify the SSH daemon configuration to limit a group to sftp only</li>
<li>Setup file system permissions</li>
<li>Configure SELinux</li>
<li>Test (of course)</li>
</ol>


<p>Without further ado, lets get started. It should only take about 10 minutes, nothing here is especially complex.</p>

<p>Create a group that is limited to SFTP only and a user to be in that group.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>groupadd sftponly
</span><span class='line'>useradd sftptest
</span><span class='line'>usermod -aG sftponly  sftptest</span></code></pre></td></tr></table></div></figure>


<p>Now you need to make a little change to <code>/etc/ssh/sshd_config</code>. There will be a <em>Subsystem</em> line for <code>sftp</code> which you need to change to read:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Subsystem       sftp    internal-sftp</span></code></pre></td></tr></table></div></figure>


<p>Now you need to create a block at the end to limit members of a group (ie the sftponly group you created above) and chroot them. Simply add the following to the end of the file:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Match Group sftponly
</span><span class='line'>    ChrootDirectory %h
</span><span class='line'>    ForceCommand internal-sftp
</span><span class='line'>    X11Forwarding no
</span><span class='line'>    AllowTcpForwarding no</span></code></pre></td></tr></table></div></figure>


<p>These changes will require a reload of the SSH daemon: <code>service sshd reload</code></p>

<p>Now you need to make some file permission changes. For some reason which I cannot work out for now, the home directory must be owned by root and have the permissions 755. So we will also need to make a folder in the home directory to upload to and make that owned by the user.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sudo -u sftptest mkdir -pv /home/sftptest/upload
</span><span class='line'>chown root. /home/sftptest
</span><span class='line'>chmod 755 /home/sftptest
</span><span class='line'>chgrp -R sftponly /home/sftptest</span></code></pre></td></tr></table></div></figure>


<p>The last thing we need to do is tell SELinux that we want to upload files via SFTP to a chroot as it is read-only by default. Of course you are running SELinux in enforcing mode aren&#8217;t you :)</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>setsebool -P ssh_chroot_rw_homedirs on</span></code></pre></td></tr></table></div></figure>


<p>Now from another console you can sftp to your server</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sftp sftptest@&lt;server&gt;</span></code></pre></td></tr></table></div></figure>


<p>You should then be able to put a file in your upload folder. However if you try to ssh to the server as the user <em>sftptest</em> it should tell you to go away. Of course you should be able to <em>ssh</em> as your normal user with no problem. Pro tip: make sure to leave a root terminal open just in case.</p>

<p>Required reading:</p>

<ul>
<li><a href="http://wiki.centos.org/HowTos/SELinux">CentOS Wiki SELinux</a></li>
<li><a href="http://wiki.centos.org/TipsAndTricks/SelinuxBooleans">CentOS Wiki SELinuxBooleans</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Installing Adito/OpenVPN-ALS on CentOS]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2012/10/26/installing-adito-slash-openvpn-als-on-centos/"/>
    <updated>2012-10-26T21:19:00+02:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2012/10/26/installing-adito-slash-openvpn-als-on-centos</id>
    <content type="html"><![CDATA[<p><a href="http://sourceforge.net/projects/openvpn-als">OpenVPN-ALS</a>, formerly known as Adito, is not to be confused with <a href="http://www.openvpn.net">OpenVPN</a>. They both brilliant tools that work in completely different things, but in a similar way. Confused? Excellent&#8230;</p>

<!-- more -->


<p>OpenVPN-ALS (from now on known as Adito, because I find it less confusing) is a browser based SSL VPN that enables you to acess resources on your own network, even if you are behind a restrictive proxy and/or firewall.</p>

<p>First you need a basic install of CentOS. The absolute base system is plenty. One thing to note is that to get the best from this it cannot share space with another web server as it takes up port 443. Make sure Apache/Nginx et al are not installed.</p>

<p>The next step is to install a couple of essentials. OpenVPN-ALS is a java applications, so obviously you need a JRE (in fact you need a JDK), plus it uses Ant for building. The Adito project work purely in branches, to the trunk should be stable.</p>

<p>First get <a href="www.oracle.com">Oracle Java</a> and install it. You can use the instructions <a href="http://www.if-not-true-then-false.com/2010/install-sun-oracle-java-jdk-jre-6-on-fedora-centos-red-hat-rhel/">here</a> to help you. You will need to configure <code>javac</code> and <code>jar</code> as well.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sudo yum install subversion ant </span></code></pre></td></tr></table></div></figure>


<p>Just to be sure run <code>sudo update-alternatives -config java</code> to make sure you are using the latest one:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[chris@adito ~]$ sudo update-alternatives --config java
</span><span class='line'>
</span><span class='line'>There is 3 program that provides 'java'.
</span><span class='line'>
</span><span class='line'>  Selection    Command
</span><span class='line'>-----------------------------------------------
</span><span class='line'> + 1           /usr/lib/jvm/jre-1.5.0-gcj/bin/java
</span><span class='line'>   2           /usr/lib/jvm/jre-1.6.0-openjdk.x86_64/bin/java
</span><span class='line'>*  3           /usr/java/jdk1.7.0_07/jre/bin/java
</span><span class='line'>
</span><span class='line'>Enter to keep the current selection[+], or type selection number: 3  </span></code></pre></td></tr></table></div></figure>


<p>Now check out the current trunk:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sudo svn co https://openvpn-als.svn.sourceforge.net/svnroot/openvpn-als/adito/trunk /opt/openvpn-als</span></code></pre></td></tr></table></div></figure>


<p>Adito needs the tools.jar file that is bundles with the JDK, so copy that into place. You can then go ahead and build.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sudo cp /usr/java/jdk1.7.0_07/lib/tools.jar /opt/openvpn-als/adito/lib/
</span><span class='line'>cd /opt/openvpn-als
</span><span class='line'>sudo ant install</span></code></pre></td></tr></table></div></figure>


<p>This will generate a lot of output, but will eventually print something like:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'> [java] Starting installation wizard........................Point your browser to http://adito.chriscowley.local:28080. 
</span><span class='line'> [java] 
</span><span class='line'> [java] Press CTRL+C or use the 'Shutdown' option from the web interface to leave the installation wizard.</span></code></pre></td></tr></table></div></figure>


<p>Go to the address it gives you and work your way through the wizard. At the end it will exit and tell you to restart the service.</p>

<p>You can return to your console and run</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sudo ant install-agent
</span><span class='line'>sudo ant install-service
</span><span class='line'>sudo /etc/init.d/adito start
</span><span class='line'>sudo chkconfig adito on</span></code></pre></td></tr></table></div></figure>




<iframe class="imgur-album" width="100%" height="550" frameborder="0" src="http://imgur.com/a/yIVhT/embed"></iframe>


<p>You can now log into it, but it will not do much as there are no applications installed. You need to check them out of Subversion, compile and upload them. You can do this on your local machine.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>svn co https://openvpn-als.svn.sourceforge.net/svnroot/openvpn-als/adito-applications/
</span><span class='line'>cd adito-aplications</span></code></pre></td></tr></table></div></figure>


<p>There are quite a few there, but we will just do the portable Putty application.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd adito-application-putty-portable-ssh
</span><span class='line'>ant</span></code></pre></td></tr></table></div></figure>


<p>The output will tell you the Zip file it has built which you can now upload. Go to the &#8220;Extension Manger&#8221; from the menu on the left. On the right you wil see &#8220;Upload Extension&#8221;. Choose the Zip file and you can configure it to connect to whatever Linux machine you want. &#8220;Putty SSH&#8221; will now be available in the list of installed applications.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Cycling Related Review]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2012/10/23/a-cycling-related-review/"/>
    <updated>2012-10-23T20:31:00+02:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2012/10/23/a-cycling-related-review</id>
    <content type="html"><![CDATA[<p><img class="right" src="http://www.arkon.com/images/slimgrip/sm532.jpg" width="300" height="300"> A little bit if a move away from computers and related subjects today. I was recently contacted by <a href="http://www.gearzap.com">Gearzap</a>/<a href="http://www.mobilefun.co.uk">MobileFun</a> who had seen that I had a Raspberry Pi. They asked if I would like to review a case and I cheekily asked if they could also give me something to mount my phone on the handlebars of my bike. Much to my pleasant surprise, they said yes.</p>

<!-- more -->


<p>The sent me the <a href="http://www.arkon.com/slimgrip/slimgrip-bike-mount.html">Arkon SM532</a>. Unsurprisingly it is made in China, but then what is not nowadays. Having said that, the build quality is pretty good and the instructions pretty clear.</p>

<p>I mounted it on my beloved <a href="http://flic.kr/p/cDCcFd">Felt</a> road bike as it is what I am currently using day to day. This unfortunately highlighted the only real problem I found. Modern road bikes have thicker handle bars than many other types of bike. This meant that fitting the bracket to them was tight. It comes with several rubber shims, that a) protect you handlebars, b) stop it from slipping around. It was not possible to use those. It has to be said that is pretty ugly as well, but if that worries you then buy a Garmin.</p>

<p>Anyway, I persevered and attached it without the shims and prayed that it would not do any damage. As I said, the instructions were very clear and this only took a few minutes.  Once on it all seemed perfectly secure, so the next day I put my phone in it. The clips keep it in place pretty well and the you also have the bungie cord as an extra, so I was not worried it would fall out at all. The route I took is a collection of French farm lanes. Some are very smooth, but some are horrible. Also, it is harvest time, so there is plenty of dried mud on the road to give it a bit more of a shaking. It held up very well, nothing shifted at all and I could easily angle the phone so it was just right.</p>

<p>Later I took it off to ensure that it would fit properly on a &#8220;normal&#8221; sized handlebar by putting it on my mountain bike - it was fine. Also, there was absolutely no scratches on my Felt&#8217;s bars at all, despite my concerns about not having any shims to protect them.</p>

<p>All in all it is a pretty good piece of kit. It can be had from <a href="http://www.mobilefun.co.uk">MobileFun</a> for £13.95, which strikes me as pretty good value.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[My new job]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2012/10/01/my-new-job/"/>
    <updated>2012-10-01T22:16:00+02:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2012/10/01/my-new-job</id>
    <content type="html"><![CDATA[<p>I have now got new challenges and am designing much bigger systems. Whereas before I would have take one of these:
<img class="center" src="http://www.chriscowley.me.uk/images/p2000-g3-sff.png" width="250" height="400"></p>

<!-- more -->


<p>Plugged it into a pair of these:
<img class="center" src="http://www.chriscowley.me.uk/images/silkworm.jpg" width="250" height="450">
And finally plugged in a pair of these:
<img class="center" src="http://www.chriscowley.me.uk/images/dl380g7.jpg" width="200" height="400"></p>

<p>Now I do not actually do the plugging in - that is not part of the documented process. Also, the system I am designing is on the other side of the world. However I now take one of <a href="http://uk.emc.com/storage/symmetrix-vmax/vmax-20k.htm" target="_blank">these</a>, a couple of <a href="http://www.cisco.com/en/US/products/hw/ps4159/ps4358/ps5395/index.html" target="_blank">these</a> and add a couple of racks of <a href="http://www.cisco.com/en/US/products/ps10265/index.html" target="_blank">these</a>. Finally it all plugs into a couple of <a href="http://www.cisco.com/en/US/products/ps10098/index.html" target="_blank">these</a>.</p>

<p>True I am not as hand-ons as I would like to be (for now). However, I am getting exposed to a the real big boys kit with systems that are used by 100k+ users, as opposed to 100 users. The challenge is very real, but exciting. As we quite often said at Snell:</p>

<p>&#8220;Challenge accepted&#8230;&#8221;</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[My new life]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2012/10/01/my-new-life/"/>
    <updated>2012-10-01T22:06:00+02:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2012/10/01/my-new-life</id>
    <content type="html"><![CDATA[<p>I am now a month in to life in France. Make no mistake I am so far very happy that we have made the right decision, even though not everything is perfect. The biggest beef is without a doubt the paperwork!</p>

<!-- more -->


<p>For example, it took us a month to get a phone line and the stumbling block was not having the right piece of paper. To get onto the system you have to have a bill, but to get a bill you need a bill. Call it a catch-22 or a chicken-egg take your pick, but french beaurocrats love that.</p>

<p>Working in France is a little different to England. Add to that the fact that I am going from a <a href="http://www.snellgroup.com" target="_blank">small company</a> to working on a project for a <b>much</b> <a href="http://www.orange-business.com" target="_blank">larger one</a> and you have quite a culture shock.</p>

<p><span class='pullquote-right' data-pullquote='All my colleagues take a 2 hour lunch'>
The attitude to lunch is probably the biggest single change of all that can only be attributed to country. All my colleagues take a 2 hour lunch religiously, during which they go elsewhere. I am used to taking a 30 minute break to read the Register with a sandwhich/salad at my desk. For now, past an hour, I get bored an return to work. Maybe I will extend my lunch as I get used to it, but maybe not - shorter lunch means I get to go home earlier.
</span></p>

<h2>Commuting</h2>

<p>My commute is quite a lot further than when I was at Snell:</p>

<iframe width="500" height="300" scrolling="no" frameborder="no" src="https://www.google.com/fusiontables/embedviz?viz=MAP&amp;q=select+col2+from+1J0AXs2Oyzs-J9ChL5U7hgKWkHX-HimQZ699VSO4&amp;h=false&amp;lat=50.820603567709554&amp;lng=-1.011776909545925&amp;z=13&amp;t=1&amp;l=col2"></iframe>


<p>That was quite a nice route once you got to Langston harbour. What I do now is very different. For a start I have not cycled all the way yet. Partly because it it a lot further, but also because trains are a <em>lot</em> cheaper here. I pay 80 euros for a month, half of which gets re-imbursed. When your train costs less than 2 euros a day, it does not even make sense to take a car.</p>

<p>The actual cycling is a lot more pleasant as well. I live in a hamlet, and my route take me past the <em>le lac tranquille</em> (the calm lake). In the morning it gets very cold and misty, which is very nice.</p>

<p><a href="http://imgur.com/Trejw"><img src="http://i.imgur.com/Trejw.jpg" title="Hosted by imgur.com" alt="" /></a></p>

<p>Drivers are about the same. The vast majority give me plenty of space, but obviously you get the odd idiot. Just today someone shouted &#8220;Je vais te touer&#8221; so some things are pretty universal it would appear.</p>

<p>I have also found that the paint on the roads (at least in Rennes) is more slippery in the wet. The simple answer has been to avoid the cycle lanes - no real change there then.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[I have a Raspberry Pi]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2012/08/04/i-have-a-raspberry-pi/"/>
    <updated>2012-08-04T21:47:00+02:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2012/08/04/i-have-a-raspberry-pi</id>
    <content type="html"><![CDATA[<p>Just got my new toy! I have to admit that I am not completely certain what I will do with it yet though.</p>

<!-- more -->


<p>It will certainly start by running <a href="http://mpd.wikia.com/wiki/Music_Player_Daemon_Wiki" target="_blank">Music Player Daemon</a>.</p>

<p><a href="http://www.flickr.com/photos/chriscowleyunix/7705281518/" title="Raspberry Pi by chriscowleysound, on Flickr"><img src="http://farm9.staticflickr.com/8023/7705281518_73c2ee8c18.jpg" width="500" height="375" alt="Raspberry Pi"></a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Isle of Wight Ride]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2012/07/27/isle-of-wight-ride/"/>
    <updated>2012-07-27T16:02:00+02:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2012/07/27/isle-of-wight-ride</id>
    <content type="html"><![CDATA[<p>Last week I successfully did my first 65 mile ride. Of course it was not a race, but that never stopped a group of men being over competitive - I won by the way.</p>

<!-- more -->


<p>I do not own a cycle computer, nor did I remember to run My Tracks. However, a colleague did record it on his Garmin GPS:</p>

<iframe width='465' height='548' frameborder='0' src='http://connect.garmin.com:80/activity/embed/203587506'></iframe>




<object width="400" height="300"> <param name="flashvars" value="offsite=true&lang=en-us&page_show_url=%2Fphotos%2F83132329%40N04%2Fsets%2F72157630749918206%2Fshow%2F&page_show_back_url=%2Fphotos%2F83132329%40N04%2Fsets%2F72157630749918206%2F&set_id=72157630749918206&jump_to="></param> <param name="movie" value="http://www.flickr.com/apps/slideshow/show.swf?v=109615"></param> <param name="allowFullScreen" value="true"></param><embed type="application/x-shockwave-flash" src="http://www.flickr.com/apps/slideshow/show.swf?v=109615" allowFullScreen="true" flashvars="offsite=true&lang=en-us&page_show_url=%2Fphotos%2F83132329%40N04%2Fsets%2F72157630749918206%2Fshow%2F&page_show_back_url=%2Fphotos%2F83132329%40N04%2Fsets%2F72157630749918206%2F&set_id=72157630749918206&jump_to=" width="400" height="300"></embed></object>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Mirror a Subversion repo with svnsync]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2012/07/17/mirror-a-subversion-repo-with-svnsync/"/>
    <updated>2012-07-17T12:03:00+02:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2012/07/17/mirror-a-subversion-repo-with-svnsync</id>
    <content type="html"><![CDATA[<p>The basic idea is that everytime a change happens on the master, it gets pushed to the slave. In this set up it will <em>not</em> get you any more capacity; you cannot commit back to the slave. If you do it will get out of sync, resulting in a <em>split brain</em> situation. This is what we sysadmins call a &#8220;bad thing&#8221;. I am doing this in order to have <a href="http://www.atlassian.com/software/fisheye/overview" target="_blank">Atlassian Fisheye</a> can scan the repository without having to go over the network. The basic layout is:</p>

<!-- more -->


<p><img class="center" src="http://www.chriscowley.me.uk/images/svnsync.png"></p>

<p>First the master needs to be able to send the data to the slave without any user interaction. On both the slave create a user to perform the sync:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>useradd svnsync</span></code></pre></td></tr></table></div></figure>


<p>On the master, I use https to access my repositories. This means that all my hook scripts run as the apache user. Change to that user with <code>sudo su -s /bin/bash - apache</code></p>

<p>Create an ssh key-pair (<code>ssh-keygen</code>) and add the contents of <code>~apache\.ssh\id_rsa.pub</code> on the master to <code>~svnsync/.ssh/authorized_keys2</code> on the slave.</p>

<p>Now you can push push data to the slave without a password. Test it by running:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ssh svnsync@&lt;slave&gt;</span></code></pre></td></tr></table></div></figure>


<h2>On Slave</h2>

<p>Create a new repository to store what gets pushed to it</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>svnadmin create --fs-type=fsfs /var/local/svnsync/&lt;reponame&gt;
</span><span class='line'>chown -Rv svnsync:svnsync /var/local/svnsync/&lt;reponame&gt;</span></code></pre></td></tr></table></div></figure>


<p>The process will need to make modifications to the properties, so you need to enable that. Put the following into <code>/var/local/svnsync/&lt;reponame&gt;/hooks/pre-revprop-change</code></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#!/bin/sh
</span><span class='line'>USER="$3"
</span><span class='line'>
</span><span class='line'>if [ "$USER" = "svnsync" ]; then exit 0; fi
</span><span class='line'>
</span><span class='line'>echo "Only the svnsync user can change revprops" &gt;&2
</span><span class='line'>exit 1</span></code></pre></td></tr></table></div></figure>


<p>Finally make it executable with</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>chmod +x /var/local/svnsync/&lt;reponame&gt;/hooks/pre-revprop-change</span></code></pre></td></tr></table></div></figure>


<h2>On Master</h2>

<p>First initialize the transfer and do the initial population. Do all this as the apache user again.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>svnsync init --username svnsync \
</span><span class='line'>    svn+ssh://svnsync@&lt;slave&gt;/var/local/svnsync/&lt;reponame&gt; \
</span><span class='line'>    file:///var/svn/&lt;reponame&gt;</span></code></pre></td></tr></table></div></figure>


<p>Now we need to configure the Master repo to push all changes to the slave. Create a post-commit hook script containing</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#!/bin/bash
</span><span class='line'>svnsync --username svnsync --non-interactive sync \
</span><span class='line'>    svn+ssh://svnsync@&lt;slave&gt;/var/local/svnsync/&lt;reponame&gt;</span></code></pre></td></tr></table></div></figure>


<p>Finally create another hook script to keep revision properties in sync in <code>/var/svn/&lt;reponame&gt;/hooks/post-revprop-change</code></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#!/bin/bash
</span><span class='line'>REPOS="$1"
</span><span class='line'>REV="$2"
</span><span class='line'>USER="$3"
</span><span class='line'>PROPNAME="$4"
</span><span class='line'>ACTION="$5"
</span><span class='line'>
</span><span class='line'>svnsync --username svnsync --non-interactive copy-revprops \
</span><span class='line'>    svn+ssh://svnsync@&lt;slave&gt;/var/local/svnsync/&lt;reponame&gt; $REV && \
</span><span class='line'>    exit 0</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
</feed>
