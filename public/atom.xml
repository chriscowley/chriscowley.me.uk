<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Just Another Linux Blog]]></title>
  <link href="http://www.chriscowley.me.uk/atom.xml" rel="self"/>
  <link href="http://www.chriscowley.me.uk/"/>
  <updated>2013-12-16T11:56:15+01:00</updated>
  <id>http://www.chriscowley.me.uk/</id>
  <author>
    <name><![CDATA[Chris Cowley]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Integrating RHEL with Active Directory]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2013/12/16/integrating-rhel-with-active-directory/"/>
    <updated>2013-12-16T09:52:00+01:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2013/12/16/integrating-rhel-with-active-directory</id>
    <content type="html"><![CDATA[<p>I had a request on Reddit to share a document I wrote about connect Red Hat Enterprise Linux with Active Directory. The original document I wrote is confidential, but I said I would write it up.</p>

<p>This works for both Server 2008(R2) and 2012. If I recall correctly it will also work with 2003, but may need to minor terminology changes on the Windows side. From the Linux side, it should be fine with RHEL 6 and similar (CentOS and Scientific Linux). It should also apply to Fedora, but your mileage may vary.</p>

<!-- more -->


<p>So without further ado, let&#8217;s dive in. To do this you need to know what is actually happening under the surface when you authenticate to AD from a client. The basic idea looks something like this:</p>

<p><img class="center" src="https://docs.google.com/drawings/d/1tjaacfXrTJtOZCREonoXzdHfgZQHssQ2zkDzFpLGeX0/pub?w=960&h=720"></p>

<p>Integration with AD requires the installation of a few services in Red Hat, along with some minor modifications on the Windows Domain Controllers. On the Linux side, everything revolves around the System Security Services Daemon (SSSD). All communication between the PAM and the various possible back-ends is brokered through this daemon. This is only one solution, there are several. The others involve Winbind (which I have found problematic), or LDAP/Kerberos directly (no offline authentication, more difficult to set up). Note that this does not give you any file sharing, but this can easily be extended to do so using Samba.</p>

<p>PAM communicates with SSSD, which in turn talks to Active Directory via LDAP and Kerberos. Identification is performed via LDAP, with the user is authenticated using Kerberos.
These different components have some prerequisites on Windows.</p>

<ul>
<li>DNS must be working fully - both forward and reverse lookups should be functional. If the Kerberos server (Windows Domain Controller cannot identify the client via DNS, Kerberos will fail.</li>
<li>Good time is essential – if the two systems have too larger difference in time, Kerberos will fail.</li>
<li>The Active Directory needs to be extended to include the relevant information for *NIX systems (home directory, shell, UUID/GUID primarily).

<ul>
<li>They are actually there, but empty and uneditable. The necessary GUI fields are part of “Identity Management for UNIX”</li>
</ul>
</li>
<li>It must be possible for the Linux client to perform an LDAP search. This could be either via an anonymous bind or authenticated.

<ul>
<li>Anonymous is obviously not recommended.</li>
<li>Simple binds (username/password) do work but are not recommended. Although I am not one to practise what I preach (see below).</li>
<li>The best option is SASL/GSSAPI, using a keytab generated by Samba. This does not require Admin privileges on Windows, only permissions to join computers to the domain.</li>
</ul>
</li>
</ul>


<h1>Prerequisites</h1>

<p>In order for the kerberized network to be available, both DNS and NTP must be configured for all servers and clients. In this guide, we will use the Active directory DNS and NTP server. DNS and NTP configuration for the server are not covered here.</p>

<h1>Preparing Active Directory</h1>

<p>In Server Manager, add the Role Service &#8220;Identity Management for UNIX&#8221;. This is under the Role &#8220;Active Directory Domain Services&#8221; (took me a while to find that). When it asks, use your AD domain name as the NIS name. For example, with a AD domain of <em>chriscowley.lab</em>, use <em>chriscowley</em>.</p>

<p>Once that is installed, create a pair of groups. For the sake of argument, lets call them <em>LinuxAdmin</em> and <em>LinuxUser</em>. The intended roles of these 2 groups is left as an exercise for the reader. When you create these groups, you will see a new tab in the properties window for both groups and users: &#8220;UNIX Attributes&#8221;.</p>

<p>Now go ahead and create a user (or edit an existing one). Go into the UNIX tab and set the configure the user for UNIX access: <img class="right" src="http://i.imgur.com/Ox9kuAy.png"></p>

<ul>
<li>Select the NIS domain you created earlier</li>
<li>Set an approprate UUID (default should be fine)</li>
<li>Set the login shell as <code>/bin/bash</code>, <code>/bin/sh</code> should be fine most of the time, but I have seen a few odd things happen (details escape me)</li>
<li>Set the home directory. I seperate them out from local users to something like <code>/home/&lt;DOMAIN&gt;/&lt;username&gt;</code></li>
</ul>


<p>Open up one of your groups (let&#8217;s start with LinuxAdmin) and add the user to that group. Note you have to do it 2 places (don&#8217;t blame me, I am just the messenger). Both in the standard Groups tab, but also in the UNIX attributes tab.</p>

<p>That should be everything on the Windows side.</p>

<h1>Configure RHEL as a client</h1>

<p>Most of the heavy lifting is done by the <em>System Security Service Daemon</em> (SSSD).</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yum install sssd sssd-client krb5-workstation samba openldap-clients policycoreutils-python</span></code></pre></td></tr></table></div></figure>


<p>This should also pull in all the dependencies.</p>

<h2>Configure Kerberos</h2>

<p>I&#8217;ve already said, this but I will repeat myself as getting it wrong will cause many lost hours.</p>

<ul>
<li>DNS must be working for both forward and reverse lookups</li>
<li>Time must be in sync accross all the clients</li>
</ul>


<p>Make sure that /etc/resolv.conf contains your domain controllers.</p>

<p><strong>Gotcha</strong>: In RHEL/Fedora the DNS setting are defined in /etc/sysconfig/network-settings/ifcfg-eth0 (or whichever NIC comes first) by Anaconda. This will over-write /etc/resolv.conf on reboot. For no good reason other than stubbornness I tend to remove these entries and define resolv.conf myself (or via configuration management). Alternatively put DNS1 and DNS2 entries in the network configuration files.</p>

<p>In <code>/etc/ntp.conf</code> change you servers to point at your Domain Controllers.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[logging]
</span><span class='line'> default = FILE:/var/log/krb5libs.log
</span><span class='line'>
</span><span class='line'>[libdefaults]
</span><span class='line'> default_realm = AD.EXAMPLE.COM
</span><span class='line'> dns_lookup_realm = true
</span><span class='line'> dns_lookup_kdc = true
</span><span class='line'> ticket_lifetime = 24h
</span><span class='line'> renew_lifetime = 7d
</span><span class='line'> rdns = false
</span><span class='line'> forwardable = yes
</span><span class='line'>
</span><span class='line'>[realms]
</span><span class='line'> AD.EXAMPLE.COM = {
</span><span class='line'>  # Define the server only if DNS lookups are not working
</span><span class='line'>#  kdc = server.ad.example.com
</span><span class='line'>#  admin_server = server.ad.example.com
</span><span class='line'> }
</span><span class='line'>
</span><span class='line'>[domain_realm]
</span><span class='line'> .ad.example.com = AD.EXAMPLE.COM
</span><span class='line'> ad.example.com = AD.EXAMPLE.COM</span></code></pre></td></tr></table></div></figure>


<p>You should now be able to run:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>kinit aduser@AD.EXAMPLE.COM</span></code></pre></td></tr></table></div></figure>


<p>That should obtain a kerberos ticket (check with <code>klist</code>) and you can move on. If it does not work fix it now - Kerberos is horrible to debug later.</p>

<h2>Enable LDAP Searches</h2>

<p>The best way to bind to AD is using SASL/GSSAPI as no passwords are needed.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>kinit Administrator@AD.EXAMPLE.COM
</span><span class='line'>net ads join createupn=host/client.ad.example.com@AD.EXAMPLE.COM –k
</span><span class='line'>net ads keytab create 
</span><span class='line'>net ads keytab add host/client.ad.example.com@AD.EXAMPLE.COM</span></code></pre></td></tr></table></div></figure>


<p>You should now be able to get information about yourself from AD using ldapsearch:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ldapsearch -H ldap://server.ad.example.com/ -Y GSSAPI -N -b "dc=ad,dc=example,dc=com" "(&(objectClass=user)(sAMAccountName=aduser))"</span></code></pre></td></tr></table></div></figure>


<h2>Configure SSSD</h2>

<p>Everything in SSSD revolves around a single config file (/etc/sssd/ssd.conf).</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[sssd]
</span><span class='line'> config_file_version = 2
</span><span class='line'> domains = ad.example.com
</span><span class='line'> services = nss, pam
</span><span class='line'> debug_level = 0
</span><span class='line'>
</span><span class='line'>[nss]
</span><span class='line'>
</span><span class='line'>[pam]
</span><span class='line'>
</span><span class='line'>[domain/ad.example.com]
</span><span class='line'> id_provider = ldap
</span><span class='line'> auth_provider = krb5 
</span><span class='line'> chpass_provider = krb5
</span><span class='line'> access_provider = ldap
</span><span class='line'>
</span><span class='line'> # To use Kerberos, un comment the next line
</span><span class='line'> #ldap_sasl_mech = GSSAPI
</span><span class='line'>
</span><span class='line'> # The following 3 lines bind to AD. Comment them out to use Kerberos
</span><span class='line'> ldap_default_bind_dn = CN=svc_unix,OU=useraccounts,DC=ad,DC=example,DC=com
</span><span class='line'> ldap_default_authtok_type = password
</span><span class='line'> ldap_default_authtok = Welcome_2014
</span><span class='line'>
</span><span class='line'> ldap_schema = rfc2307bis
</span><span class='line'>
</span><span class='line'> ldap_user_search_base = ,dc=ad,dc=example,dc=com
</span><span class='line'> ldap_user_object_class = user
</span><span class='line'> 
</span><span class='line'> ldap_user_home_directory = unixHomeDirectory
</span><span class='line'> ldap_user_principal = userPrincipalName
</span><span class='line'>
</span><span class='line'> ldap_group_search_base = ou=groups,dc=ad,dc=example,dc=com
</span><span class='line'> ldap_group_object_class = group
</span><span class='line'> 
</span><span class='line'> ldap_access_order = expire
</span><span class='line'> ldap_account_expire_policy = ad
</span><span class='line'> ldap_force_upper_case_realm = true
</span><span class='line'>
</span><span class='line'> krb5_realm = AD.EXAMPLE.COM</span></code></pre></td></tr></table></div></figure>


<p>There is something wrong here. Note the lines:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'> # To use Kerberos, un comment the next line
</span><span class='line'> #ldap_sasl_mech = GSSAPI
</span><span class='line'> 
</span><span class='line'> # The following 3 lines bind to AD. Comment them out to use Kerberos
</span><span class='line'> ldap_default_bind_dn = CN=svc_unix,OU=useraccounts,DC=ad,DC=example,DC=com
</span><span class='line'> ldap_default_authtok_type = password
</span><span class='line'> ldap_default_authtok = Welcome_2012</span></code></pre></td></tr></table></div></figure>


<p>Instead of doing the SASL/GSSAPI bind I would prefer to do I have chickened out and done a simple bind. Why? Because I am weak&#8230; :-(</p>

<p>Try with kerberos first, if it works then awesome, if not then create a service account in AD that can do nothing other than perform a search and use that to perform the bind. Make sure its path matches that of the <em>ldap_default_bind_dn</em> path, also make sure the password is more complex than &#8220;Welcome_2014&#8221;.</p>

<p>For now this does nothing, we need to tell PAM to use it. The easiest way to enable this on RHEL is to use the authconfig command:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>authconfig --enablesssd --enablesssdauth --enablemkhomedir –update</span></code></pre></td></tr></table></div></figure>


<p>This will update <code>/etc/nsswitch.conf</code> and various files in <code>/etc/pam.d</code> to tell the system to authenticate against SSSD. SSSD will in turn talk to Active Directory, using LDAP for Identification and Kerberos for authentication.
Finally you can enable your LinuxAdmin’s to use sudo. Run the command visudo and add the line:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>%LinuxAdmin ALL=(ALL)       ALL
</span><span class='line'># note the % sign, the defines it as a group not a user</span></code></pre></td></tr></table></div></figure>


<p>Now your admin’s can run commands as root by prefacing them with sudo. For an encore, I would suggest disabling root login via SSH. Log in as your AD user (leave your root session open, just in case) and run:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sudo sed -i 's/PermitRootLogin no/PermitRootLogin yes/' /etc/ssh/sshd_config
</span><span class='line'>sudo service sshd reload</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The End of Centralised Storage]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2013/09/12/the-end-of-centralised-storage/"/>
    <updated>2013-09-12T20:20:00+02:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2013/09/12/the-end-of-centralised-storage</id>
    <content type="html"><![CDATA[<p><img class="right" src="http://www.chriscowley.me.uk/images/NetappClustering.jpg">That is a pretty drastic title, especially given that I spend a significant part of my day job working with EMC storage arrays. The other day I replied to a tweet by <a href="http://blog.scottlowe.org">Scott Lowe</a> :</p>

<blockquote class="twitter-tweet"><p><a href="https://twitter.com/scott_lowe">@scott_lowe</a> with things like Gluster and Ceph what does shared storage actually give apart from complications?</p>&mdash; Chris Cowley (@chriscowleyunix) <a href="https://twitter.com/chriscowleyunix/statuses/377900529760083968">September 11, 2013</a></blockquote>


<script async src="http://www.chriscowley.me.uk//platform.twitter.com/widgets.js" charset="utf-8"></script>


<!-- more -->


<p>Due to time-zone differences between France and the USA I missed out on most of the heated conversation that ensued. From what I could see it quickly got out of hand, with people replying to so many others that they barely had any space to say anything. I am sure it has spawned a load of blog posts, as Twitter is eminently unsuitable for that sort of conversation (at least I have seen one by <a href="http://storagezilla.typepad.com/storagezilla/2013/09/tomorrows-das-yesterday.html">StorageZilla</a>.</p>

<p>The boundary between DAS (Direct Attached Storage) and remote storage (be that a SAN or NAS) is blurring. Traditionally a SAN/NAS array is a proprietary box that gives you bits of disk space that is available to whatever server (or servers) that you want. Conversely, DAS is attached either inside the server or to the back of it. Sharing between multiple servers is possible, but not very slick - no switched fabric, no software configuration, cables have to be physically moved.</p>

<p>Now everything is blurring. In the FLOSS world there is the like of Ceph and GlusterFS, which take your DAS (or whatever) and turn that into a shared pool of storage. You can put this on dedicated boxes, depending on your workload that may well be the best idea. However you are not forced to. To my mind this is a more elegant solution. I have a collection of identical servers, I use some for compute, other for storage, others for both. You can pick and choose, even doing it live.</p>

<p>The thing is, even the array vendors are now using DAS. An EMC VNX is commodity hardware, as is the VMAX (mostly, I believe there is an ASIC used in the encryption engine), Isilion, NetApp, Dell Compellent, HP StoreVirtual (formerly Lefthand). What is the difference in the way they attach their disks? Technically none I suppose, it is just hidden away.</p>

<p>Back to the cloud providers, when you provision a VM there is a process that happens (I am considering Openstack, as that is my area of interest/expertise). You provision an instance and it takes the template you select and copies it to the local storage on that host. Yes you can short-circuit that and use shared storage, but that is unnecessarily complex and introduces a potential failure point. OK, the disk in the host could fail, but then so would the host and it would just go to a new host.</p>

<p>With Openstack, you can use either Ceph or GlusterFS for your block storage (amongst others). When you create block storage for your instance it is created in that pool and replicated. Again, these will in most cases be distributing and replicating local storage. I have known people use SAN arrays as the back-end for Ceph, but that was because they already had them lying around.</p>

<p>There have been various products around for a while to share out your local storage on VMware hosts. VMware&#8217;s own VSA, HP StoreVirtual and now Virtual SAN takes this even deeper, giving tiering and tying directly into the host rather than using a VSA. It certainly seems that DAS is the way forward (or a hybrid approach such as PernixData FVP). This makes a huge amount of sense, especially in the brave new world of SSDs. The latencies involved in spinning rust effective masked those of the storage fabric. Now though SSDs are so fast, that the time it takes for a storage object to transverse the SAN becomes a factor. Getting at least the performance storage layer as physically close to the computer layer as possible is now a serious consideration.</p>

<p>Hadoop, the darling of the Big Data lovers, uses HDFS, which also distributes and replicates your data across local storage. GlusterFS can also be used too. You can use EMC arrays, but I do not hear much about that (other than from EMC themselves). The vast majority of Hadoop users seem to be on local storage/HDFS. On a similar note Lustre, very popular in the HPC world, is also designed around local storage.</p>

<p><span class='pullquote-right' data-pullquote='I just do not see anything really exciting in centralised storage'>
So what am I getting at here? To be honest I am not sure, but I can see a general move away from centralised storage. Even EMC noticed this ages ago - they were talking about running the hypervisor on the VNX/VMAX. At least that is how I remember it anyway, I may well be wrong (if I am, then it is written on the internet now, so it must be true). Red Hat own GlusterFS and are pushing it centre stage for Openstack, Ceph is also an excellent solution and has the weight of Mark Shuttleworth and Canonical behind it. VMware have been pushing Virtual SAN hard and it seems to have got a lot of people really excited. I just do not see anything really exciting in centralised storage, everything interesting is based around DAS.
</span></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Open Source Virtual SAN thought experiment]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2013/09/05/open-source-virtual-san-thought-experiment/"/>
    <updated>2013-09-05T21:19:00+02:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2013/09/05/open-source-virtual-san-thought-experiment</id>
    <content type="html"><![CDATA[<p>Okay, I know I am little slow on the uptake here, but I was on holiday at the time. The announcement of <a href="https://www.vmware.com/products/virtual-san/">Virtual SAN</a> at VMWorld the last week got me thinking a bit.</p>

<!-- more -->


<p>Very briefly, Virtual SAN takes locally attached storage on you hypervisors. It then turns it into a distributed object storage system which you can use to store your VMDKs. <a href="http://www.yellow-bricks.com/2013/09/05/how-do-you-know-where-an-object-is-located-with-virtual-san/">Plenty</a> <a href="http://www.computerweekly.com/news/2240166057/VMware-Virtual-SAN-vision-to-disrupt-storage-paradigm">of</a> <a href="http://chucksblog.emc.com/chucks_blog/2013/08/considering-vsan.html">other</a> <a href="http://architecting.it/2013/08/29/reflections-on-vmworld-2013/">people</a> have gone into a lot more detail. Unlike other systems that did a similar job previously this is not a Virtual Appliance, but runs on the hypervisors themselves.</p>

<p>The technology to do this sort of thing using purely Open Source exists. All this has added is a distributed storage layer on each hypervisor. There are plenty of these exist for Linux, with my preference probably being for GlusterFS. Something like this is what I would have in mind:</p>

<p><img class="center" src="http://i.imgur.com/NHYdf78.png"></p>

<p>Ceph is probably the closest to Virtual SAN, as it is fundamentally object-based. Yes there would be CPU and RAM overhead, but that also exists for Virtual SAN too. Something like DRBD/GFS2 is not really suitable here, because it will not scale-out as much. You would not have to have local storage in all your hypervisor nodes (as with Virtual SAN) too.</p>

<p>I honestly do not see any real problems with this.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Home-made Energy Drink]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2013/08/01/home-made-energy-drink/"/>
    <updated>2013-08-01T11:23:00+02:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2013/08/01/home-made-energy-drink</id>
    <content type="html"><![CDATA[<p><img class="right" src="http://farm8.staticflickr.com/7120/7645659336_357c65c781.jpg" width="400" height="300">This is a post which breaks from the normal subjects of Linux and storage.</p>

<p>Today I am going to share a very simple recipe for what I drink when I am cycling. I have some fairly simple requirements for this.</p>

<!-- more -->


<ol>
<li>It must work (it must rehydrate me effectively)</li>
<li>It must not be a rip off</li>
<li>I want to have a at least a reasonable idea of what is in it.</li>
</ol>


<p>You can spend a small fortune on these drinks. Exercise nutrition is big business, but starting at roughly 1 euro a bottle (3-4 euros a day at this time of year, plus my hay fever medicines) they can get pricey.</p>

<p>In reality you need 3 things:</p>

<ol>
<li>Water, and plenty of it</li>
<li>Sugar to give you back the fuel you burn.</li>
<li>Salt to help you absorb the fluid</li>
</ol>


<p>I use  500ml bottles from my <a href="http://www.laboutiqueducycle.fr/">LBS</a> that I chose for very technical reasons (they gave them to me free).</p>

<p>The important thing to get right is the proportions. Not enough sugar and you will not replace the glucose that you burn, too much and you will struggle to absorb it. Likewise with salt, not enough and you&#8217;re absorption rate will be to slow, too much and it will 1) taste nasty and 2) dehydrate you.</p>

<p>For each 500ml bottle I go for:</p>

<ul>
<li>3 teaspoon sugar (15-20g)</li>
<li>2-3  pinchs of salt</li>
</ul>


<p>That would taste disgusting, so add a touch of fruit squash to taste. I use Grenadine, because the children love it so it always in the fridge, but a sugar-free squash would be fine or a dash for fresh fruit juice. To allow for the extra sugar in the Grenadine, I cut down the added sugar by 1 teaspoon.</p>

<p>Finally, I use unrefined sugar and for the salt I use <a href="http://en.wikipedia.org/wiki/Gu%C3%A9rande#Salt_marshes">Sel de Guérande</a>. That way I know I am using the best quality ingredients, something I am sure the likes of Gatorade do not do.</p>

<p>Final cost is minimal, but it seems to work for me. Everyone is different, so these levels need adjusting for you.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Connect to Fedora 19 with FreeNX]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2013/08/01/connect-to-fedora-19-with-freenx/"/>
    <updated>2013-08-01T10:17:00+02:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2013/08/01/connect-to-fedora-19-with-freenx</id>
    <content type="html"><![CDATA[<p><img class="right" src="http://i.imgur.com/Z8LFhPUl.png" width="400" height="250"> FreeNX is a great remote desktop protocol. I find it more responsive than VNC and it uses less bandwidth. The biggest advantage though (in my eyes) is that it uses SSH to do the authentication. With VNC, each user has to arrange another password to connect to their VNC session.</p>

<!-- more -->


<p>However, FreeNX is still not really working nicely with GNOME 3. If you use KDE you are fine, but I like GNOME and many of the programs are GTK as a result. This means that they look out of place on KDE, which causes my engineer OCD super-sensory powers to go mad.</p>

<p>My workaround is to effectively go back to the tried and tested Gnome 2 environment, nowadays know as MATE.</p>

<p>Get the server installed and configured along with the MATE desktop:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sudo yum -y groupinstall "MATE desktop"
</span><span class='line'>sudo yum -y install freenx-server
</span><span class='line'>sudo /usr/libexec/nx/nxsetup --install --setup-nomachine-key
</span><span class='line'>sudo chkconfig freenx-server on</span></code></pre></td></tr></table></div></figure>


<p>Now open <code>/etc/nxserver/node.conf</code> and un-comment the line that sets the <code>COMMAND_START_GNOME</code> variable. You need to edit this line to read:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>COMMAND_START_GNOME=mate-session</span></code></pre></td></tr></table></div></figure>


<p>and restart the server with <code>sudo service freenx-server restart</code></p>

<p>Now connect to it using the NX client and chose to use a unix-gnome desktop. Instead of firing up <code>gnome-session</code> (which will fail) it will now run <code>mate-session</code> and you are happy.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Automated GlusterFS]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2013/06/23/automated-glusterfs/"/>
    <updated>2013-06-23T22:02:00+02:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2013/06/23/automated-glusterfs</id>
    <content type="html"><![CDATA[<p><img class="right" src="http://www.hastexo.com/system/files/imagecache/sidebar/20120221105324808-f2df3ea3e3aeab8_250_0.png"> As I promised on Twitter, this is how I automate a GlusterFS deployment. I&#8217;m making a few assumptions here:</p>

<!-- more -->


<ul>
<li>I am using CentOS 6, so should work on RHEL 6 and Scientific Linux 6 too. Others may work, but YMMV.

<ul>
<li>As I use XFS, RHEL users will need the <em>Scalable Storage</em> option. Ext4 will work, but XFS is recommended.</li>
</ul>
</li>
<li>That you have a way of automating your base OS installation. My personal preference is to use <a href="https://github.com/puppetlabs/Razor">Razor</a>.</li>
<li>You have a system with at least a complete spare disk dedicated to a GlusterFS brick. That is the best way to run GlusterFS anyway.</li>
<li>You have 2 nodes and want to replicate the data</li>
<li>You have a simple setup with only a single network, because I am being lazy. As a proof-of concept this is fine. Modifying this for second network is quite easy, just change the IP address in you use.</li>
</ul>


<p><img src="https://docs.google.com/drawings/d/1XA7GH3a4BL1uszFXrSsZjysi59Iinh-0RmhqdDbt7QQ/pub?w=673&h=315" title="'simple gluster architecture'" ></p>

<p>The diagram above shows the basic layout of what to start from in terms of hardware. In terms of software, you just need a basic CentOS 6 install and to have Puppet working.</p>

<p>I use a pair of Puppet modules (both in the Forge): <a href="http://forge.puppetlabs.com/thias/glusterfs">thias/glusterfs</a> and <a href="http://forge.puppetlabs.com/puppetlabs/lvm">puppetlabs/lvm</a>. The GlusterFS module CAN do the LVM config, but that strikes me as not the best idea. The UNIX philosophy of &#8220;do one job well&#8221;  holds up for Puppet modules as well. You will also need my <a href="https://github.com/chriscowley/puppet-yumrepos">yumrepos</a> module.</p>

<p>Clone those 3 modules into your modules directory:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd /etc/puppet/
</span><span class='line'>git clone git://github.com/chriscowley/puppet-yumrepos.git modules/yumrepos
</span><span class='line'>puppet module install puppetlabs/lvm --version 0.1.2
</span><span class='line'>puppet module install thias/glusterfs --version 0.0.3</span></code></pre></td></tr></table></div></figure>


<p>I have specified the versions as that is what was the latest at the time of writing. You should be able to take the latest as well, but comment with any differences if any. That gives the core of what you need so you can now move on to you <code>nodes.pp</code>.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>class basenode {
</span><span class='line'>  class { 'yumrepos': }
</span><span class='line'>  class { 'yumrepos::epel': }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>class glusternode {
</span><span class='line'>  class { 'basenode': }
</span><span class='line'>  class { 'yumrepos::gluster': }
</span><span class='line'>  
</span><span class='line'>  volume_group { "vg0":
</span><span class='line'>    ensure =&gt; present,
</span><span class='line'>    physical_volumes =&gt; "/dev/sdb",
</span><span class='line'>    require =&gt; Physical_volume["/dev/sdb"]
</span><span class='line'>  }
</span><span class='line'>  physical_volume { "/dev/sdb":
</span><span class='line'>    ensure =&gt; present
</span><span class='line'>  }
</span><span class='line'>  logical_volume { "gv0":
</span><span class='line'>    ensure =&gt; present,
</span><span class='line'>    require =&gt; Volume_group['vg0'],
</span><span class='line'>    volume_group =&gt; "vg0",
</span><span class='line'>    size =&gt; "7G",
</span><span class='line'>  }
</span><span class='line'>  file { [ '/export', '/export/gv0']:
</span><span class='line'>    seltype =&gt; 'usr_t',
</span><span class='line'>    ensure =&gt; directory,
</span><span class='line'>  }
</span><span class='line'>  package { 'xfsprogs': ensure =&gt; installed
</span><span class='line'>  }
</span><span class='line'>  filesystem { "/dev/vg0/gv0":
</span><span class='line'>    ensure =&gt; present,
</span><span class='line'>    fs_type =&gt; "xfs",
</span><span class='line'>    options =&gt; "-i size=512",
</span><span class='line'>    require =&gt; [Package['xfsprogs'], Logical_volume['gv0'] ],
</span><span class='line'>  }
</span><span class='line'>  
</span><span class='line'>  mount { '/export/gv0':
</span><span class='line'>    device =&gt; '/dev/vg0/gv0',
</span><span class='line'>    fstype =&gt; 'xfs',
</span><span class='line'>    options =&gt; 'defaults',
</span><span class='line'>    ensure =&gt; mounted,
</span><span class='line'>    require =&gt; [ Filesystem['/dev/vg0/gv0'], File['/export/gv0'] ],
</span><span class='line'>  }
</span><span class='line'>  class { 'glusterfs::server':
</span><span class='line'>    peers =&gt; $::hostname ? {
</span><span class='line'>      'gluster1' =&gt; '192.168.1.38', # Note these are the IPs of the other nodes
</span><span class='line'>      'gluster2' =&gt; '192.168.1.84',
</span><span class='line'>    },
</span><span class='line'>  }
</span><span class='line'>  glusterfs::volume { 'gv0':
</span><span class='line'>    create_options =&gt; 'replica 2 192.168.1.38:/export/gv0 192.168.1.84:/export/gv0',
</span><span class='line'>    require =&gt; Mount['/export/gv0'],
</span><span class='line'>  }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>node 'gluster1' {
</span><span class='line'>  include glusternode
</span><span class='line'>  file { '/var/www': ensure =&gt; directory }
</span><span class='line'>  glusterfs::mount { '/var/www':
</span><span class='line'>    device =&gt; $::hostname ? {
</span><span class='line'>      'gluster1' =&gt; '192.168.1.84:/gv0',
</span><span class='line'>    }
</span><span class='line'>  }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>node 'gluster2' {
</span><span class='line'>  include glusternode
</span><span class='line'>  file { '/var/www': ensure =&gt; directory }
</span><span class='line'>  glusterfs::mount { '/var/www':
</span><span class='line'>    device =&gt; $::hostname ? {
</span><span class='line'>      'gluster2' =&gt; '192.168.1.38:/gv0',
</span><span class='line'>    }
</span><span class='line'>  }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>What does all that do? Starting from the top:</p>

<ul>
<li> The <code>basenode</code> class does all your basic configuration across all your hosts. Mine actually does a lot more, but these are the relevant parts.</li>
<li> The <code>glusternode</code> class is shared between all your GlusterFS nodes. This is where all your Server configuration is.</li>
<li> Configures LVM

<ul>
<li>Defines the Volume Group &#8220;vg0&#8221; with the Physical Volume <code>/dev/sdb</code></li>
<li>Creates a Logical Volume &#8220;gv0&#8221; for GlusterFS use and make it 7GB</li>
</ul>
</li>
<li> Configures the file system

<ul>
<li>Creates the directory <code>/export/gv0</code></li>
<li>Formats the LV created previously with XFS (installs the package if necessary)</li>
<li>Mounts the LV at <code>/export/gv0</code></li>
</ul>
</li>
</ul>


<p>This is now all ready for the GlusterFS module to do its stuff. All this happens in those last two sections.</p>

<ul>
<li> The class <code>glusterfs::Server</code> sets up the peering between the two hosts. This will actually generate a errors, but do not worry. This because gluster1 successfully peers with gluster2. As a result gluster2 fails to peer with gluster1 as they are already peered.</li>
<li> Now <code>glusterfs::volume</code> creates a replicated volume, having first ensured that the LV is mounted correctly.</li>
<li> All this is then included in the node declarations for <code>gluster1</code> and <code>gluster2</code>.</li>
</ul>


<p>All that creates the server very nicely. It will need a few passes to get everything in place, while giving a few red herring errors. It should would however, all the errors are there in the README for the GlusterFS module in PuppetForge, so do not panic.</p>

<p>A multi-petabyte scale-out storage system is pretty useless if the data cannot be read by anything. So lets use those nodes and mount the volume. This could also be a separate node (but once again I am being lazy) the process will be exactly the same.</p>

<ul>
<li> Create a mount point for it ( `file {&#8216;/var/www&#8217;: ensure => directory }</li>
<li> Define your <code>glusterfs::mount</code> using any of the hosts in the cluster.</li>
</ul>


<p>Voila, that should all pull together and give you a fully automated GlusterFS set up. The sort of scale that GlusterFS can reach makes this sort of automation absolutely essential in my opinion. This should be relatively easy to convert to Chef or Ansible, whatever takes your fancy. I have just used Puppet because of my familiarity with it.</p>

<p>This is only one way of doing this, and I make no claims to being the most adept Puppet user in the world. All I hope to achieve is that someone finds this useful. Courteous comments welcome.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Dell Announces VRTX]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2013/06/04/dell-announces-vrtx/"/>
    <updated>2013-06-04T22:15:00+02:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2013/06/04/dell-announces-vrtx</id>
    <content type="html"><![CDATA[<p><img class="right" src="http://en.community.dell.com/cfs-file.ashx/__key/communityserver-blogs-components-weblogfiles/00-00-00-37-45/6886.vrtx.JPG"> Dell has announced the new PowerEdge VRTX (pronounced Vertex). The name comes from a vertex being &#8220;the intersection of multiple lines&#8221;, alluding to this being a mixture of a rack server, a blade server and a SAN.</p>

<!-- more -->


<p>It is aimed at branch offices, so it contains 4 servers, storage, networking and (unusually) the ability to add PCI-e cards (up to 8, including 3 FH/FL). These cards can be connected to which ever server you want. These features put in competition with the HP C3000 and the Supermicro OfficeBlade.</p>

<p>The other 2 are basically standard blade chassis that have been given quiet fans and IEC power connectors. You can pick and choose storage, PCI-E and compute blades depending on your needs. They also have the full array of networking options: anything from 2 1Gb uplinks to full on 40GB QDR infiniband. VRTX on the other hand is a fixed configuration of a 2U SAS array (either 2.5&#8221; or 3.5&#8221; disks) and 4 compute servers. You can add PCI-e cards, but support is limited. Basically, it expands the limited networking available in the blades themselves (no 10Gb at launch, max of 8x 1Gb uplink with no redundant fabric). There is support for a GPU, but it is AMD only with  no Nvidia Tesla support.</p>

<p>So what we have is a system that takes the same amount of space as it competitors and is less flexible. So why would you want it? In several cases I have wanted something that would give me a simple solution to run VMware (or similar) properly (i.e. shared storage and at least 2 nodes) and go in the corner of the office on a standard plug. The other solutions can do this with a bit of thought (more so with the Supermicro), but with the VRTX will do that out-of-the-box.</p>

<p>If I could make 1 request of Dell, it would be to do a &#8220;VRTX lite&#8221; that drops the PCI-e slots and (perhaps) halves the number of disks and servers. To get a pair of computer servers and a small SAN in a 4 bay NAS sized box would be awesome for many a SMB branch office.</p>

<iframe width="640" height="360" src="http://www.youtube.com/embed/16IlDQnIMrk?rel=0" frameborder="0" allowfullscreen></iframe>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Jellybean and OpenVPN]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2013/05/30/jellybean-and-openvpn/"/>
    <updated>2013-05-30T22:14:00+02:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2013/05/30/jellybean-and-openvpn</id>
    <content type="html"><![CDATA[<p><img class="right" src="http://openvpn.net/templates/telethra/img/ovpntech_logo-s.png">Setting up the server is well documented elsewhere, the <a href="http://openvpn.net/index.php/open-source/documentation/howto.html#quick">official howto</a> works nicely for me. I use a Virtual TUN network (routed) with clients connecting via UDP 1194, with all the network config pushed out by the server. Follow the howto and you will get that at the end.</p>

<!-- more -->


<p>When you create the key for your Jellybean client, create a PKCS12 certificate. This just means using the <code>build-key-pkcs12</code> script instead of <code>build-key</code>. Copy that .p12 key to the root of the SD card in your phone.</p>

<p>Now you can make that certificate available for use by the OpenVPN client. Open <em>Setting -> Security -> Install from SD Card</em> and choose the .p12 file you just copied there.</p>

<p>Install <a href="http://code.google.com/p/ics-openvpn/">OpenVPN for Android/ICS-openvpn</a>. In that create a new profile with a suitable name. Under &#8220;Basic&#8221; configure the <em>Server Address</em> and choose your certificate.</p>

<p>The end&#8230;</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Enable Developer Options on Android Jellybean]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2013/05/29/enable-developer-options-on-android-jellybean/"/>
    <updated>2013-05-29T21:13:00+02:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2013/05/29/enable-developer-options-on-android-jellybean</id>
    <content type="html"><![CDATA[<p><img class="right" src="http://www.android.com/images/whatsnew/jb-new-logo.png" title="" >This was weird! I just updated my venerable ZTE Blade to Cyanogenmod 10.1 (Jellybean). I flashed it, then rebooted having completely forgotten to to install GApps. No problem, reboot into Clockmod Recovery and install them. Promlem: no option to reboot to recovery.</p>

<!-- more -->


<p>It turns out that you need to enable <em>Advanced reboot</em> in the *Developer Options. Problem where are those options. In Jellybean 4.2 they are hidden.</p>

<ul>
<li>Open up <em>Settings</em> -> <em>About Phone</em></li>
<li>Find the entry for <em>Build number</em></li>
<li>Tap on it 7 times (honest), after the 3rd it will start to count down.</li>
</ul>


<p>Now you will have a new option under <em>Settings</em> for <em>Developer Options</em> in which you can turn on *Advanced Reboot&#8221;</p>

<p>For anyone who has a ZTE Blade and would like to get the latest and greatest Android on it, the files are <a href="https://copy.com/Dqx4qRjgs6KK">here</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[EMC ViPR thoughts]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2013/05/13/emc-vipr-thoughts/"/>
    <updated>2013-05-13T21:45:00+02:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2013/05/13/emc-vipr-thoughts</id>
    <content type="html"><![CDATA[<p>I have been a little slow on the uptake on this one. I would like to say it is because I was carefully digesting the information, but that is not true; the reality is that I have just had 2 5 day weekends in 2 weeks :-).</p>

<!-- more -->


<p>The big announcement at this years EMC World is ViPR. Plenty of people with far bigger reputations than me in the industry have already made their comments:</p>

<ul>
<li><a href="http://virtualgeek.typepad.com/virtual_geek/2013/05/storage-virtualization-platform-re-imagined.html">Chad Sakac</a> has really good and deep, but long.</li>
<li><a href="http://chucksblog.emc.com/chucks_blog/2013/05/introducing-emc-vipr-a-breathtaking-approach-to-software-defined-storage.html">Chuck Hollis</a> is nowhere near as technical but (as is normal for Chuck) sells it beautifully</li>
<li><a href="http://blog.scottlowe.org/2013/05/06/very-early-thoughts-about-emc-vipr/">Scott Lowe</a> has an excellent overview</li>
<li><a href="http://h30507.www3.hp.com/t5/Around-the-Storage-Block-Blog/ViPR-or-Vapor-The-Software-Defined-Storage-saga-continues/ba-p/138013?utm_source=feedly#.UZCd_covj3w">Kate Davies</a> gives HP&#8217;s take on it, which I sort of agree with, but not completely. As she says, the StoreAll VSA is not really in the same market, but I think it is the closest thing HP have so comparisons will always be drawn.</li>
</ul>


<p>ViPR is EMC&#8217;s response to two major storage problems:
1.   Storage is missing some sort of abstraction layer, particularly for management (the Control Plane).
1.   There is more to storage than NFS and iSCSI. As well as NAS/SAN we now have multiple forms of object stores, plus important non-POSIX file systems such as HDFS.</p>

<p>Another problem I would add is that of <em>Openness</em>. For now there is not really any protocols for managing multiple arrays from different manufacturers, even at a basic level. They have been attempts in the past (SMI-S), but they have never taken off. ViPR attacks that problem as well, sort of.</p>

<p>In some respects I am quite excited about ViPR. The ability to completely abstract the management of my storage is potentially very powerful. For now it is not really possible to integrate storage with Configuration Management tools. ViPR gives all supported arrays a REST API, thus it would be very simple to create bindings for the scripting language of your choice. Low and behold, a Puppet module to manage all my storage arrays becomes possible. This very neatly solves problem #1.</p>

<p>This is where my excitement ends however. The problem is that issue of <em>Openness</em> I mentioned above. EMC has gone to great lengths to describe ViPR as open, but the fact remains that it is not. EMC have published the specifications of the REST API, they have also created a plugin interface for third-parties to add their own arrays; this is where it ends however. All development of ViPR is at the mercy of EMC, so why would other vendors support it?</p>

<p>A lot of the management tools in ViPR are already in Openstack Cinder, which supports a much wider range of backends than ViPR at present. In that vendors have a completely open source management layer to develop against. Why would they sell their souls to a competitor? Simple, they will not. EMC exclusive shops will find ViPR to be an excellent way integrating their storage with a DevOps style workflow. Unfortunately my experience is that the sort of organizations that buy EMC (especially the big ones like VMAX) are not really ready for DevOps yet.</p>

<p>Another feature that EMC has been touted is multi-protocol access to your data. Block volumes can be accessed via both iSCSI and FC protocols - nothing really clever there I&#8217;m afraid. Dot Hill has been doing that for several years with the <a href="http://www.dothill.com/wp-content/uploads/2011/08/AssuredSAN-n-3920-3930-C-10.15.11.pdf">AssuredSAN 39x0</a> models (and by extension the the HP P2000 as well). That is also easy enough to do on commodity hardware using  <a href="http://linux-iscsi.org/wiki/Main_Page">LIO target</a> plus a whole lot more. On the file side, it gives you not only access to your data via both CIFS and NFS, but it does add object access to that. They touted this as being very clever, but once again you can already do this using well respected, production proven open source. Glusterfs has an object translator, so that covers that super clever feature. All the data abstraction features it has are already there in in the open source world. If you want object and NAS access to the same peta-byte storage system, you have it in both Glusterfs and Ceph, both of which can easily be managed by CM tools such as Puppet.</p>

<p><span class='pullquote-right' data-pullquote='To be a universal standard it would need to be an open (source) standard'>
EMC has really pushed ViPR in the last couple of weeks, but it fails to impress me. This is a shame, because in general I like EMC&#8217;s products. I don&#8217;t like their marketing, but in their gear does just work. ViPR will probably do well with large EMC/NetApp shops, but it is by no means the ground-breaking product that EMC would have people believe (to be honest, I&#8217;m not sure anything ever is). It can never be the universal gateway to manage our storage, it is too tied in to EMC. To be a universal standard it would need to be an open (source) standard, which is not really part of EMC&#8217;s culture (with the exception of the awesome Razor).
</span></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Bamboo Invoice on Centos with Nginx]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2013/04/29/bamboo-invoice-on-centos-with-nginx/"/>
    <updated>2013-04-29T21:16:00+02:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2013/04/29/bamboo-invoice-on-centos-with-nginx</id>
    <content type="html"><![CDATA[<p><a href="http://www.bambooinvoice.org/">BambooInvoice</a> is free Open Source invoicing software intended for small businesses and independent contractors. It is easy to use and creates pretty good looking invoices.</p>

<!-- more -->


<p>It is a simple PHP application that is based on the CodeIgniter framework. This means it is really simple to install on a typically LAMP stack. I however use Nginx and could not find any notes on how to configure it. It is pretty typical you can get most of the way by reading any of the Nginx howto documents on the web. Personally, for PHP apps, I use PHP-FPM, so you could use <a href="http://www.howtoforge.com/installing-nginx-with-php5-and-php-fpm-and-mysql-support-on-centos-6.4">this on Howtoforge</a> to get most of the way. That will get you a working Nginx, PHP and MySQL system.</p>

<p>Download the install file from [http://bambooinvoice.org/] an unzip is in your www folder:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nb">cd</span> /var/www
</span><span class='line'>wget http://bambooinvoice.org/img/bambooinvoice_089.zip
</span><span class='line'>unzip bambooinvoice_089.zip
</span></code></pre></td></tr></table></div></figure>


<p>You next step is to create a database for it along with a user:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='mysql'><span class='line'><span class="k">CREATE</span> <span class="k">DATABASE</span> <span class="n">bambooinvoice</span> <span class="k">DEFAULT</span> <span class="k">CHARACTER</span> <span class="kt">SET</span> <span class="n">utf8</span><span class="p">;</span>
</span><span class='line'><span class="k">GRANT</span> <span class="k">ALL</span> <span class="k">ON</span> <span class="n">bambooinvoice</span><span class="p">.</span><span class="o">*</span> <span class="k">TO</span> <span class="s1">&#39;bambooinvoice&#39;</span><span class="o">@</span><span class="s1">&#39;localhost&#39;</span> <span class="n">IDENTIFIED</span> <span class="k">BY</span> <span class="s1">&#39;bambooinvoice&#39;</span><span class="p">;</span>
</span><span class='line'><span class="n">FLUSH</span> <span class="n">PRIVILEGES</span><span class="p">;</span>
</span><span class='line'><span class="k">exit</span>
</span></code></pre></td></tr></table></div></figure>


<p>Now you can edit the config files to point at the database:</p>

<figure class='code'><figcaption><span>/var/www/bambooinvoices/bamboo_system_files/application/config/database.php</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class='php'><span class='line'><span class="cp">&lt;?php</span>  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="nb">defined</span><span class="p">(</span><span class="s1">&#39;BASEPATH&#39;</span><span class="p">))</span> <span class="k">exit</span><span class="p">(</span><span class="s1">&#39;No direct script access allowed&#39;</span><span class="p">);</span>
</span><span class='line'><span class="nv">$active_group</span> <span class="o">=</span> <span class="s1">&#39;default&#39;</span><span class="p">;</span>
</span><span class='line'>
</span><span class='line'><span class="nv">$db</span><span class="p">[</span><span class="s1">&#39;default&#39;</span><span class="p">][</span><span class="s1">&#39;hostname&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;localhost&#39;</span><span class="p">;</span>
</span><span class='line'><span class="nv">$db</span><span class="p">[</span><span class="s1">&#39;default&#39;</span><span class="p">][</span><span class="s1">&#39;username&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;bambooinvoice&#39;</span><span class="p">;</span>
</span><span class='line'><span class="nv">$db</span><span class="p">[</span><span class="s1">&#39;default&#39;</span><span class="p">][</span><span class="s1">&#39;password&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;bambooinvoice&#39;</span><span class="p">;</span>
</span><span class='line'><span class="nv">$db</span><span class="p">[</span><span class="s1">&#39;default&#39;</span><span class="p">][</span><span class="s1">&#39;database&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;bambooinvoice&#39;</span><span class="p">;</span>
</span><span class='line'><span class="nv">$db</span><span class="p">[</span><span class="s1">&#39;default&#39;</span><span class="p">][</span><span class="s1">&#39;dbdriver&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;mysql&#39;</span><span class="p">;</span>
</span><span class='line'><span class="nv">$db</span><span class="p">[</span><span class="s1">&#39;default&#39;</span><span class="p">][</span><span class="s1">&#39;dbprefix&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;bamboo_&#39;</span><span class="p">;</span>
</span><span class='line'><span class="nv">$db</span><span class="p">[</span><span class="s1">&#39;default&#39;</span><span class="p">][</span><span class="s1">&#39;active_r&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="k">TRUE</span><span class="p">;</span>
</span><span class='line'><span class="nv">$db</span><span class="p">[</span><span class="s1">&#39;default&#39;</span><span class="p">][</span><span class="s1">&#39;pconnect&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="k">FALSE</span><span class="p">;</span>
</span><span class='line'><span class="nv">$db</span><span class="p">[</span><span class="s1">&#39;default&#39;</span><span class="p">][</span><span class="s1">&#39;db_debug&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="k">TRUE</span><span class="p">;</span>
</span><span class='line'><span class="nv">$db</span><span class="p">[</span><span class="s1">&#39;default&#39;</span><span class="p">][</span><span class="s1">&#39;cache_on&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="k">FALSE</span><span class="p">;</span>
</span><span class='line'><span class="nv">$db</span><span class="p">[</span><span class="s1">&#39;default&#39;</span><span class="p">][</span><span class="s1">&#39;cachedir&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">;</span>
</span><span class='line'><span class="nv">$db</span><span class="p">[</span><span class="s1">&#39;default&#39;</span><span class="p">][</span><span class="s1">&#39;char_set&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;utf8&#39;</span><span class="p">;</span>
</span><span class='line'><span class="nv">$db</span><span class="p">[</span><span class="s1">&#39;default&#39;</span><span class="p">][</span><span class="s1">&#39;dbcollat&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;utf8_general_ci&#39;</span><span class="p">;</span>
</span><span class='line'><span class="cp">?&gt;</span><span class="x"></span>
</span></code></pre></td></tr></table></div></figure>


<p>Next you need set the base_url in <code>/var/www/bambooinvoices/bamboo_system_files/application/config/config.php</code>. Nothing else is essential in that file, but read the docs in the ZIP file to see what else you want to change.</p>

<p>Now the all important bit.</p>

<figure class='code'><figcaption><span>/etc/nginx/conf.d/bamboo.conf</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
</pre></td><td class='code'><pre><code class='nginx'><span class='line'><span class="k">server</span> <span class="p">{</span>
</span><span class='line'>    <span class="kn">listen</span> <span class="mi">80</span><span class="p">;</span>
</span><span class='line'>    <span class="kn">server_name</span> <span class="s">bamboo.example</span><span class="p">;</span>
</span><span class='line'>    <span class="kn">root</span> <span class="s">/var/www/bambooinvoice/</span><span class="p">;</span>
</span><span class='line'>    <span class="kn">index</span> <span class="s">index.php</span> <span class="s">index.html</span><span class="p">;</span>
</span><span class='line'>    <span class="kn">access_log</span>              <span class="s">/var/log/nginx/bamboo_access.log</span><span class="p">;</span>
</span><span class='line'>    <span class="kn">error_log</span>               <span class="s">/var/log/nginx/bamboo_error.log</span><span class="p">;</span>
</span><span class='line'>
</span><span class='line'>    <span class="kn">location</span> <span class="p">=</span> <span class="s">/robots.txt</span> <span class="p">{</span>
</span><span class='line'>        <span class="kn">allow</span> <span class="s">all</span><span class="p">;</span>
</span><span class='line'>        <span class="kn">log_not_found</span> <span class="no">off</span><span class="p">;</span>
</span><span class='line'>        <span class="kn">access_log</span> <span class="no">off</span><span class="p">;</span>
</span><span class='line'>    <span class="p">}</span>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>    <span class="c1"># Deny all attempts to access hidden files such as .htaccess, .htpasswd, .DS_Store (Mac).</span>
</span><span class='line'>    <span class="kn">location</span> <span class="p">~</span> <span class="sr">/\.</span> <span class="p">{</span>
</span><span class='line'>        <span class="kn">deny</span> <span class="s">all</span><span class="p">;</span>
</span><span class='line'>        <span class="kn">access_log</span> <span class="no">off</span><span class="p">;</span>
</span><span class='line'>        <span class="kn">log_not_found</span> <span class="no">off</span><span class="p">;</span>
</span><span class='line'>     <span class="p">}</span>
</span><span class='line'>     <span class="kn">location</span> <span class="s">/</span> <span class="p">{</span>
</span><span class='line'>         <span class="kn">try_files</span> <span class="nv">$uri</span> <span class="nv">$uri/</span> <span class="s">/index.php</span><span class="nv">$request_uri</span> <span class="s">/index.php</span><span class="p">;</span>
</span><span class='line'>     <span class="p">}</span>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>     <span class="kn">location</span> <span class="p">~</span> <span class="sr">\.php($|/)</span> <span class="p">{</span>
</span><span class='line'>         <span class="kn">try_files</span> <span class="nv">$uri</span> <span class="p">=</span><span class="mi">404</span><span class="p">;</span>
</span><span class='line'>         <span class="kn">fastcgi_pass</span> <span class="n">127.0.0.1</span><span class="p">:</span><span class="mi">9000</span><span class="p">;</span>
</span><span class='line'>         <span class="kn">include</span> <span class="s">/etc/nginx/fastcgi_params</span><span class="p">;</span>
</span><span class='line'>         <span class="kn">fastcgi_index</span> <span class="s">index.php</span><span class="p">;</span>
</span><span class='line'>         <span class="kn">set</span> <span class="nv">$script</span> <span class="nv">$uri</span><span class="p">;</span>
</span><span class='line'>         <span class="kn">set</span> <span class="nv">$path_info</span> <span class="s">&quot;&quot;</span><span class="p">;</span>
</span><span class='line'>         <span class="kn">if</span> <span class="s">(</span><span class="nv">$uri</span> <span class="p">~</span> <span class="sr">&quot;^(.+\.php)(/.+)&quot;)</span> <span class="p">{</span>
</span><span class='line'>             <span class="kn">set</span> <span class="nv">$script</span> <span class="nv">$1</span><span class="p">;</span>
</span><span class='line'>             <span class="kn">set</span> <span class="nv">$path_info</span> <span class="nv">$2</span><span class="p">;</span>
</span><span class='line'>         <span class="p">}</span>
</span><span class='line'>         <span class="kn">fastcgi_param</span> <span class="s">URI</span> <span class="nv">$uri</span><span class="p">;</span>
</span><span class='line'>         <span class="c1"># Next two lines are fix the 502 (Bad gateway) error</span>
</span><span class='line'>         <span class="kn">fastcgi_buffers</span> <span class="mi">8</span> <span class="mi">16k</span><span class="p">;</span>
</span><span class='line'>         <span class="kn">fastcgi_buffer_size</span> <span class="mi">32k</span><span class="p">;</span>
</span><span class='line'>
</span><span class='line'>         <span class="kn">fastcgi_param</span> <span class="s">PATH_INFO</span> <span class="nv">$path_info</span><span class="p">;</span>
</span><span class='line'>         <span class="kn">fastcgi_param</span> <span class="s">SCRIPT_NAME</span> <span class="nv">$script</span><span class="p">;</span>
</span><span class='line'>         <span class="kn">fastcgi_param</span> <span class="s">SCRIPT_FILENAME</span> <span class="nv">$document_root$script</span><span class="p">;</span>
</span><span class='line'>      <span class="p">}</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>At first glance, there is nothing out of the ordinary. This is pretty much what Howtoforge gives you. Look more closely and I have added the 3 lines 39-41. This solves a gateway problem I had when creating a client.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Hiera with Puppet]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2013/04/11/using-hiera-with-puppet/"/>
    <updated>2013-04-11T20:24:00+02:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2013/04/11/using-hiera-with-puppet</id>
    <content type="html"><![CDATA[<p>Using Hiera with Puppet is something I have struggled with a bit. I could see the benefits, namely decoupling my site configuration from my logic. However, for some reason I struggled a bit to really get my head around it. This was compounded by it being quite new (only really integrated in Puppet 3), so the docs are  little lacking.</p>

<!-- more -->


<p>There is some though, the <a href="http://docs.puppetlabs.com/hiera/latest/">documentation on PuppetLab&#8217;s site</a> is excellent, but a bit light. It explains the principles well, but is a little limited in real-world examples. Probably the best resource I found was Kelsey Hightower&#8217;s excellent presentation at <a href="http://youtu.be/z9TK-gUNFHk">PuppetConf 2012</a>:</p>

<p>I learnt a lot from that, but it would be nice if there was an equivalent written down. I suppose that is what I am aiming at here.</p>

<h1>Configuration</h1>

<ul>
<li><a href="https://github.com/chriscowley/puppet-nfs">NFS Module</a></li>
<li><a href="https://github.com/chriscowley/my-master-puppet/blob/master/hiera.yaml">Hiera Config</a></li>
<li><a href="https://github.com/chriscowley/my-master-puppet/tree/master/hieradata">Hiera Data</a></li>
</ul>


<p>I am using Open Source Puppet 3. If you are using 2.7 or Puppet Enterprise, files will be in a slightly different place. That is all explained in the documentation linked above.</p>

<p>The first thing you need to do is configure Hiera using the file <code>/etc/puppet/hiera.yaml</code>. Mine looks like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>---
</span><span class='line'>:backends:
</span><span class='line'>- yaml
</span><span class='line'>:yaml:
</span><span class='line'>:datadir: /etc/puppet/hieradata/
</span><span class='line'>:hierarchy:
</span><span class='line'>- %{::clientcert}
</span><span class='line'>- common</span></code></pre></td></tr></table></div></figure>


<p>This tells Hiera to use only the YAML backend - I do not like JSON because it always looks messy to me. It will look for the data in the folder <code>/etc/puppet/hieradata</code>. Finally it will look in that folder for a file called <clientcert>.yaml, then common.yaml. The process it uses to apply the values is explained very nicely in this image:
<img src="http://docs.puppetlabs.com/hiera/latest/images/hierarchy1.png"></p>

<p>Next, create the file <code>/etc/puppet/hieradata/&lt;certname&gt;.yaml</code> that contains your NFS exports:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>---
</span><span class='line'>exports:
</span><span class='line'>- /srv/iso
</span><span class='line'>- /srv/images</span></code></pre></td></tr></table></div></figure>


<p>Now, checkout my NFS module from Github links above. If you are not on RHEL6 or similar (I use Centos personally) you will have to modify it as needed.</p>

<p>There are 2 files that are really interesting here. The manifest file (manifests/server.pp) and the template to build the <code>/etc/exports</code> file (templates/exports.erb). We&#8217;ll take apart the manifest, the template just iterates over the data passed to it from that.</p>

<p>The first line creates an array variable called $exports from the Hiera data. Specifically, it looks for a key called <em>exports</em>. Hiera then goes through the hierarchy explained earlier looking for that key. In this case it will find it in the <certname>.yaml.</p>

<p>This data is now used for 2 things. First it creates the necessary folders, then it build <code>/etc/exports</code>. Here there is a minor problem, because you cannot do a <em>for each</em> loop in a Puppet manifest. We can fiddle it a bit by using a <a href="http://docs.puppetlabs.com/puppet/3/reference/lang_defined_types.html">defined type</a>.</p>

<p>The line <code>list_exports { $exports:; }</code> passes the <code>$exports</code> array to the type we define above it. This then goes ahead and creates the folders ready to be exported. The <code>-&gt;</code> builds an <a href="http://docs.puppetlabs.com/puppet/3/reference/lang_relationships.html#chaining-arrows">order relationship</a> with the File resource for <em>/etc/exports</em>. Specifically, that the directories need to be created before they are exported.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  define list_exports {
</span><span class='line'>    $export = $name
</span><span class='line'>    file { $export:
</span><span class='line'>      ensure =&gt; directory,
</span><span class='line'>      mode =&gt; '0755',
</span><span class='line'>      owner =&gt; 'root',
</span><span class='line'>      group =&gt; 'root'
</span><span class='line'>    }
</span><span class='line'>  }
</span><span class='line'>  list_exports { $exports:; } -&gt; File['/etc/exports']</span></code></pre></td></tr></table></div></figure>


<p>Now it can go ahead and build the <code>/etc/exports</code> file using that same $exports array in the <code>templates/exports.erb</code> template:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  &lt;% [exports].flatten.each do |export| -%&gt;
</span><span class='line'>  &lt;%= export %&gt; 192.168.1.0/255.255.255.0(rw,no_root_squash,no_subtree_check)
</span><span class='line'>  &lt;% end -%&gt;</span></code></pre></td></tr></table></div></figure>


<p>There is nothing especially Hiera&#8217;y about this, other than where the data in that array came from.</p>

<p>The rest of the manifest deals with installing the packages and configuring services. Once again, nothing especially linked with Hiera, but hopefully it will be useful for anyone wanting to Puppetize their NFS servers - which of course you should be.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Stop the hate on software RAID]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2013/04/07/stop-the-hate-on-software-raid/"/>
    <updated>2013-04-07T20:21:00+02:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2013/04/07/stop-the-hate-on-software-raid</id>
    <content type="html"><![CDATA[<p><img class="right" src="http://www.chriscowley.me.uk/images/NetappClustering.jpg">I&#8217;ve had a another bee in my bonnet recently. Specifically, it has been to do with hardware vs software RAID, but I think it goes deeper than that. It started a couple of months back with a discussion on <a href="http://redd.it/18dp63">Reddit</a>. Some of the comments were:</p>

<blockquote><p>Get out, get out now.</p>

<p>while he still can..</p>

<p>WHAT!?
60 TB on software raid.
Jeezus.</p>

<p>Software raid? Get rid of it.</p></blockquote>

<p>It then got re-awakened the other day when Matt Simmons (aka <a href="http://www.standalone-sysadmin.com/blog/">The Standalone Sysadmin</a>) asked the following question on Twitter:</p>

<blockquote class="twitter-tweet"><p>So what are the modern arguments for / against hardware / software RAID? I don&#8217;t get out much. <a href="https://twitter.com/search/%23sysadmin">#sysadmin</a></p>&mdash; Matt Simmons (@standaloneSA) <a href="https://twitter.com/standaloneSA/status/319932013492703233">April 4, 2013</a></blockquote>


<script async src="http://www.chriscowley.me.uk//platform.twitter.com/widgets.js" charset="utf-8"></script>


<!-- more -->


<p>At the time of writing, 2 people replied: myself and <a href="http://utcc.utoronto.ca/~cks/space/blog/">Chris Siebenmann</a>. Both of us basically said software RAID is better, hardware is at best pointless.</p>

<p>First of all, I need to define what I mean by hardware RAID. First, I do not care about what you are using for your c:\ drive in Windows, or your / partition in Linux. I am talking about the place where you store your business critical data. If you file server goes down, that is a bad day, but the business will live on. Lose your business data, then you will be out of a job (most likely alongside everyone else). Hardware RAID can thus fall into to categories:</p>

<ul>
<li>a bunch of disks attached to a PCI-e card in a big server</li>
<li>an external storage array. This could be either SAN or NAS, once again I do not care in this instance.</li>
</ul>


<p>I am firmly of the opinion that hardware RAID cards should no longer exist. They are at best pointless and at worst a liability. Modern systems are so fast that there is no real performance hit. Also management is a lot easier; if you have a hardware array then you will need to load the manufacturer&#8217;s utilities in order to manage it. By manage, I mean to be told when a disk has failed. On Linux, there is no guarantee that will work. There is a couple of vendors that require packages from RHEL4 to be installed on RHEL6 systems to install their tools. Also, they are invariable closed source, will most likely taint my kernel with binary blobs and generally cause a mess on my previously clean system. By contrast, using software RAID means that I can do all the management with trivial little scripts that can easily be integrated with any monitoring system that I choose to use.</p>

<p>I can understand why people are skeptical of software RAID. There have been performance reasons and practical reasons for it not to be trusted. I&#8217;m not going to address the performance argument, suffice to say that RAID is now 25 years old - CPUs have moved on a lot in that time. I remember when the first Promise IDE controllers came out, that used a kind of pseudo-hardware RAID - it was not pretty. The preconceptions are compounded by the plethora of nasty controllers built in to consumer motherboards and (possibly worst of all) Window&#8217;s built in RAID that was just bad.</p>

<p>The thing is, those days are now a long way behind us. For Linux there is absolutely no need for hardware RAID, even Windows will be just fine with an motherboard based RAID for its c: drive.</p>

<p><span class='pullquote-right' data-pullquote='I would say that hardware RAID is a liability'>
In fact I would say that hardware RAID is a liability. You go to all that effort to safe-guard your data, but the card becomes a single-point-of-failure. It dies, then you spend your time searching Ebay for the same model of card. You buy it, then you pray that the RAID data is stored on the disks and not the controller (not always the case). By contrast, if you use software RAID and the motherboard dies, then you pull the disks and plug them into whatever box running Linux and you recover your data.
</span></p>

<p>There is definitely a time and place for an external array. If you are using virtualisation properly, you need shared storage. The best way to do that, 9 times out of 10, is with an external array. However, even that may well not be as it seems. There are some that still develop dedicated hardware and come out with exciting gear (HP 3Par and Hitachi Data Systems spring to mind). However, the majority of storage is now on Software.</p>

<p>Let take a look at these things and see just how much &#8220;hardware&#8221; is actually involved.</p>

<p>The EMC VMAX is a big, big black box of storage. Even the &#8220;baby&#8221; 10k one scales up to 1.5PB and 4 engines. The 40k will go up to 3PB and 8 engines. Look a little deeper (one line further on the spec sheet) and you find that what those engines are: quad Xeons (dual on the 10/20k). The great big bad VMAX is a bunch of standard x86 servers running funky software to do all the management and RAID calculations.</p>

<p><span class='pullquote-right' data-pullquote='since the Clariion CX4 EMC has been using Windows Storage Server'>
Like its big brother, the VNX is also a pair of Xeon servers. Even more, it runs Windows. In fact since the Clariion CX4 EMC has been using Windows Storage Server (based on XP) Move along to EMC&#8217;s other lines we find Isilion is nothing more than a big pile of Supermicro servers running (IIRC) FreeBSD.
</span></p>

<p>Netapp&#8217;s famed FAS range similarly runs on commodity hardware,OnTAP is <a href="https://en.wikipedia.org/wiki/NetApp_filer">BSD</a> based.</p>

<p>The list goes on, Dell Compellent? When I looked at it in early 2012, it was still running on Supermicro dual Xeons. The plan was to move it to Dell R-series servers as soon as possible. They were doing validation at the time, I suspect the move is complete now. Reading between the lines, I came away with the impression that it runs on FreeBSD, but I do not know for sure. CoRAID use Supermicro servers, they unusually run Plan9 as their OS. HP StoreVirtual (formerly Lefthand) runs or Proliant Gen8 servers or VMware. In all these cases, there is no extra hardware involved.</p>

<p><span class='pullquote-right' data-pullquote='The people that write the MD stack in the Linux kernel are not cowboys'>
The people that write the MD stack in the Linux kernel are not cowboys. It has proved over and over again that is both stable and fast. I have trusted some of the most important data under my care to their software:  for many years the ERP system at <a href="http://www.snellgroup.com">Snell</a> has been running on MD devices quite happily. We found it much faster than the P410 cards in the DL360G5 servers that host it. Additionally, you do not need to load in any funky modules or utilities - everything you need to manage the devices is there in the distribution.
</span></p>

<p>ZFS also recommends to bypass any RAID devices and let it do everything in software, as does Btrfs. With <em>Storage Spaces</em> in Server 2012 Microsoft is definitely angling towards software controlled storage as well.</p>

<p>As with everything in IT, hardware is falling by the wayside in storage. Modern processors can do the processing so fast that there is no performance need for hardware in between your OS and the disks any more. The OS layers (Storage Spaces on Windows and especially MD/LVM on Linux) are so mature now that their reliability can be taken as a given. With the management advantages, there really is no technical reason to stick with hardware RAID. In fact the closer you can get the raw disks to your OS the better.</p>

<p><span class='pullquote-right' data-pullquote='we need to know what is inside that magic black box, especially when it is in the spec sheet'>
As I said at the start, the subject here is software vs hardware RAID, but my problem goes deeper than that particular argument. As technology professionals, we are technical people. We need to understand what is going on under the bonnet - that is our job! It may be fine for a vendor to pull the wool over a CFO&#8217;s eyes, but we need to know what is inside that magic black box, especially when it is in the spec sheet.
</span></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Writeable TFTP Server On CentOS]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2013/03/25/writeable-tftp-server-on-centos/"/>
    <updated>2013-03-25T15:45:00+01:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2013/03/25/writeable-tftp-server-on-centos</id>
    <content type="html"><![CDATA[<p>Well this caught me out for an embarassingly long time. There are <a href="http://blog.penumbra.be/tag/tftp/">loads</a> <a href="http://www.question-defense.com/2008/11/13/linux-setup-tftp-server-on-centos">of</a> <a href="http://wiki.centos.org/EdHeron/PXESetup">examples</a> of setting up a TFTP server on the web. The vast majority of them assume that you are using them read-only for PXE booting.</p>

<!-- more -->


<p>I needed to make it writeable so that it could be used for storing switch/router backups. It is trivially simple once you have read the man page (pro tip: RTFM).</p>

<p>I am doing this on RHEL6, it should be fine on Centos, Scientific Linux or Fedora as is. Any other distro it will require some modification. First install it (install the client as well to test at the end:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yum install tftp tftp-server xinetd
</span><span class='line'>chkconfig xinetd on</span></code></pre></td></tr></table></div></figure>


<p>Now edit the file `/etc/xinetd.d/tftp to read:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>service tftp
</span><span class='line'>{
</span><span class='line'>    socket_type = dgram
</span><span class='line'>    protocol    = udp
</span><span class='line'>    wait        = yes
</span><span class='line'>    user        = root
</span><span class='line'>    server      = /usr/sbin/in.tftpd
</span><span class='line'>    server_args = -c -s /var/lib/tftpboot
</span><span class='line'>    disable     = no
</span><span class='line'>    per_source  = 11
</span><span class='line'>    cps         = 100 2
</span><span class='line'>    flags       = IPv4
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>There are 2 changes to this file from the defaults. The <code>disable</code> line enables the service. Normally that is where you leave it. However, you cannot upload to the server in this case without pre-creating the files.</p>

<p>The second change adds a <code>-c</code> flag to the <code>server_args</code> line. This tells the service to create the files as necessary.</p>

<p>It still will not work though. You need to tweak the filesystem permissions and SELinux:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>chmod 777 /var/lib/tftpboot
</span><span class='line'>setsebool -P tftp_anon_write 1</span></code></pre></td></tr></table></div></figure>


<p>Of course you&#8217;ll also need to open up the firewall. So add the following line to <code>/etc/sysconfig/iptables</code>:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>-A INPUT -m state --state NEW -m udp -p udp -m udp --dport 69 -j ACCEPT</span></code></pre></td></tr></table></div></figure>


<p>If your IPtables set up is what comes out of the box, there will be a similar line to allow SSH access (tcp:22), I would add this line just after that one. If you have something more complicated, then you will probably know how to add this one as well anyway.</p>

<p>You should now be able to upload something to the server</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>echo "stuff" &gt; test
</span><span class='line'>tftp localhost -c put test</span></code></pre></td></tr></table></div></figure>


<p>Your test file should now be in <code>var/lib/tftpboot</code>.</p>

<p>One final note with regards to VMware. This does not work if you are using the VMXNET3 adapter, so make sure you are using the E1000. GETs will work and the file will be created, but no data will be put on the server. To annoy you even more, the test PUTting to localhost will work, but PUTs from a remote host will not.</p>

<p>It has been noted in the VMware forums <a href="http://communities.vmware.com/thread/215456">here</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What a boss owes their staff]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2013/03/18/what-a-boss-owes-their-staff/"/>
    <updated>2013-03-18T20:47:00+01:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2013/03/18/what-a-boss-owes-their-staff</id>
    <content type="html"><![CDATA[<p><img class="right" src="http://www.chriscowley.me.uk/images/reputation-management-starts-with-trust.jpg"> I recently had a conversation on Twitter with my friend <a href="http://www.robborley.com/">Rob Borley</a> who runs a <a href="http://www.dootrix.com/">mobile startup</a>. He had asked what interesting perks he should be giving his <a href="https://twitter.com/bobscape/statuses/313610008535367680">staff</a>.</p>

<!-- more -->


<p><span class='pullquote-right' data-pullquote='When you have a great work environment what is it that is at the root?'>
My initial response was the standard IT answer. Training, certifications and a lab to play in, which they already have. I like to find the root cause of things, usually that means looking for the underlying reason something is broken. In this case I wanted to put a more positive spin on it. When you have a great work environment what is it that is at the root? The answer is simple: trust.
</span></p>

<p>By way of a silly example, if I were to put a cake in the middle of my son&#8217;s classroom, I can guarantee that the majority of the cake will go into the mouths of a few, while most will probably not get any. Why? They are children, that is why. However, if I give it to his teacher then she will make sure that it gets evenly distributed to everyone. She, like your staff, is an adult and she behaves as such.</p>

<p><span class='pullquote-right' data-pullquote='if someone is going to sit there surfing Engadget all day, you are powerless to stop them'>
The has been a lot in the news recently about remote-working. Chiefly because of the new Yahoo CEO <a href="http://allthingsd.com/20130222/physically-together-heres-the-internal-yahoo-no-work-from-home-memo-which-extends-beyond-remote-workers/">putting a stop</a> to it. I have to fall in line with what Tony Schwartz <a href="http://www.businessinsider.com/want-productive-employees-treat-them-like-adults-2013-3">wrote in response</a> to that on Business Insider. Basically, if you cannot trust your staff to work when they are not in the office, you have hired the wrong people. You cannot be watching them all the time, nor can middle-management once you are past the start-up stage. Basically, if someone is going to sit there surfing Engadget all day, you are powerless to stop them. However, they will not be delivering, so they have to go. Likewise I have had colleagues who everytime I looked at their screen were surfing Ebay, or the Register. We hardly ever discussed computers, we mostly discussed trains and bikes. We delivered however, so who cares what was in our browser window and conversation? I myself got pulled to one side one day by my old boss to ask why I was playing around with an ESX server. We had no VMware servers, nor did we have any plans to. My response was that it would help make me better at my job. A year later we started rolling out a VMware infrastructrue, a project which I lead because I had taken the time to learn stuff. My boss had <em>trusted</em> me that I was not wasting my time and it paid off for him because we did not have to get in expensive consultants.
</span></p>

<p><span class='pullquote-right' data-pullquote='Trust leads to everything else that we like about work.'>
Trust leads to everything else that we like about work. Allowing your staff to work from home whenever they want is a question of trust. Perhaps one of them is spending time learning how to program in <a href="http://golang.org/">Go</a> even though you are a Dot Net house. Let them do so, trust them that they are going to make themselves a better programmer.
</span></p>

<p>This stuff may pay off directly (as in my VMware example), may be it won&#8217;t. If you let people work from home, maybe at times you will wonder what they are doing. You will however have a happier employee. If that employee has no desire to go anywhere else, but wants to deliver the best they can for your company then you can only win.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Thoughts on the shiney new VMAX]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2013/03/01/thoughts-on-the-shiney-new-vmax/"/>
    <updated>2013-03-01T15:21:00+01:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2013/03/01/thoughts-on-the-shiney-new-vmax</id>
    <content type="html"><![CDATA[<p><img class="right" src="http://www.emc.com/R1/images/EMC_Image_C_1310593327367_header-image-vmax-10k.png"> I&#8217;ve spent a significant amount of time recently swatting up on EMC&#8217;s new <a href="http://chucksblog.emc.com/chucks_blog/2013/02/introducing-vmax-cloud-edition.html">VMAX Cloud Edition</a>. It has to be said that this looks like one of the most interesting storage announcements I have seen in a long time. In fact I have a project coming up that I think it may well be a perfect fit for.</p>

<!-- more -->


<p>First a massive thanks to EMC&#8217;s Matthew Yeager (@mpyeager) who answered a couple of questions I had. He really went the extra mile to clarify a couple of things and the <a href="http://www.youtube.com/watch?v=WoElTAevLDs">video</a> he made is well worth a watch. Also Martin Glassborow (@storagebod) has <a href="http://www.storagebod.com/wordpress/?p=1293">interesting things to say</a> as well.</p>

<p>This is a product that could put a lot of people out of a job. If you are the sort of person who likes to keep hold of your little castle&#8217;s of knowledge then you will not like this from what I can see. Finally we are able to be truly customer focused, balancing cost, performance and capacity to give them exactly what they want. EMC claim this is a world first and to my knowledge they are right.</p>

<p><span class='pullquote-right' data-pullquote='With that amount of data the amount of art involved diminishes'>
Storage architects put a lot of time and effort in to tweaking quotes and systems to balance price, capacity and performance for a given work load. However, most of this is just reading up on the best-practises for a given array and situation and applying them. There is nothing that clever to it - reading and practise is what it comes down to. However, it has alway been as much an art as a science because an individual architect does not have a very large dataset to refer to. On the other hand EMC have got 60 million hours of metrics across more than 7000 VMAX systems out in the field. With that amount of data the amount of art involved diminishes and it becomes purely a science.
</span></p>

<p>What you get is a <a href="http://www.emc.com/storage/symmetrix-vmax/vmax-10k.htm">VMAX 10k</a>, but instead of defining storage pools, tiering policies, RAID levels etc you balance 3 facters: Space, performance and cost. Need a certain performance level for a certain amount of space no matter the cost? Just dial it in and mail EMC a cheque. Have a certain budget, need a certain amount of space, but performance not a problem? Same again.</p>

<p>No longer will we  be carefully balancing the number of SATA and FC spindles and the types of RAID level. No longer will be worrying about what percentage of our workload we need to keep on the SSD layer to assure the necessary number of IOPS. We will not even be calculating how much space we have after the RAID overheads.</p>

<p><span class='pullquote-right' data-pullquote='Here we have the abilty to easily integrate a VMAX with the likes of OpenStack Cinder, Puppet, Libvirt, or whatever'>
That is all very interesting, but so far it is just a new approach to the UI. It is an excellent approach, but nothing especially clever. One of things I gravitated towards was the white paper about integrating with <a href="http://www.emc.com/collateral/white-papers/h11468-vmax-cloud-edition-wp.pdf">vCloud</a>. Despite it being geared toward VMware (I wonder why? - not!) the principles equally apply to any situation where automation is required. I am a huge DevOps fan (Puppet in particular). Storage arrays have never been particularly automation friendly. In addition to the cloud portal, the VMAX CE also has a RESTful API. Now that is awesome! Here we have the abilty to easily integrate a VMAX with the likes of OpenStack Cinder, Puppet, Libvirt, or whatever you want.
</span></p>

<p>Finally <a href="http://virtualgeek.typepad.com">Chad Sakac</a> informs me that VMAX CE is just the first. EMC intend to roll this management style out to other product lines. Personally I think this would suit both Isilion and Atmos lines very nicely.</p>

<p>I am really excited about this product. It brings a paradigm shift in storage management and automation. Also I am led to believe that the price is exceptional as well, to point that it seems EMC may even be pushing VNX down a market level (to where it should be perhaps?). I have been <a href="http://www.chriscowley.me.uk/blog/2012/12/10/emc-extremio-thoughts/">a bit nasty</a> to EMC in the past, but recently they are doing some stuff that has really got me interested. This and <a href="https://github.com/puppetlabs/Razor">Razor</a> are 2 projects that are definitely worth keeping an eye on.</p>

<iframe width="420" height="315" src="http://www.youtube.com/embed/WoElTAevLDs" frameborder="0" allowfullscreen></iframe>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Linux to Storage]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2013/02/25/the-linux-to-storage/"/>
    <updated>2013-02-25T13:09:00+01:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2013/02/25/the-linux-to-storage</id>
    <content type="html"><![CDATA[<p>Martin &#8220;Storagebod&#8221; Glassborow recently wrote an interesting article where he asked &#8220;Who&#8217;ll do a Linux to Storage?&#8221;. As someone who is equal parts Storage and Linux, the same question runs around my head quite often. Not just that, but how to do it. It is safe to say that all the constituent parts are already in the Open Source Ecosystem. It just needs someone to pull them all together wrap them up in an integrated interface (be that a GUI, CLI, an API or all).</p>

<!--more -->


<p>Linux, obviously, has excellent NFS support. Until recently it was a little lacking in terms of block support. <a href="http://sourceforge.net/projects/iscsitarget/">iSCSI Enterprise Target</a> is ok, but is not packaged for RHEL, which for most shops makes it a big no-no. Likewise <a href="http://stgt.sourceforge.net/">TGT</a> is not bad, I have certainly used it to to good effect, but administering it is a bit like pulling teeth. Additionally, neither are VMware certified and I am pretty sure that TGT at  least is missing a required feature for certification as well (may be persistent reservations). There is a third SCSI target in Linux though: <a href="http://www.linux-iscsi.org/">LIO Kernel Target</a> by Rising Tide Systems. This is a lot newer, but is already VMware Ready certified. Red Hat used it in RHEL6 for FCoE target support, but not for iSCSI. in RHEL7 they will be <a href="http://groveronline.com/2012/11/tgtd-lio-in-rhel-7/">using it for all block storage</a>. It has a much nicer interface than the other targets on Linux, using a very intuitive CLI, nice JSON config files and a rather handy API. Rising Tide are a bit of an unknown however, or at least I thought so. It turns our that both QNAP and Netgear use LIO Kernel Target in their larger devices - hence the VMware certification. In any case, Red Hat are behind it, although I think they are working on a fork of at least the CLI, so I think success is assured there. That solves the problem of block storage, be it iSCSI, Fibre-Channel, FCoE, Infiniband or even USB.</p>

<p>Another important building block in an enterprise storage system is some way of distributing the data for both redunancy and performance. Marin mentions <a href="http://ceph.com/">Ceph</a> which is an excellent system. Personally I would put my money on <a href="http://www.gluster.org/">GlusterFS</a> though. I have had slightly better performance from it. Red Hat bought Gluster about a year ago, and have put some serious development effort into it. As well as POSIX access via Fuse, it has Object storage for use with OpenStack, a native Qemu connector is coming in the next versions. Hadoop can also access it directly. There is also a very good Puppet module for it, which gets around one of Martin&#8217;s critisms of Ceph.</p>

<p>Which brings me nicely to managing this theoretical system. Embedding Puppet in this sort of solution would also make sense. There will be need to a way of keep config sync&#8217;ed on all the nodes (I mentioned that this disruptive product will be scale-out didn&#8217;t I? NO? OK, it will be - prediction for the day). Puppet does this already very well, so why re-invent the wheel.</p>

<p>All this can sit on top of Btrfs allowing each node to have up to 16 exabytes of local storage. For now I am not convinced by it, at least on RHEL 6 as I have seen numerous kernel panics, nor did I have a huge amount of joy on Fedora 17, but there is no doubt that it will get there. Alternatively, there is always the combination of XFS and LVM. XFS is getting on a bit now, but it has been revived by Red Hat in recent years and it is a proven performer with plenty of life left in it yet.</p>

<p>After all that, who do I think is ripe to do some serious disrupting in the storage market? Who will &#8220;Do a Red Hat&#8221; as Martin puts it? Simple: it will be Red Hat! Look at the best of breed tools at every level of the storage stack on Linux and you will find it is either from Red Hat (Gluster) or they are heavily involved (LIO target). They have the resources and the market/mind share to do it. Also they have a long history of working with and feeding back to the community, so the fortuitous circle will continue.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Something from the shadows]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2013/02/21/something-from-the-shadows/"/>
    <updated>2013-02-21T16:06:00+01:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2013/02/21/something-from-the-shadows</id>
    <content type="html"><![CDATA[<p>An intriguing startup came out of stealth mode a few days ago. <a href="http://pernixdata.com/">Pernix Data</a> was founded by Pookan Kumar and Satyam Vaghani, both of who were pretty near top of the pile in VMware&#8217;s storage team.</p>

<!--more -->


<p>What they are offering is, to me at least, a blinding flash of the obvious. It is a softweare layer that runs on a VMware hypervisor that uses local flash as a cache for whatevery is coming off your main storage array. <img class="right" src="http://pernixdata.com/images/home_graphic3.png" width="300" height="217">. That could be an SSD (or multiple) or a PCI-e card.</p>

<p>Reading what they have to say, it is completely transparent to the hypervisor, so everything just works. Obviously me being an Open Source fanatic I imediately started thinking how I could do this with Linux; it took me about 5 minutes.</p>

<p>You take your SAN array and give your LUN to your Hypervisors (running KVM obviously, and with a local SSD). Normally you would stick a clustered file system (such as GFS2) on that shared LUN. Instead you use a tiered block device on top of that LUN. There are two that come immediately to mind: <a href="https://github.com/facebook/flashcache/">Flashcache</a> and <a href="http://sourceforge.net/projects/tier/files/">Btier</a>.</p>

<p>Finally, you can put your clustered file system on that tiered device. I do not have the time or facilities to test this, but I cannot see why it would not work. Maybe someone at Red Hat (seeing as they do the bulk of KVM and GFS2 development) can run with this and see what happens.
What their plans are I do not know. It is very early days, maybe they will be a success maybe not. As they are both ex-VMware, I would not be at all surprised if they get bought back into the VMware fold. Certainly this is a functionality that I would have like to have seen in the past.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How much should you spend on IT]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2013/02/06/how-much-should-you-spend-on-it/"/>
    <updated>2013-02-06T16:04:00+01:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2013/02/06/how-much-should-you-spend-on-it</id>
    <content type="html"><![CDATA[<p>A recent discussion/argument I had on Reddit got me thinking about the cost of solutions we put in.</p>

<p>In an ideal world everything would did have full redundancy, and the customer would never have any downtime. Everything would always be up-to-date and keeping it so would require restarting. The reality is very different unfortunately.</p>

<!--more-->


<p>This potentially rambling post was inspired by someone accusing me of having &#8220;a horrible idea&#8221; because I suggested someone put pfsense on an Atom PC as a VPN router for a small office. He then proceeded to expain to me how you should always buy an expensive black box from a vendor (he didn&#8217;t say black box if I am honest, I am interpreting), how you have to always have support on absolutely everything. I called &#8216;bullshit&#8217; and the whole thing went round in circles a bit until we both realised that were actually singing from the same song sheet, but from different ends of the room.</p>

<p><span class='pullquote-right' data-pullquote='it is always necessary to look at the actual requirements of the end-user'>
When looking at a solution it is always necessary to look at the actual requirements of the end-user. I had a conversation with a Director at $lastjob once. We had recently had a planned outage on the website for a few minutes one Sunday night so I could de-commission the old SAN. He said that he wanted us to get to 99.999% IT uptime. My reply after some quick calculations was that we had actually achieved that for the last 3 years at least, but that I would not like to guarantee it in the future with our current and planned infrastructure. This lead to him asking me to go ahead and do the calculations on how to guarantee it. When I went back to him with my figure (done using lots of Open Source, and no vendor support) he changed his mind. This was in what would be classed as an SME - heading towards £100 million a year turnover and one of world&#8217;s best in their field. Not a small company by any means, but they could not justify that cost.
</span></p>

<p>Having said that they could justify a lot. All our servers were clustered, storage was Fibre-channel, they had a 100TB 8Gb array for a team of 2 people who crunched monster video files all day. All that was justified expenditure, but they were not an internet company, so a bit of downtime could be justified. Even when we had a major disaster and a large swathe of Linux VMs disappeared from this world, nobody actually had to stop working and no money was lost.</p>

<p>A small business is not going to dump the money for multi-thousand pound Cisco router and a zero-contention synchronous internet connection. They may think that they need the best of everything, they may even be willing to pay for it if they have got enough of daddy&#8217;s funding behind them. However that would be foolish, that money would be better spent on giving everyone a Christmas bonus.</p>

<p>Support contracts are another bone of contention. Now everything I have is under one, but that is not always necessary. I once needed to get a couple of TBs of storage into a large office asap. I happenned to have a few FC HBAs, a couple of old Proliants and a pile of MSA1000s in a cupboard. I built up a box with a pair of HBAs and a single MSA1000 and sent the whole lot up to the office with strict instructions that all the extras were for spares only. If something broke, no need for support - just swap it out. I figured it would be good for at least another 3 years. Especially as backups were pretty reliable there. Would a new SAN with expensive support have been more reliable, I doubt it. We would have to wait 4 hours for a new disk, rather than the 5 minutes a took to walk to the cupboard.</p>

<p><span class='pullquote-right' data-pullquote='wisdom can fall at either end of the price-spectrum'>
It is not always necessary to get the shiniest stuff, with the longest/quickest support contract. We know our gear, we know how reliable it is, we know how long it lasts. The people paying the bills do not, they rely on us to advise them honestly and wisely. That wisdom can fall at either end of the price-spectrum, but needs to be based on the ACTUAL risks and their effect.
</span></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[In praise of old school UNIX]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2013/02/05/in-praise-of-old-school-unix/"/>
    <updated>2013-02-05T16:13:00+01:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2013/02/05/in-praise-of-old-school-unix</id>
    <content type="html"><![CDATA[<p>What am I doing today? Documentation that is what. I am writing a document on how to do <a href="http://www.chriscowley.me.uk/blog/2012/11/19/sftp-chroot-on-centos/">this</a>. To any Linux user it is a very simple process and I could just give them a link to my own website.</p>

<!--more-->


<p>I am not writing this for a techinical audience though. The people who are going to perform this work will be the &#8216;Level 1 operatives&#8217;. This translates roughly to &#8220;anyone we can find on the street corners of some Far East city&#8221;. If I tell them to press the red button labelled &#8220;press me&#8221; and it turns out to be orange, they will stop. I cannot assume the ability to edit a file in Vi. How can you work around this, well you need to make everything a copy and paste operation. This is easily done in Bash thanks to IO redirection and of course Sed.</p>

<p>Now, a brief recap may be in order, as there are some perfectly knowledgable Linux users who do not know what Sed is. Really, one of them sits behind me. Sed stands for Stream EDitor, and it parses text and applies transformations to it. It was one of the first UNIX utilities. It kind of sits between <a href="https://en.wikipedia.org/wiki/Grep">Grep</a> and <a href="https://en.wikipedia.org/wiki/AWK_programming_language">Awk</a> and is <a href="http://uuner.doslash.org/forfun/">surprisingly powerful</a>.</p>

<p>Anyway, I need to edit a line in a file then add a block of code at the end.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cp -v /etc/ssh/sshd_config{,.dist}
</span><span class='line'>sed -i ''/^Subsystem/s#/usr/libexec/openssh/sftp-server#internal-sftp#g' \ 
</span><span class='line'>    /etc/ssh/sshd_config</span></code></pre></td></tr></table></div></figure>


<p>First line obviously is a contracted cp line which puts the suffix <em>.dist</em> on the copy.</p>

<p>The basic idea is that it runs through the file (/etc/ssh/sshd_config) and looks for any line that starts with &#8220;Subsystem&#8221; (<code>/^Subsystem/</code>). If it finds a line that matches it then will perform a &#8220;substituion&#8221; (<code>/s#</code>). The next 2 blocks tell it what the substitution will be in the order &#8220;#From#To#&#8221;. The reason for  the change from <code>/</code> to <code>#</code> is because of the / in the path name (thanks to <a href="http://www.reddit.com/user/z0nk">Z0nk</a>  for reminding me that you can use arbitary seperators). The &#8220;#g&#8221; tells Sed to perform the substituion on every instance it finds on the line, rather than just the first one. It is completely superfluous in this example, but I tend to put it in from force of habit. Finally the &#8220;-i&#8221; tells Sed to perform the edit in place, rather than outputing to Stdout.</p>

<p>The next bit is a bit cleverer. With a single command I want to add a block of text to the file.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cat &lt;&lt;EOF | while read inrec; do echo $inrec &gt;&gt; /etc/ssh/sshd_config; done
</span><span class='line'>Match Group transfer
</span><span class='line'>ChrootDirectory /var/local/
</span><span class='line'>ForceCommmand internal-sftp
</span><span class='line'>X11Forwarding no
</span><span class='line'>AllowTcpForwarding no
</span><span class='line'>
</span><span class='line'>EOF</span></code></pre></td></tr></table></div></figure>


<p>Here <code>cat &lt;&lt;EOF</code> tells it send everything you type to Stdout until it sees the string EOF. This then gets piped to a <code>while</code> loop that appends each line of that Stdout to the file we want to extend (<em>/etc/ssh/sshd_config</em> in this case).</p>

<p>Using these old tools and a bit of knowledge of how redirection works has enabled me to make a document that anyone who can copy/paste can follow. It is very easy for technical people to forget that not everyone has the knowledge we have. To us opening Vi is perfectly obvious, but to others maybe it isn&#8217;t and they are not being paid enough to know. They are just being paid to follow a script. I may not like it, but it is the case - it also helped turn a boring documentation session into something a little more interesting. Which is nice!</p>
]]></content>
  </entry>
  
</feed>
