<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: storage | Just Another Linux Blog]]></title>
  <link href="http://www.chriscowley.me.uk/blog/categories/storage/atom.xml" rel="self"/>
  <link href="http://www.chriscowley.me.uk/"/>
  <updated>2013-02-21T16:43:36+01:00</updated>
  <id>http://www.chriscowley.me.uk/</id>
  <author>
    <name><![CDATA[Chris Cowley]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Something from the shadows]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2013/02/21/something-from-the-shadows/"/>
    <updated>2013-02-21T16:06:00+01:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2013/02/21/something-from-the-shadows</id>
    <content type="html"><![CDATA[<p>An intriguing startup came out of stealth mode a few days ago. <a href="http://pernixdata.com/">Pernix Data</a> was founded by Pookan Kumar and satyam Vaghani, both of who were pretty near top of the pile in VMware's storage team.</p>

<!--more -->


<p>What they are offering is, to me at least, a blinding flash of the obvious. It is a softweare layer that runs on a VMware hypervisor that uses local flash as a cache for whatevery is coming off your main storage array. <img class="right" src="http://pernixdata.com/images/home_graphic3.png" width="300" height="217">. That could be an SSD (or multiple) or a PCI-e card.</p>

<p>Reading what they have to say, it is completely transparent to the hypervisor, so everything just works. Obviously me being an Open Source fanatic I imediately started thinking how I could do this with Linux; it took me about 5 minutes.</p>

<p>You take your SAN array and give your LUN to your Hypervisors (running KVM obviously, and with a local SSD). Normally you would stick a clustered file system (such as GFS2) on that shared LUN. Instead you use a tiered block device on top of that LUN. There are two that come immediately to mind: <a href="https://github.com/facebook/flashcache/">Flashcache</a> and <a href="http://sourceforge.net/projects/tier/files/">Btier</a>.</p>

<p>Finally, you can put your clustered file system on that tiered device. I do not have the time or facilities to test this, but I cannot see why it would not work. Maybe someone at Red Hat (seeing as they do the bulk of KVM and GFS2 development) can run with this and see what happens.
What their plans are I do not know. It is very early days, maybe they will be a success maybe not. As they are both ex-VMware, I would not be at all surprised if they get bought back into the VMware fold. Certainly this is a functionality that I would have like to have seen in the past.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What will HP do next?]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2012/12/17/what-will-hp-do-next/"/>
    <updated>2012-12-17T15:16:00+01:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2012/12/17/what-will-hp-do-next</id>
    <content type="html"><![CDATA[<p>HP recently announced a new range of 3Par based arrays that are aimed at mid-range enterprise. There now appear to be 2 ranges for the future:</p>

<!--more -->


<ul>
<li>HP StoreServ 10000 is the big boy, scales up to 1.6PB, 192 FC ports, 32 10Gb iSCSI - the works.</li>
<li>HP StoreServ 7000 is the mid-range one, with <em>only</em> 24 FC and 8 1-Gb iSCSI. This split into the 7200 (2U) and 7400 (4U)</li>
</ul>


<p>With the entry level <a href="http://www8.hp.com/us/en/hp-news/press-release.html?id=1332554#.UM8Mm3eTW01">7200 starting at $20k</a> that does not leave a lot of room at the low end for both the P4000 and the P2000 ranges. At the higher end the 7400 starts at $32k, which certainly leaves no space for the venerable EVA.</p>

<p>In <a href="http://h30507.www3.hp.com/t5/Around-the-Storage-Block-Blog/Blogger-Q-amp-A-with-David-Scott/ba-p/128097">an interview with Around the Storage Block</a> HP Storage GM David Scott is quite critical of EMC who have a range of different and fairly unrelated product lines (Atmos, VMAX, VNX/VNXe, Isilion). For now HP is fairly similar: P9000 (Hitachi), P4000 (Lefthand, P2000 (Dot Hill). When you look at where they have priced the 3Par gear, it does appear that they are betting the farm on it.</p>

<p>Something I have been quite vocal about over the last 5 years or so is that fact that HP's storage portfolio is all over the place. Compared to Netapp, who have a very homogenous portfolio (everything runs OnTAP, you know one Netapp product, you can jump on to the rest), HP have got a one interface for P2000, another for P4000, another for EVA. Nothing sits together. HP needs to get all this in line. EMC have already started with Unisphere, but they still have multiple product architectures (VMAX, VNX, Isilion for example).</p>

<p>I personally think that these other ranges will drop by the wayside, although I am reading a bit between the lines here. Dot Hill do seem to be setting themselves up to be more than just an OEM supplier to HP. Maybe it is wishful thinking as I am a <a href="http://www.chriscowley.me.uk/blog/2010/01/12/some-great-new-san-gear/">huge fan of Dot Hill</a>, but they have some very interesting products. I hope/expect to see a lot more of Dot Hill themselves over the next few years, rather than just being behind Oracle/Netapp/HP badges.</p>

<p>The P9000 range is a similar story at the other end of the market. The Storserve 10k seems to be very similar, pretty much the same capacity and number of ports. Feature set is also close enough. I also have the impression that Hitachi are starting to push a bit harder in their own right as well.</p>

<p>Essentially I think 3Par will become HP's own architecture. It has the flexibity to  cover everything from a single bay with 12 SATA disks at the low end (perhaps on DL hardware) all the way up to PB+ scales, taking in all-flash on the way.</p>

<p>This leaves the P4000, which has been re-branded the <a href="http://www8.hp.com/us/en/products/disk-storage/product-detail.html?oid=4118659">StoreVirtual 4000</a>. This seems to me to be a no-brainer. It is already running on commodity DL180 hardware and includes an appliance option. My guess is the physical implementation of this will be phased out. It will become the Virtual Appliance front-end to all this new 3Par based physical goodness.</p>

<p>Finally, I have skipped over the EVA. What does the future hold in store for HP's venerable high-end platform. I think nothing. It will go into maintenance mode and be quietly end-of-life'd. Existing customers will be pushed to <a href="http://h30507.www3.hp.com/t5/Around-the-Storage-Block-Blog/EVA-to-HP-3PAR-StoreServ-online-import/ba-p/128391">migrate</a> over to Storeserv 7000.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[EMC ExtremIO Thoughts]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2012/12/10/emc-extremio-thoughts/"/>
    <updated>2012-12-10T16:39:00+01:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2012/12/10/emc-extremio-thoughts</id>
    <content type="html"><![CDATA[<p>There has been quite a bit of musing recently on the web about EMC and what will come out of their ExtremIO acquisition. They have recently (finally) started demonstrating an all-flash array. The name says it all: ExtremIO. It is for super high IOPS applications - Virtual desktops, enormous DBs, that sort of thing.</p>

<!-- more -->


<p>It is a bit of a depature from traditional EMC, in that it <a href="http://storagenewsletter.com/news/systems/all-ssd-system-from-emc-xtremio-">appears</a> that it is going to be a true scale-out architecture. This is has more in common with Isilion (not developed at EMC) than VMAX (developed at EMC).</p>

<p>The problem is that EMC are <em>extremely</em> late to the market this time around. VMAX was ahead of the curve by adding flash. In the all flash arena there are several options already there, <a href="http://violin-memory.com">Violin</a>, <a href="http://whiptail.com/">Whiptail</a> spring straight to mind.</p>

<p>Over at <a href="http://blog.thestoragearchitect.com/2012/12/10/xtremio-aka-project-x-wheres-the-innovation/">The Storage Architect</a> Chris Evans gives the standard counter-arguments to EMC's marketing spin. Namely:</p>

<ol>
<li>Other vendor solutions aren't as resilient</li>
<li>It's a 1.0 product, expect more from 2.0 and beyond</li>
<li>It gives our customers choice</li>
</ol>


<p>I hate to say it, but EMC have one HUGE advantage over all the startups. Quite simply they are EMC! As experts we know that Violin (for example) have a more mature product than EMC do.</p>

<p><span class='pullquote-right' data-pullquote='When the guy with the credit card sees the name &#8220;EMC&#8221; it will be hard to persuade him that such a mature brand has the more immature product'>
When the guy with the credit card sees the name "EMC" it will be hard to persuade him that such a mature brand has the more immature product. This won't be the case everywhere, but in a lot of large enterprise they would go to their storage experts (like me) and ask for advice on which flash array to go for; they then stipulate that it has to come from EMC's portfolio. At which point I through my hands up in despair, tell the to buy ExtremIO, the guy who has the better, more mature, solution loses the business that was rightfully theirs.
</span></p>

<p>It is not a 1.0 product, I can not accept that EMC acquired ExtremIO based on stuff that was only on paper. At best this is a 1.5 product, but realistically it is a 2.0 product. From a company with the resources of EMC, this should be coming out of the blocks running - it should be the best in class. OK, ExtremIO were further from version 1.0 than EMC were maybe lead to believe at the time, but they have got a lot of resources. After a year, they should not be in damage control mode.</p>

<p>Does it really give the customers choice? I would go one step further than what Chris has said - that an all-flash VMAX or VNX would have made sense. I agree with him, but I also think that they have actually removed choice.</p>

<p>I would say that EMC have cocked-up here. They under-estimated the market for all-flash arrays. Even my <a href="http://www.violin-memory.com/news/press-releases/nats-selects-violin-memory-flash-storage-for-virtual-desktop-infrastructure/">old employer have got some</a> and that is in Air Traffic control - there is no-one else who relies more on "tried and tested" technology than them. They then rushed to through some money at the problem, but like of Violin were already happy where they were.</p>

<p><span class='pullquote-right' data-pullquote='I wish that EMC would be punished for this'>
Robin Harris at <a href="http://storagemojo.com/2012/12/05/emcs-xtreme-embarrassment/">StorageMojo</a> thinks this will be a costly mistake for EMC. I disagree, I think by announcing that this is coming EMC will stall the market and thus come out of this fine. Unfortunately there is too much latency in the enterprise storage space for it to be otherwise. I wish it were a bit more dynamic and I wish that EMC would be punished for this, thus rewarding one of the underdogs. That does not happen enough in the enterprise space, especially for an Englishman like me (we do love the underdogs).
</span></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Add SAN functions to Highly Available NFS/NAS]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2012/03/20/add-san-functions-to-highly-available-nfs-slash-nas/"/>
    <updated>2012-03-20T21:07:00+01:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2012/03/20/add-san-functions-to-highly-available-nfs-slash-nas</id>
    <content type="html"><![CDATA[<p>This based on my last post where I documented building a Highly Available NFS/NAS server.</p>

<p>There is not a huge amount that needs to be done in order to add iSCSI functionality as well.</p>

<!-- more -->


<p>Add a file called <em>/etc/drbd/iscsi.res</em> containing:</p>

<p>```
resource iscsi {</p>

<pre><code>on nfs1 {
    device /dev/drbd1;
    disk   /dev/vdc;
    meta-disk internal;
    address   10.0.0.1:7789;
}
on nfs2 {
    device /dev/drbd1;
    disk   /dev/vdc;
    meta-disk internal;
    address   10.0.0.2:7789;
}
</code></pre>

<p>}
```</p>

<p>This differs from the previous resource in 2 ways. Obviously it using a different physical disk. Also the port number of the address is incremented; each resource has to have its own port to communicate on.</p>

<h2>Configure Heartbeat</h2>

<p>Add a new resource to <em>/etc/ha.d/haresources</em>:</p>

<p><code>
iscsi1.snellwilcox.local IPaddr::10.0.0.101/24/eth0 drbddisk::iscsi tgtd
</code></p>

<p>Same primary host, new IP address, new drbd resource and of course the service to be controlled (tgtd in this case).</p>

<p>I also made a couple of changes to <em>/etc/ha.d/ha.cf</em>:</p>

<p><code>
keepalive 500ms
deadtime 5
warntime 10
initdead 120
</code></p>

<p>This changes the regularity of the heartbeat packets from every 2 seconds to 2 every second. We also say that a node is dead after only 5 seconds rather than after 30.</p>

<h2>Configure an iSCSI Target</h2>

<p>Tgtd has a config file that you can use in <em>/etc/tgt/targets.conf</em>. It is an XML file, so add entry like:</p>

<p>```
<target iqn.2011-07.world.server:target0></p>

<pre><code>    # provided devicce as a iSCSI target
    backing-store /dev/vg_matthew/lv_iscsi1
    # iSCSI Initiator's IP address you allow to connect
    initiator-address 192.168.1.20
    # authentication info ( set anyone you like for "username", "password" )
</code></pre>

<p></target>
```</p>

<p>The target name is by convention <em>iqn.year-month.reverse-domainname:hostname.targetname</em>. Each backing store will be a seperate LUN. A discussion of this is out of the scope of this article.</p>

<p>By default, this config file is disabled. Enable it by un-commenting the line <code>#TGTD_CONFIG=/etc/tgt/targets.conf</code> in <em>/etc/sysconfig/tgtd</em>. You can now enable the target with service tgtd reload.</p>

<p>Now when you run <code>tgtadm –mode target –op show</code> you should get something like:</p>

<p>```
Target 1: iqn.2012-03.com.example:iscsi.target1</p>

<pre><code>System information:
    Driver: iscsi
    State: ready
I_T nexus information:
LUN information:
    LUN: 0
        Type: controller
        SCSI ID: IET     00010000
        SCSI SN: beaf10
        Size: 0 MB, Block size: 1
        Online: Yes
        Removable media: No
        Readonly: No
        Backing store type: null
        Backing store path: None
        Backing store flags:
    LUN: 1
        Type: disk
        SCSI ID: IET     00010001
        SCSI SN: beaf11
        Size: 8590 MB, Block size: 512
        Online: Yes
        Removable media: No
        Readonly: No
        Backing store type: rdwr
        Backing store path: /dev/drbd/by-res/iscsi
        Backing store flags:
Account information:
ACL information:
    ALL
</code></pre>

<p>```</p>

<h2>Connect An Initiator</h2>

<p>Install the iscsi utils:</p>

<p><code>
yum install iscsi-initiator-utils
chkconfig iscsi on
chkconfig iscsid on
</code></p>

<p>Discover the targets on the host and login to the target.
<code>
iscsiadm -m discovery -t sendtargets -p 10.0.0.101
iscsiadm -m node --login
</code></p>

<p>If you run <code>cat /proc/partitions</code> you will see an new partition has appeared. You can do whatever you want with it.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Highly Available NFS/NAS]]></title>
    <link href="http://www.chriscowley.me.uk/blog/2012/03/19/highly-available-nfs-slash-nas/"/>
    <updated>2012-03-19T16:59:00+01:00</updated>
    <id>http://www.chriscowley.me.uk/blog/2012/03/19/highly-available-nfs-slash-nas</id>
    <content type="html"><![CDATA[<p>Take 2 Centos Servers (nfs1 and nfs2 will do nicely) and install ELrepo and EPEL on them both:</p>

<!-- more -->


<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yum install \&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;pre>&lt;code>http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-5.noarch.rpm \
</span><span class='line'>http://elrepo.org/elrepo-release-6-4.el6.elrepo.noarch.rpm --nogpgcheck
</span><span class='line'>&lt;/code>&lt;/pre>
</span><span class='line'>
</span><span class='line'>&lt;p></span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>Each of them should ideally have 2 NICS, with the secondary ones just used for DRBD sync purposes. We’ll give these the address 10.0.0.1/32 and 10.0.0.2/32.</p>

<p>I am also assuming that you have disabled the firewall and SELinux – I do not recommend that for production, but for testing it is fine.</p>

<h2>DRBD Configuration</h2>

<p>Install DRBD 8.4 on the both:
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yum install drbd84-utils kmod-drbd84</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>On each node the file /etc/drbd.d/global_common.conf should contain:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>global {
</span><span class='line'>  usage-count yes;
</span><span class='line'>}
</span><span class='line'>common {
</span><span class='line'>  net {&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;pre>&lt;code>protocol C;
</span><span class='line'>&lt;/code>&lt;/pre>
</span><span class='line'>
</span><span class='line'>&lt;p>  }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>and /etc/drbd.d/main.res should contain:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>resource main {
</span><span class='line'>  on nfs1 {&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;pre>&lt;code>device    /dev/drbd0;
</span><span class='line'>disk      /dev/sdb;
</span><span class='line'>address   10.0.0.1:7788;
</span><span class='line'>meta-disk internal;
</span><span class='line'>&lt;/code>&lt;/pre>
</span><span class='line'>
</span><span class='line'>&lt;p>  }
</span><span class='line'>  on nfs2 {&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;pre>&lt;code>device    /dev/drbd0;
</span><span class='line'>disk      /dev/sdb;
</span><span class='line'>address   10.0.0.2:7788;
</span><span class='line'>meta-disk internal;
</span><span class='line'>&lt;/code>&lt;/pre>
</span><span class='line'>
</span><span class='line'>&lt;p>  }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>On both nodes you will need to create the resource metadata:
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>drbdadm create-md main</span></code></pre></td></tr></table></div></figure></notextile></div>
and start the daemons
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>service drbd start
</span><span class='line'>chkconfig drbd on</span></code></pre></td></tr></table></div></figure></notextile></div>
Now <code>service drbd status</code> will give you:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>drbd driver loaded OK; device status:
</span><span class='line'>version: 8.4.1 (api:1/proto:86-100)
</span><span class='line'>GIT-hash: 91b4c048c1a0e06777b5f65d312b38d47abaea80 build by dag@Build64R6, 2011-12-21 06:08:50
</span><span class='line'>m:res   cs         ro                   ds                         p  mounted  fstype
</span><span class='line'>0:main  Connected  Secondary/Secondary  Inconsistent/Inconsistent  C</span></code></pre></td></tr></table></div></figure></notextile></div>
Both devices or secondary and inconsistent, this is normal at this stage. Choose a node to be your primary and run:
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>drbdadm primary --force main</span></code></pre></td></tr></table></div></figure></notextile></div>
And it start sync’ing, which will take a long time. You can temporarily make it faster with (on one node:
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>drbdadm disk-options --resync-rate=110M main</span></code></pre></td></tr></table></div></figure></notextile></div>
Put it back again with drbdadm adjust main</p>

<p>On your primary node you can now create a fiiesystem. I’m using ext4 for no good reason other than it being the default. Use whatever you are most comfortable with.
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>mkfs.ext4 /dev/drbd0</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<h2>Configure NFS</h2>

<p>If you diid a minimal Centos install, then you willl need to install the nfs-utils package (yum install nfs-utils). Prepare your mount points and exports on both servers:
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>mkdir /drbd
</span><span class='line'>echo "/drbd/main *(rw)" >> /etc/exports</span></code></pre></td></tr></table></div></figure></notextile></div>
Now we do the actual NFS set up. We previously choose nfs1 as our master when you used it to trigger the initial sync. On nfs1 mount the replicated volumes, move the NFS data to it, then create symlinks to our replicated data.
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>mount /dev/drbd0 /drbd
</span><span class='line'>mkdir /drbd/main
</span><span class='line'>mv /var/lib/nfs/ /drbd/
</span><span class='line'>ln -s /drbd/nfs/ /var/lib/nfs
</span><span class='line'>umount /drbd</span></code></pre></td></tr></table></div></figure></notextile></div>
If you get errors about not bring able to remove directories in /var/lib/nfs do not worry.</p>

<p>Now a little preparation on nfs2:
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>mv /var/lib/nfs /var/lib/nfs.bak
</span><span class='line'>ln -s /drbd/nfs/ /var/lib/nfs</span></code></pre></td></tr></table></div></figure></notextile></div>
This will create a broken symbolic link, but it will be fixed when everything fails over.</p>

<h2>Heartbeat Configuration</h2>

<p>Heartbeat is in the EPEL repository, so enable that and install it on both nodes:
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yum -y install heartbeat</span></code></pre></td></tr></table></div></figure></notextile></div>
Make sure that <em>/etc/ha.d/ha.cf</em> contains:
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>keepalive 2
</span><span class='line'>deadtime 30
</span><span class='line'>bcast eth0
</span><span class='line'>node nfs1 nfs2</span></code></pre></td></tr></table></div></figure></notextile></div>
The values in node should be whatever <code>uname -n</code> returns.</p>

<p>Now create <em>/etc/ha.d/haresources</em>:
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>nfs1 IPaddr::10.0.0.100/24/eth0 drbddisk::main Filesystem::/dev/drbd0::/drbd::ext4 nfslock nfs</span></code></pre></td></tr></table></div></figure></notextile></div>
That is a little cryptic, so I’ll explain; nfs1 is the primary node, IPaddr sets up a floating address on eth0 that our clients will connect to. This has a resource drbddisk::main bound to it, which sets our main to resource to primary on nfs1. Filesystem mounts /dev/drbd0 at /drbd on nfs1. Finally the the services nfslock and nfs are started on nfs1.</p>

<p>Finally, it needs an authentication file in /etc/ha.d/authkeys, which should be chmod’ed to 600 to be only readable by root.
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>auth 3
</span><span class='line'>3 md5 mypassword123</span></code></pre></td></tr></table></div></figure></notextile></div>
You should also make sure that nfslock and nfs do not start up by themselves:
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>chkconfig nfs off
</span><span class='line'>chkconfig nfslock off</span></code></pre></td></tr></table></div></figure></notextile></div>
Now you can start heartbeat and check it is working:
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>service heartbeat start
</span><span class='line'>chkconfig heartbeat on</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<h2>Testing</h2>

<p>Running <code>ifconfig</code> on nfs1 should give you something like:
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>eth0      Link encap:Ethernet  HWaddr 52:54:00:84:73:BD&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;pre>&lt;code>      inet addr:10.0.0.1  Bcast:10.0.0.255  Mask:255.255.255.0
</span><span class='line'>      inet6 addr: fe80::5054:ff:fe84:73bd/64 Scope:Link
</span><span class='line'>      UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
</span><span class='line'>      RX packets:881922 errors:0 dropped:0 overruns:0 frame:0
</span><span class='line'>      TX packets:1302012 errors:0 dropped:0 overruns:0 carrier:0
</span><span class='line'>      collisions:0 txqueuelen:1000
</span><span class='line'>      RX bytes:239440621 (228.3 MiB)  TX bytes:5791818459 (5.3 GiB)
</span><span class='line'>&lt;/code>&lt;/pre>
</span><span class='line'>
</span><span class='line'>&lt;p>eth0:0    Link encap:Ethernet  HWaddr 52:54:00:84:73:BD&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;pre>&lt;code>      inet addr:10.0.0.100  Bcast:10.0.0.255  Mask:255.255.255.0
</span><span class='line'>      UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
</span><span class='line'>&lt;/code>&lt;/pre>
</span><span class='line'>
</span><span class='line'>&lt;p>lo        Link encap:Local Loopback&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;pre>&lt;code>      inet addr:127.0.0.1  Mask:255.0.0.0
</span><span class='line'>      inet6 addr: ::1/128 Scope:Host
</span><span class='line'>      UP LOOPBACK RUNNING  MTU:16436  Metric:1
</span><span class='line'>      RX packets:2 errors:0 dropped:0 overruns:0 frame:0
</span><span class='line'>      TX packets:2 errors:0 dropped:0 overruns:0 carrier:0
</span><span class='line'>      collisions:0 txqueuelen:0
</span><span class='line'>      RX bytes:224 (224.0 b)  TX bytes:224 (224.0 b)
</span><span class='line'>&lt;/code>&lt;/pre>
</span><span class='line'>
</span><span class='line'>&lt;p></span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>Note an entry for <em>eth0:0</em> has miraculously appeared.</p>

<p>Also <code>df</code> should include the entry:
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>/dev/drbd0             20G  172M   19G   1% /drbd</span></code></pre></td></tr></table></div></figure></notextile></div>
Reboot nfs1 and the services should appear on nfs2.</p>

<p>Connect an NFS client to you floating address (10.0.0.100) and you should be able to kill the live node and it will carry on.</p>
]]></content>
  </entry>
  
</feed>
