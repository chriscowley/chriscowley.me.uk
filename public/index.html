
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Just Another Linux Blog</title>
  <meta name="author" content="Chris Cowley">

  
  <meta name="description" content="Install Skype – I used these instructions. This will seem to get everything working, but video will just give you a black screen and no error message &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://http://warm-sword-7449.herokuapp.com//">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="/javascripts/ender.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="Just Another Linux Blog" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  

</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Just Another Linux Blog</a></h1>
  
    <h2>quite a bit about cycling as well</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:http://warm-sword-7449.herokuapp.com/" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/about-me">About Me</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/04/18/skype-video-on-fedora-64bit/">Skype Video on Fedora 64bit</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-04-18T15:53:00+01:00" pubdate data-updated="true">Apr 18<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Install Skype – I used <a href="http://slayachronicles.blogspot.co.uk/2012/03/installing-skype-on-fedora-16-64-bit.html" target="_blank" >these</a> instructions. This will seem to get everything working, but video will just give you a black screen and no error message. This is because Skype is 32 bit and you webcam driver is 64 bit. Make sure you have libv4l.i686 installed.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sudo yum install libv4l.i686</span></code></pre></td></tr></table></div></figure>


<p>Now create a wrapper script to launch it with a custom environment. I put it in <em>/usr/local/bin/skype</em></p>

<figure class='code'><figcaption><span> (skype)</span> <a href='/downloads/code/skype'>download</a></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c">#!/bin/bash</span>
</span><span class='line'><span class="nv">LD_PRELOAD</span><span class="o">=</span>/usr/lib/libv4l/v4l1compat.so  /usr/bin/skype
</span></code></pre></td></tr></table></div></figure>


<p>This will now get loaded before the main Skype executable and you will have a working video device.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/04/16/a-learning-experience/">A Learning Experience</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-04-16T20:30:00+01:00" pubdate data-updated="true">Apr 16<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>How many times have you installed/updated a bit of software and read the line “Please take a back up” or something to that effect? 99 times out of a hundred, you will just continue and ignore it.</p>

<p>Today I had a reminder of why it is import to do so. I did a routine plug-in upgrade on our Jira installation (Customware Salesforce connector for those who want to know). I have done this several times, I had tested it in our Dev installation I was 100% confident it would work as expected. However, I actually decided to take a backup anyway.</p>

<p>I ran the upgrade in the production environment and re-indexed. Nothing out of the ordinary. 10% of the way into the index it fell over. Jira’s database was gone! Fortunately I was able to restore from my backup and at worst a comment or two was lost, but that still caused significant downtime.</p>

<p>I had done everything I could to make sure the upgrade would go smoothly, but it still did not. That is why software vendors always tell you to take a backup before even the smallest change – DO IT!</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/04/05/pycurl-and-self-signed-ssl-certificates/">PyCurl and Self-signed SSL Certificates</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-04-05T16:50:00+01:00" pubdate data-updated="true">Apr 5<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>At <a href="http://www.snellgroup.com" target="_blank">Snell</a> we make heavy use of self-signed certificates for internal websites, such as the R&amp;D wiki. Active Directory makes it easy for us to make this transparent to the users, those that use Firefox/Chrome can find our well-published instructions to add the CA certificate to their own browsers.</p>

<p>Today I was writing a script to that pulls lots of attachments off our Confluence wiki, which we access through SSL using one of those certificates. Of course PyCurl  moaned that it could not verify the host, but I did not care – I know it is the right host!</p>

<p>Finding documentation both on SSL and PyCurl is problematic at best. OpenSSL’s documentation it complete, but could not be more unreadable if written by a right-handed doctor using a broken crayon with his left-hand; pyCurl’s documentation is non-existent.</p>

<p>After an hour of Google-Fu and DuckDuckGo-Fu I finally managed to do what I wanted:</p>

<figure class='code'><figcaption><span> (pycurl_ssl.py)</span> <a href='/downloads/code/pycurl_ssl.py'>download</a></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='py'><span class='line'><span class="c">#!/usr/bin/env python</span>
</span><span class='line'><span class="n">downloadedFile</span> <span class="o">=</span> <span class="s">&quot;/tmp/stuff&quot;</span>
</span><span class='line'><span class="n">outfile</span> <span class="o">=</span> <span class="nb">file</span><span class="p">(</span><span class="n">downloadedFile</span><span class="p">,</span> <span class="s">&#39;wb&#39;</span><span class="p">)</span>
</span><span class='line'><span class="n">url</span> <span class="o">=</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">someurl</span><span class="o">.</span><span class="n">example</span><span class="o">.</span><span class="n">com</span>
</span><span class='line'><span class="n">c</span> <span class="o">=</span> <span class="n">pycurl</span><span class="o">.</span><span class="n">Curl</span><span class="p">()</span>
</span><span class='line'><span class="n">c</span><span class="o">.</span><span class="n">setopt</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">URL</span><span class="p">,</span> <span class="n">url</span><span class="p">)</span>
</span><span class='line'><span class="n">c</span><span class="o">.</span><span class="n">setopt</span><span class="p">(</span><span class="n">pycurl</span><span class="o">.</span><span class="n">USERPWD</span><span class="p">,</span> <span class="s">&quot;</span><span class="si">%s</span><span class="s">:</span><span class="si">%s</span><span class="s">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">username</span><span class="p">,</span> <span class="n">password</span><span class="p">))</span>
</span><span class='line'><span class="n">c</span><span class="o">.</span><span class="n">setopt</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">WRITEFUNCTION</span><span class="p">,</span> <span class="n">outfile</span><span class="o">.</span><span class="n">write</span><span class="p">)</span>
</span><span class='line'><span class="n">c</span><span class="o">.</span><span class="n">setopt</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">SSL_VERIFYPEER</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="c"># That is you key line for this purpose!</span>
</span><span class='line'><span class="n">c</span><span class="o">.</span><span class="n">perform</span><span class="p">()</span>
</span><span class='line'><span class="n">c</span><span class="o">.</span><span class="n">close</span>
</span></code></pre></td></tr></table></div></figure>


<p>There you go!</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/03/19/highly-available-nfs-slash-nas/">Highly Available NFS/NAS</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-03-19T16:59:00+00:00" pubdate data-updated="true">Mar 19<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Take 2 Centos Servers (nfs1 and nfs2 will do nicely) and install ELrepo and EPEL on them both:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yum install \
</span><span class='line'>    http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-5.noarch.rpm \
</span><span class='line'>    http://elrepo.org/elrepo-release-6-4.el6.elrepo.noarch.rpm --nogpgcheck</span></code></pre></td></tr></table></div></figure>


<p>Each of them should ideally have 2 NICS, with the secondary ones just used for DRBD sync purposes. We’ll give these the address 10.0.0.1/32 and 10.0.0.2/32.</p>

<p>I am also assuming that you have disabled the firewall and SELinux – I do not recommend that for production, but for testing it is fine.</p>

<h2>DRBD Configuration</h2>

<p>Install DRBD 8.4 on the both:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yum install drbd84-utils kmod-drbd84</span></code></pre></td></tr></table></div></figure>


<p>On each node the file /etc/drbd.d/global_common.conf should contain:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>global {
</span><span class='line'>  usage-count yes;
</span><span class='line'>}
</span><span class='line'>common {
</span><span class='line'>  net {
</span><span class='line'>    protocol C;
</span><span class='line'>  }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>and /etc/drbd.d/main.res should contain:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>resource main {
</span><span class='line'>  on nfs1 {
</span><span class='line'>    device    /dev/drbd0;
</span><span class='line'>    disk      /dev/sdb;
</span><span class='line'>    address   10.0.0.1:7788;
</span><span class='line'>    meta-disk internal;
</span><span class='line'>  }
</span><span class='line'>  on nfs2 {
</span><span class='line'>    device    /dev/drbd0;
</span><span class='line'>    disk      /dev/sdb;
</span><span class='line'>    address   10.0.0.2:7788;
</span><span class='line'>    meta-disk internal;
</span><span class='line'>  }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>On both nodes you will need to create the resource metadata:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>drbdadm create-md main</span></code></pre></td></tr></table></div></figure>


<p>and start the daemons</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>service drbd start
</span><span class='line'>chkconfig drbd on</span></code></pre></td></tr></table></div></figure>


<p>Now <code>service drbd status</code> will give you:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>drbd driver loaded OK; device status:
</span><span class='line'>version: 8.4.1 (api:1/proto:86-100)
</span><span class='line'>GIT-hash: 91b4c048c1a0e06777b5f65d312b38d47abaea80 build by dag@Build64R6, 2011-12-21 06:08:50
</span><span class='line'>m:res   cs         ro                   ds                         p  mounted  fstype
</span><span class='line'>0:main  Connected  Secondary/Secondary  Inconsistent/Inconsistent  C</span></code></pre></td></tr></table></div></figure>


<p>Both devices or secondary and inconsistent, this is normal at this stage. Choose a node to be your primary and run:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>drbdadm primary --force main</span></code></pre></td></tr></table></div></figure>


<p>And it start sync’ing, which will take a long time. You can temporarily make it faster with (on one node:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>drbdadm disk-options --resync-rate=110M main</span></code></pre></td></tr></table></div></figure>


<p>Put it back again with drbdadm adjust main</p>

<p>On your primary node you can now create a fiiesystem. I’m using ext4 for no good reason other than it being the default. Use whatever you are most comfortable with.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>mkfs.ext4 /dev/drbd0</span></code></pre></td></tr></table></div></figure>


<h2>Configure NFS</h2>

<p>If you diid a minimal Centos install, then you willl need to install the nfs-utils package (yum install nfs-utils). Prepare your mount points and exports on both servers:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>mkdir /drbd
</span><span class='line'>echo "/drbd/main *(rw)" >> /etc/exports</span></code></pre></td></tr></table></div></figure>


<p>Now we do the actual NFS set up. We previously choose nfs1 as our master when you used it to trigger the initial sync. On nfs1 mount the replicated volumes, move the NFS data to it, then create symlinks to our replicated data.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>mount /dev/drbd0 /drbd
</span><span class='line'>mkdir /drbd/main
</span><span class='line'>mv /var/lib/nfs/ /drbd/
</span><span class='line'>ln -s /drbd/nfs/ /var/lib/nfs
</span><span class='line'>umount /drbd</span></code></pre></td></tr></table></div></figure>


<p>If you get errors about not bring able to remove directories in /var/lib/nfs do not worry.</p>

<p>Now a little preparation on nfs2:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>mv /var/lib/nfs /var/lib/nfs.bak
</span><span class='line'>ln -s /drbd/nfs/ /var/lib/nfs</span></code></pre></td></tr></table></div></figure>


<p>This will create a broken symbolic link, but it will be fixed when everything fails over.</p>

<h2>Heartbeat Configuration</h2>

<p>Heartbeat is in the EPEL repository, so enable that and install it on both nodes:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yum -y install heartbeat</span></code></pre></td></tr></table></div></figure>


<p>Make sure that <em>/etc/ha.d/ha.cf</em> contains:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>keepalive 2
</span><span class='line'>deadtime 30
</span><span class='line'>bcast eth0
</span><span class='line'>node nfs1 nfs2</span></code></pre></td></tr></table></div></figure>


<p>The values in node should be whatever <code>uname -n</code> returns.</p>

<p>Now create <em>/etc/ha.d/haresources</em>:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>nfs1 IPaddr::10.0.0.100/24/eth0 drbddisk::main Filesystem::/dev/drbd0::/drbd::ext4 nfslock nfs</span></code></pre></td></tr></table></div></figure>


<p>That is a little cryptic, so I’ll explain; nfs1 is the primary node, IPaddr sets up a floating address on eth0 that our clients will connect to. This has a resource drbddisk::main bound to it, which sets our main to resource to primary on nfs1. Filesystem mounts /dev/drbd0 at /drbd on nfs1. Finally the the services nfslock and nfs are started on nfs1.</p>

<p>Finally, it needs an authentication file in /etc/ha.d/authkeys, which should be chmod’ed to 600 to be only readable by root.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>auth 3
</span><span class='line'>3 md5 mypassword123</span></code></pre></td></tr></table></div></figure>


<p>You should also make sure that nfslock and nfs do not start up by themselves:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>chkconfig nfs off
</span><span class='line'>chkconfig nfslock off</span></code></pre></td></tr></table></div></figure>


<p>Now you can start heartbeat and check it is working:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>service heartbeat start
</span><span class='line'>chkconfig heartbeat on</span></code></pre></td></tr></table></div></figure>


<h2>Testing</h2>

<p>Running <code>ifconfig</code> on nfs1 should give you something like:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>eth0      Link encap:Ethernet  HWaddr 52:54:00:84:73:BD  
</span><span class='line'>          inet addr:10.0.0.1  Bcast:10.0.0.255  Mask:255.255.255.0
</span><span class='line'>          inet6 addr: fe80::5054:ff:fe84:73bd/64 Scope:Link
</span><span class='line'>          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
</span><span class='line'>          RX packets:881922 errors:0 dropped:0 overruns:0 frame:0
</span><span class='line'>          TX packets:1302012 errors:0 dropped:0 overruns:0 carrier:0
</span><span class='line'>          collisions:0 txqueuelen:1000
</span><span class='line'>          RX bytes:239440621 (228.3 MiB)  TX bytes:5791818459 (5.3 GiB)
</span><span class='line'>
</span><span class='line'>eth0:0    Link encap:Ethernet  HWaddr 52:54:00:84:73:BD  
</span><span class='line'>          inet addr:10.0.0.100  Bcast:10.0.0.255  Mask:255.255.255.0
</span><span class='line'>          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
</span><span class='line'>
</span><span class='line'>lo        Link encap:Local Loopback  
</span><span class='line'>          inet addr:127.0.0.1  Mask:255.0.0.0
</span><span class='line'>          inet6 addr: ::1/128 Scope:Host
</span><span class='line'>          UP LOOPBACK RUNNING  MTU:16436  Metric:1
</span><span class='line'>          RX packets:2 errors:0 dropped:0 overruns:0 frame:0
</span><span class='line'>          TX packets:2 errors:0 dropped:0 overruns:0 carrier:0
</span><span class='line'>          collisions:0 txqueuelen:0
</span><span class='line'>          RX bytes:224 (224.0 b)  TX bytes:224 (224.0 b)</span></code></pre></td></tr></table></div></figure>


<p>Note an entry for <em>eth0:0</em> has miraculously appeared.</p>

<p>Also <code>df</code> should include the entry:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>/dev/drbd0             20G  172M   19G   1% /drbd</span></code></pre></td></tr></table></div></figure>


<p>Reboot nfs1 and the services should appear on nfs2.</p>

<p>Connect an NFS client to you floating address (10.0.0.100) and you should be able to kill the live node and it will carry on.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/02/08/home-made-redundant-thin-provisioned-san/">Home-made Redundant Thin-provisioned SAN</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-02-08T20:02:00+00:00" pubdate data-updated="true">Feb 8<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>The inspiration for this came from a mixture of problems I was having with my HP P2000, ideas that have been floating around my head for a while, plus a post over at Bauer-power.net. Basically I got given a bunch of warranty returned Supermicro servers from our Customer Service guys  and got tasked with making it our secondary VMware store and DR snapshot storage. Incidentally, the Supermicro servers are used for our Channel-in-a-box product for those you in the broadcast world. They are not ideal, the 2U 12 disk models that Pablo uses are far more suitable.</p>

<p>Plenty of companies already build their arrays on commodity hardware like these, so I am not doing anything new:</p>

<ul>
<li> Dell Compellent (Supermicro, soon to be Dell)</li>
<li> CoRAID (Supermicro)</li>
<li> EMC  Clarion and VNX</li>
<li> HP P4000 (HP DL180)</li>
<li> 3Par</li>
<li> Pure Storage</li>
<li> Nutanix</li>
<li> Solid Fire</li>
</ul>


<p>My set up is basically the same as that used by Pablo in the second iteration of his array:</p>

<ul>
<li> Linux</li>
<li> GlusterFS</li>
<li> Tgtd</li>
<li> Heartbeat</li>
</ul>


<p>There are a couple of differences:</p>

<ul>
<li> Mine uses a new version of GlusterFS which is currently in beta. This has several new features, the one I am interested in is Granular Locking. As I am storing VM images, I do not want these being locked during a self-heal – a problem in 3.2 and before. There are also other things such as object-storage (Amazon S3 compatible) for use with Open Stack. I’d love that, but I am not using it in my environment :( .</li>
<li> I am building on top of CentOS. I started with Red Hat and will continue to use it for server environments in the forceeable future.</li>
<li> I do not have de-duplication as I am not using ZFS, I am running on top of Ext4 and will use XFS or BTRFS if I need to. I am only using 8x 1TB drives as that is what I got given for free.</li>
</ul>


<p>I have had to build a couple of custom RPMS which I have made available in my <a href="http://yum.chriscowley.me.uk/el/6/x86_64/repoview/" target="blank">Yum repository</a>.</p>

<p>I did investigate de-duplication using LessFS, but sadly that is a no go as it does not currently support Extended Attributes, which are required by GlusterFS.</p>

<h2>Installation</h2>

<p>Install a basic CentOS 6 system on each node – the base system will be fine.</p>

<p>The two servers are</p>

<ul>
<li>server1 192.168.1.1(eth0),10.0.0.1(eth1)</li>
<li>server2 192.168.1.2(eth0),10.0.0.2(eth1)</li>
</ul>


<p>They connect to the rest of your network using eth0 and eth1 is a dedicated link between the 2. I would put them via a seperate switches/vLANs rather than a direct link, that way you can scale out your pool easily.</p>

<p>In the hosts file add:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>10.0.0.1 server1.example.com
</span><span class='line'>10.0.0.2 server2.example.com</span></code></pre></td></tr></table></div></figure>


<p>Add my repository:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>rpm --import http://yum.chriscowley.me.uk/RPM-GPG-KEY-ChrisCowley
</span><span class='line'>yum install http://yum.chriscowley.me.uk/el/6/x86_64/RPMS/chriscowley-release-1-1.noarch.rpm
</span><span class='line'>rpm --import https://fedoraproject.org/static/0608B895.txt
</span><span class='line'>yum install http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-5.noarch.rpm</span></code></pre></td></tr></table></div></figure>


<p>Now you can install the necessary packages, which is not many. :</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yum install glusterfs-core glusterfs-fuse heartbeat scsi-target-utils</span></code></pre></td></tr></table></div></figure>


<p>Now you can add create a pool of servers:</p>

<h2>GlusterFS</h2>

<p>From server1:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>gluster peer probe server2</span></code></pre></td></tr></table></div></figure>


<p>You next step is to configure a Gluster Volume. Gluster’s documentation for this is excellent. For our simple 2-node cluster we just want a simple replicated volume. As you grow, you can simple add extra pairs of nodes to expand your storage pool.</p>

<p>On each node create a folder to store the data and a mount-point for the replicated data:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>mkdir /exp1
</span><span class='line'>mkdir /mnt/test-volume</span></code></pre></td></tr></table></div></figure>


<p>Now create your volume and activate it(on a single node):</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>gluster volume create test-volume replica 2 transport tcp server1:/exp1 server2:/exp1
</span><span class='line'>gluster volume start test-volume</span></code></pre></td></tr></table></div></figure>


<p>Now you need to mount that volume on each of your nodes.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>echo "`hostname`:/test-volume /mnt/test-volume glusterfs defaults,noauto,_netdev 0 0" >> /etc/fstab
</span><span class='line'>echo "mount /mnt/test-volume" >> /etc/rc.local
</span><span class='line'>mount /mnt/test-volume</span></code></pre></td></tr></table></div></figure>


<h2>Heartbeat</h2>

<p>Now you need to configure heartbeat to control a floating IP address and the associated TGTD service. You need to create a few files on each node.</p>

<p>/etc/ha.d/authkeys:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>auth 2
</span><span class='line'>2 crc</span></code></pre></td></tr></table></div></figure>


<p>/etc/ha.d/ha.cf</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>logfacility     local0
</span><span class='line'>keepalive 500ms
</span><span class='line'>deadtime 5
</span><span class='line'>warntime 10
</span><span class='line'>initdead 120
</span><span class='line'>bcast eth1
</span><span class='line'>node server1
</span><span class='line'>node server2
</span><span class='line'>auto_failback no</span></code></pre></td></tr></table></div></figure>


<p>/etc/ha.d/haresources:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>server1 IPaddr::192.168.1.3/24/eth0 tgtd</span></code></pre></td></tr></table></div></figure>


<p>There are a couple of considerations. The Gluster filesystems need to be mounted before tgtd starts. Tgtd is in turn controled by Heartbeat (see the above haresources file). To this end make sure both heartbeat and tgtd are disabled and start heartbeat from /etc/rc.local.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>echo "/etc/init.d/heartbeat start" >> /etc/rc.local</span></code></pre></td></tr></table></div></figure>


<p>With all this done on both nodes, you can now start heartbeat on each node:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>/etc/init.d/heartbeat start</span></code></pre></td></tr></table></div></figure>


<p>Checking ifconfig will show that one of your nodes now has an <em>eth0:0</em> address.You will also find that tgtd is also running on that same node.</p>

<h2>iSCSI Target</h2>

<p>First create yourself a file to use as the backend for your iSCSI target:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>dd if=/dev/zero bs=1M count=40000 of=/mnt/test-volume/test.img</span></code></pre></td></tr></table></div></figure>


<p>or, if you prefer thin provisioned:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>dd if=/dev/zero bs=1M seek=40000 count=0 of=/mnt/test-volume/test.img</span></code></pre></td></tr></table></div></figure>


<p>You now need to define this file as a target. This requires the editting of 2 files.</p>

<p>/etc/sysconfig/tgtd:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>TGTD_CONFIG=/etc/tgt/targets.conf</span></code></pre></td></tr></table></div></figure>


<p>/etc/tgtd/targets.conf, make sure there is an entry such as:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;target iqn.2012-02.com.example.gluster:isci>
</span><span class='line'>    backing-store /mnt/test-volume/test.img
</span><span class='line'>    initiator-address 192.168.1.10
</span><span class='line'>&lt;/target></span></code></pre></td></tr></table></div></figure>


<p>This will make that image file you created available to the client with the address 192.168.1.10. This targets.conf file is extremely well commented, so have a read. Now just tell tgtd to reload its configuration from the live node:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>/etc/init.d/tgtd reload</span></code></pre></td></tr></table></div></figure>


<h2>Conclusion</h2>

<p>Nothing here is particularly complicated, but it does give you a lot of storage for a very low price, using a very enterprise friendly OS.</p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
    <a href="/blog/archives">Blog Archives</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2012/04/18/skype-video-on-fedora-64bit/">Skype video on Fedora 64bit</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/04/16/a-learning-experience/">A Learning Experience</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/04/05/pycurl-and-self-signed-ssl-certificates/">PyCurl and self-signed SSL certificates</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/03/19/highly-available-nfs-slash-nas/">Highly Available NFS/NAS</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/02/08/home-made-redundant-thin-provisioned-san/">Home-made Redundant Thin-provisioned SAN</a>
      </li>
    
  </ul>
</section>






  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2012 - Chris Cowley -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'jalb';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
